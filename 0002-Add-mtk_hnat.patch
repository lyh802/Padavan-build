From 745424f62310b2ea36a12601822f5018f649493f Mon Sep 17 00:00:00 2001
From: lyh802 <lyh802@126.com>
Date: Wed, 14 Jun 2023 00:20:35 +0800
Subject: [PATCH 2/4] Add mtk_hnat

---
 .../net/ethernet/raeth/mtk_hnat/Makefile      |    5 +
 .../net/ethernet/raeth/mtk_hnat/hnat.c        |  978 +++++
 .../net/ethernet/raeth/mtk_hnat/hnat.h        | 1264 +++++++
 .../ethernet/raeth/mtk_hnat/hnat_debugfs.c    | 3233 +++++++++++++++++
 .../net/ethernet/raeth/mtk_hnat/hnat_mcast.c  |  354 ++
 .../net/ethernet/raeth/mtk_hnat/hnat_mcast.h  |   69 +
 .../ethernet/raeth/mtk_hnat/hnat_nf_hook.c    | 2945 +++++++++++++++
 .../net/ethernet/raeth/mtk_hnat/hnat_stag.c   |   63 +
 .../net/ethernet/raeth/mtk_hnat/nf_hnat_mtk.h |  179 +
 9 files changed, 9090 insertions(+)
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/Makefile
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.c
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.h
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_debugfs.c
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.c
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.h
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_nf_hook.c
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_stag.c
 create mode 100644 trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/nf_hnat_mtk.h

diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/Makefile b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/Makefile
new file mode 100644
index 00000000..bf1bbcbc
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/Makefile
@@ -0,0 +1,5 @@
+ccflags-y=-Werror
+
+obj-$(CONFIG_NET_MEDIATEK_HNAT)         += mtkhnat.o
+mtkhnat-objs := hnat.o hnat_nf_hook.o hnat_debugfs.o hnat_mcast.o
+mtkhnat-$(CONFIG_NET_DSA_MT7530)	+= hnat_stag.o
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.c b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.c
new file mode 100644
index 00000000..a3eef581
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.c
@@ -0,0 +1,978 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Sean Wang <sean.wang@mediatek.com>
+ *   Copyright (C) 2016-2017 John Crispin <blogic@openwrt.org>
+ */
+
+#include <linux/dma-mapping.h>
+#include <linux/delay.h>
+#include <linux/if.h>
+#include <linux/io.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/platform_device.h>
+#include <linux/reset.h>
+#include <linux/rtnetlink.h>
+#include <net/netlink.h>
+
+#include "nf_hnat_mtk.h"
+#include "hnat.h"
+
+struct mtk_hnat *hnat_priv;
+static struct socket *_hnat_roam_sock;
+static struct work_struct _hnat_roam_work;
+
+int (*ra_sw_nat_hook_rx)(struct sk_buff *skb) = NULL;
+EXPORT_SYMBOL(ra_sw_nat_hook_rx);
+int (*ra_sw_nat_hook_tx)(struct sk_buff *skb, int gmac_no) = NULL;
+EXPORT_SYMBOL(ra_sw_nat_hook_tx);
+void (*ra_sw_nat_clear_bind_entries)(void) = NULL;
+EXPORT_SYMBOL(ra_sw_nat_clear_bind_entries);
+
+int (*ppe_del_entry_by_mac)(unsigned char *mac) = NULL;
+EXPORT_SYMBOL(ppe_del_entry_by_mac);
+
+void (*ppe_dev_register_hook)(struct net_device *dev) = NULL;
+EXPORT_SYMBOL(ppe_dev_register_hook);
+void (*ppe_dev_unregister_hook)(struct net_device *dev) = NULL;
+EXPORT_SYMBOL(ppe_dev_unregister_hook);
+
+static void hnat_sma_build_entry(struct timer_list *t)
+{
+	int i;
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_TB_CFG,
+			     SMA, SMA_FWD_CPU_BUILD_ENTRY);
+}
+
+void hnat_cache_ebl(int enable)
+{
+	int i;
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_CAH_CTRL, CAH_X_MODE, 1);
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_CAH_CTRL, CAH_X_MODE, 0);
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_CAH_CTRL, CAH_EN, enable);
+	}
+}
+
+static void hnat_reset_timestamp(struct timer_list *t)
+{
+	struct foe_entry *entry;
+	int hash_index;
+
+	hnat_cache_ebl(0);
+	cr_set_field(hnat_priv->ppe_base[0] + PPE_TB_CFG, TCP_AGE, 0);
+	cr_set_field(hnat_priv->ppe_base[0] + PPE_TB_CFG, UDP_AGE, 0);
+	writel(0, hnat_priv->fe_base + 0x0010);
+
+	for (hash_index = 0; hash_index < hnat_priv->foe_etry_num; hash_index++) {
+		entry = hnat_priv->foe_table_cpu[0] + hash_index;
+		if (entry->bfib1.state == BIND)
+			entry->bfib1.time_stamp =
+				readl(hnat_priv->fe_base + 0x0010) & (0xFFFF);
+	}
+
+	cr_set_field(hnat_priv->ppe_base[0] + PPE_TB_CFG, TCP_AGE, 1);
+	cr_set_field(hnat_priv->ppe_base[0] + PPE_TB_CFG, UDP_AGE, 1);
+	hnat_cache_ebl(1);
+
+	mod_timer(&hnat_priv->hnat_reset_timestamp_timer, jiffies + 14400 * HZ);
+}
+
+static void cr_set_bits(void __iomem *reg, u32 bs)
+{
+	u32 val = readl(reg);
+
+	val |= bs;
+	writel(val, reg);
+}
+
+static void cr_clr_bits(void __iomem *reg, u32 bs)
+{
+	u32 val = readl(reg);
+
+	val &= ~bs;
+	writel(val, reg);
+}
+
+void cr_set_field(void __iomem *reg, u32 field, u32 val)
+{
+	unsigned int tv = readl(reg);
+
+	tv &= ~field;
+	tv |= ((val) << (ffs((unsigned int)field) - 1));
+	writel(tv, reg);
+}
+
+/*boundary entry can't be used to accelerate data flow*/
+static void exclude_boundary_entry(struct foe_entry *foe_table_cpu)
+{
+	int entry_base = 0;
+	int bad_entry, i, j;
+	struct foe_entry *foe_entry;
+	/*these entries are boundary every 128 entries*/
+	int boundary_entry_offset[8] = { 12, 25, 38, 51, 76, 89, 102, 115};
+
+	if (!foe_table_cpu)
+		return;
+
+	for (i = 0; entry_base < hnat_priv->foe_etry_num; i++) {
+		/* set boundary entries as static*/
+		for (j = 0; j < 8; j++) {
+			bad_entry = entry_base + boundary_entry_offset[j];
+			foe_entry = &foe_table_cpu[bad_entry];
+			foe_entry->udib1.sta = 1;
+		}
+		entry_base = (i + 1) * 128;
+	}
+}
+
+void set_gmac_ppe_fwd(int id, int enable)
+{
+	void __iomem *reg;
+	u32 val;
+
+	reg = hnat_priv->fe_base +
+		((id == NR_GMAC1_PORT) ? GDMA1_FWD_CFG :
+		 (id == NR_GMAC2_PORT) ? GDMA2_FWD_CFG : GDMA3_FWD_CFG);
+
+	if (enable) {
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+		if (CFG_PPE_NUM == 3 && id == NR_GMAC3_PORT)
+			cr_set_bits(reg, BITS_GDM_ALL_FRC_P_PPE2);
+		else if (CFG_PPE_NUM == 3 && id == NR_GMAC2_PORT)
+			cr_set_bits(reg, BITS_GDM_ALL_FRC_P_PPE1);
+#endif
+		cr_set_bits(reg, BITS_GDM_ALL_FRC_P_PPE);
+
+		return;
+	}
+
+	/*disabled */
+	val = readl(reg);
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+	if ((CFG_PPE_NUM == 3 &&
+	    ((val & GDM_ALL_FRC_MASK) == BITS_GDM_ALL_FRC_P_PPE1 ||
+	     (val & GDM_ALL_FRC_MASK) == BITS_GDM_ALL_FRC_P_PPE2)))
+		cr_set_field(reg, GDM_ALL_FRC_MASK,
+			     BITS_GDM_ALL_FRC_P_CPU_PDMA);
+#endif
+
+	if ((val & GDM_ALL_FRC_MASK) == BITS_GDM_ALL_FRC_P_PPE)
+		cr_set_field(reg, GDM_ALL_FRC_MASK,
+				 BITS_GDM_ALL_FRC_P_CPU_PDMA);
+
+}
+
+static int entry_mac_cmp(struct foe_entry *entry, u8 *mac)
+{
+	int ret = 0;
+
+	if(IS_IPV4_GRP(entry)) {
+		if(((swab32(entry->ipv4_hnapt.dmac_hi) == *(u32 *)mac) &&
+			(swab16(entry->ipv4_hnapt.dmac_lo) == *(u16 *)&mac[4])) ||
+			((swab32(entry->ipv4_hnapt.smac_hi) == *(u32 *)mac) &&
+			(swab16(entry->ipv4_hnapt.smac_lo) == *(u16 *)&mac[4])))
+			ret = 1;
+	} else {
+		if(((swab32(entry->ipv6_5t_route.dmac_hi) == *(u32 *)mac) &&
+			(swab16(entry->ipv6_5t_route.dmac_lo) == *(u16 *)&mac[4])) ||
+			((swab32(entry->ipv6_5t_route.smac_hi) == *(u32 *)mac) &&
+			(swab16(entry->ipv6_5t_route.smac_lo) == *(u16 *)&mac[4])))
+			ret = 1;
+	}
+
+	if (ret && debug_level >= 2)
+		pr_info("mac=%pM\n", mac);
+
+	return ret;
+}
+
+int entry_delete_by_mac(u8 *mac)
+{
+	struct foe_entry *entry = NULL;
+	int index, i, ret = 0;
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		entry = hnat_priv->foe_table_cpu[i];
+		for (index = 0; index < DEF_ETRY_NUM; entry++, index++) {
+			if(entry->bfib1.state == BIND && entry_mac_cmp(entry, mac)) {
+				memset(entry, 0, sizeof(*entry));
+				hnat_cache_ebl(1);
+				if (debug_level >= 2)
+					pr_info("delete entry idx = %d\n", index);
+				ret++;
+			}
+		}
+	}
+
+	if(!ret && debug_level >= 2)
+		pr_info("entry not found\n");
+
+	return ret;
+}
+
+static void hnat_roam_handler(struct work_struct *work)
+{
+	struct kvec iov;
+	struct msghdr msg;
+	struct nlmsghdr *nlh;
+	struct ndmsg *ndm;
+	struct nlattr *nla;
+	u8 rcv_buf[512];
+	int len;
+
+	if (!_hnat_roam_sock)
+		return;
+
+	iov.iov_base = rcv_buf;
+	iov.iov_len = sizeof(rcv_buf);
+	memset(&msg, 0, sizeof(msg));
+	msg.msg_namelen = sizeof(struct sockaddr_nl);
+
+	len = kernel_recvmsg(_hnat_roam_sock, &msg, &iov, 1, iov.iov_len, 0);
+	if (len <= 0)
+		goto out;
+
+	nlh = (struct nlmsghdr*)rcv_buf;
+	if (!NLMSG_OK(nlh, len) || nlh->nlmsg_type != RTM_NEWNEIGH)
+		goto out;
+
+	len = nlh->nlmsg_len - NLMSG_HDRLEN;
+	ndm = (struct ndmsg *)NLMSG_DATA(nlh);
+	if (ndm->ndm_family != PF_BRIDGE)
+		goto out;
+
+	nla = (struct nlattr *)((u8 *)ndm + sizeof(struct ndmsg));
+	len -= NLMSG_LENGTH(sizeof(struct ndmsg));
+	while (nla_ok(nla, len)) {
+		if (nla_type(nla) == NDA_LLADDR) {
+			entry_delete_by_mac(nla_data(nla));
+		}
+		nla = nla_next(nla, &len);
+	}
+
+out:
+	schedule_work(&_hnat_roam_work);
+}
+
+static int hnat_roaming_enable(void)
+{
+	struct socket *sock = NULL;
+	struct sockaddr_nl addr;
+	int ret;
+
+	INIT_WORK(&_hnat_roam_work, hnat_roam_handler);
+
+	ret = sock_create_kern(&init_net, AF_NETLINK, SOCK_RAW, NETLINK_ROUTE, &sock);
+	if (ret < 0)
+		goto out;
+
+	_hnat_roam_sock = sock;
+
+	addr.nl_family = AF_NETLINK;
+	addr.nl_pad = 0;
+	addr.nl_pid = 65534;
+	addr.nl_groups = 1 << (RTNLGRP_NEIGH - 1);
+	ret = kernel_bind(sock, (struct sockaddr *)&addr, sizeof(addr));
+	if (ret < 0)
+		goto out;
+
+	schedule_work(&_hnat_roam_work);
+	pr_info("hnat roaming work enable\n");
+
+	return 0;
+out:
+	if (sock)
+		sock_release(sock);
+
+	return ret;
+}
+
+static void hnat_roaming_disable(void)
+{
+	if (_hnat_roam_sock)
+		sock_release(_hnat_roam_sock);
+	_hnat_roam_sock = NULL;
+	pr_info("hnat roaming work disable\n");
+}
+
+static int hnat_hw_init(u32 ppe_id)
+{
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	/* setup hashing */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, TB_ETRY_NUM, hnat_priv->etry_num_cfg);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, HASH_MODE, HASH_MODE_1);
+	writel(HASH_SEED_KEY, hnat_priv->ppe_base[ppe_id] + PPE_HASH_SEED);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, XMODE, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, TB_ENTRY_SIZE,
+		     (hnat_priv->data->version == MTK_HNAT_V3) ? ENTRY_128B :
+		     (hnat_priv->data->version == MTK_HNAT_V2) ? ENTRY_96B :
+								 ENTRY_80B);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, SMA, SMA_FWD_CPU_BUILD_ENTRY);
+
+	/* set ip proto */
+	writel(0xFFFFFFFF, hnat_priv->ppe_base[ppe_id] + PPE_IP_PROT_CHK);
+
+	/* setup caching */
+	hnat_cache_ebl(1);
+
+	/* enable FOE */
+	cr_set_bits(hnat_priv->ppe_base[ppe_id] + PPE_FLOW_CFG,
+		    BIT_IPV4_NAT_EN | BIT_IPV4_NAPT_EN |
+		    BIT_IPV4_NAT_FRAG_EN | BIT_IPV4_HASH_GREK |
+		    BIT_IPV4_DSL_EN | BIT_IPV6_6RD_EN |
+		    BIT_IPV6_3T_ROUTE_EN | BIT_IPV6_5T_ROUTE_EN);
+
+	if (hnat_priv->data->version == MTK_HNAT_V2 ||
+	    hnat_priv->data->version == MTK_HNAT_V3)
+		cr_set_bits(hnat_priv->ppe_base[ppe_id] + PPE_FLOW_CFG,
+			    BIT_IPV4_MAPE_EN | BIT_IPV4_MAPT_EN);
+
+	if (hnat_priv->data->version == MTK_HNAT_V3)
+		cr_set_bits(hnat_priv->ppe_base[ppe_id] + PPE_FLOW_CFG,
+			    BIT_IPV6_NAT_EN | BIT_IPV6_NAPT_EN |
+			    BIT_CS0_RM_ALL_IP6_IP_EN);
+
+	/* setup FOE aging */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, NTU_AGE, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, UNBD_AGE, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_UNB_AGE, UNB_MNP, 1000);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_UNB_AGE, UNB_DLTA, 3);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, TCP_AGE, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, UDP_AGE, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, FIN_AGE, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BND_AGE_0, UDP_DLTA, 12);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BND_AGE_0, NTU_DLTA, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BND_AGE_1, FIN_DLTA, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BND_AGE_1, TCP_DLTA, 7);
+
+	/* setup FOE ka */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, KA_CFG, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BIND_LMT_1, NTU_KA, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_KA, KA_T, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_KA, TCP_KA, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_KA, UDP_KA, 0);
+	mdelay(10);
+
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, SCAN_MODE, 2);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, KA_CFG, 3);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, TICK_SEL, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_KA, KA_T, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_KA, TCP_KA, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_KA, UDP_KA, 1);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BIND_LMT_1, NTU_KA, 1);
+
+	/* setup FOE rate limit */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BIND_LMT_0, QURT_LMT, 16383);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BIND_LMT_0, HALF_LMT, 16383);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BIND_LMT_1, FULL_LMT, 16383);
+	/* setup binding threshold as 30 packets per second */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_BNDR, BIND_RATE, 0x1E);
+
+	/* setup FOE cf gen */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_GLO_CFG, PPE_EN, 1);
+	writel(0, hnat_priv->ppe_base[ppe_id] + PPE_DFT_CPORT); /* pdma */
+	/* writel(0x55555555, hnat_priv->ppe_base[ppe_id] + PPE_DFT_CPORT); */ /* qdma */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_GLO_CFG, TTL0_DRP, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_GLO_CFG, MCAST_TB_EN, 1);
+
+	if (hnat_priv->data->version == MTK_HNAT_V2 ||
+	    hnat_priv->data->version == MTK_HNAT_V3) {
+		writel(0xcb777, hnat_priv->ppe_base[ppe_id] + PPE_DFT_CPORT1);
+		writel(0x7f, hnat_priv->ppe_base[ppe_id] + PPE_SBW_CTRL);
+	}
+
+	if (hnat_priv->data->version == MTK_HNAT_V3) {
+		cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_SB_FIFO_DBG,
+			     SB_MED_FULL_DRP_EN, 1);
+	}
+
+	/*enable ppe mib counter*/
+	if (hnat_priv->data->per_flow_accounting) {
+		cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MIB_CFG, MIB_EN, 1);
+		cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MIB_CFG, MIB_READ_CLEAR, 1);
+		cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MIB_CAH_CTRL, MIB_CAH_EN, 1);
+	}
+
+	hnat_priv->g_ppdev = dev_get_by_name(&init_net, hnat_priv->ppd);
+	hnat_priv->g_wandev = dev_get_by_name(&init_net, hnat_priv->wan);
+
+	dev_info(hnat_priv->dev, "PPE%d hwnat start\n", ppe_id);
+
+	return 0;
+}
+
+static int hnat_start(u32 ppe_id)
+{
+	u32 foe_table_sz;
+	u32 foe_mib_tb_sz;
+	int etry_num_cfg;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	/* mapp the FOE table */
+	for (etry_num_cfg = DEF_ETRY_NUM_CFG ; etry_num_cfg >= 0 ;
+	     etry_num_cfg--, hnat_priv->foe_etry_num /= 2) {
+		foe_table_sz = hnat_priv->foe_etry_num * sizeof(struct foe_entry);
+		hnat_priv->foe_table_cpu[ppe_id] = dma_alloc_coherent(
+				hnat_priv->dev, foe_table_sz,
+				&hnat_priv->foe_table_dev[ppe_id], GFP_KERNEL);
+
+		if (hnat_priv->foe_table_cpu[ppe_id])
+			break;
+	}
+
+	if (!hnat_priv->foe_table_cpu[ppe_id])
+		return -1;
+	dev_info(hnat_priv->dev, "PPE%d entry number = %d\n",
+		 ppe_id, hnat_priv->foe_etry_num);
+
+	writel(hnat_priv->foe_table_dev[ppe_id], hnat_priv->ppe_base[ppe_id] + PPE_TB_BASE);
+	memset(hnat_priv->foe_table_cpu[ppe_id], 0, foe_table_sz);
+
+	if (hnat_priv->data->version == MTK_HNAT_V1_1)
+		exclude_boundary_entry(hnat_priv->foe_table_cpu[ppe_id]);
+
+	if (hnat_priv->data->per_flow_accounting) {
+		foe_mib_tb_sz = hnat_priv->foe_etry_num * sizeof(struct mib_entry);
+		hnat_priv->foe_mib_cpu[ppe_id] =
+			dma_alloc_coherent(hnat_priv->dev, foe_mib_tb_sz,
+					   &hnat_priv->foe_mib_dev[ppe_id], GFP_KERNEL);
+		if (!hnat_priv->foe_mib_cpu[ppe_id])
+			return -1;
+		writel(hnat_priv->foe_mib_dev[ppe_id],
+		       hnat_priv->ppe_base[ppe_id] + PPE_MIB_TB_BASE);
+		memset(hnat_priv->foe_mib_cpu[ppe_id], 0, foe_mib_tb_sz);
+
+		hnat_priv->acct[ppe_id] =
+			kzalloc(hnat_priv->foe_etry_num * sizeof(struct hnat_accounting),
+				GFP_KERNEL);
+		if (!hnat_priv->acct[ppe_id])
+			return -1;
+	}
+
+	hnat_priv->etry_num_cfg = etry_num_cfg;
+	hnat_hw_init(ppe_id);
+
+	return 0;
+}
+
+static int ppe_busy_wait(u32 ppe_id)
+{
+	unsigned long t_start = jiffies;
+	u32 r = 0;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	while (1) {
+		r = readl((hnat_priv->ppe_base[ppe_id] + 0x0));
+		if (!(r & BIT(31)))
+			return 0;
+		if (time_after(jiffies, t_start + HZ))
+			break;
+		mdelay(10);
+	}
+
+	dev_notice(hnat_priv->dev, "ppe:%s timeout\n", __func__);
+
+	return -1;
+}
+
+static void hnat_stop(u32 ppe_id)
+{
+	u32 foe_table_sz;
+	u32 foe_mib_tb_sz;
+	struct foe_entry *entry, *end;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return;
+
+	/* send all traffic back to the DMA engine */
+	set_gmac_ppe_fwd(NR_GMAC1_PORT, 0);
+	set_gmac_ppe_fwd(NR_GMAC2_PORT, 0);
+	set_gmac_ppe_fwd(NR_GMAC3_PORT, 0);
+
+	dev_info(hnat_priv->dev, "hwnat stop\n");
+
+	if (hnat_priv->foe_table_cpu[ppe_id]) {
+		entry = hnat_priv->foe_table_cpu[ppe_id];
+		end = hnat_priv->foe_table_cpu[ppe_id] + hnat_priv->foe_etry_num;
+		while (entry < end) {
+			entry->bfib1.state = INVALID;
+			entry++;
+		}
+	}
+	/* disable caching */
+	hnat_cache_ebl(0);
+
+	/* flush cache has to be ahead of hnat disable --*/
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_GLO_CFG, PPE_EN, 0);
+
+	/* disable scan mode and keep-alive */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, SCAN_MODE, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, KA_CFG, 0);
+
+	ppe_busy_wait(ppe_id);
+
+	/* disable FOE */
+	cr_clr_bits(hnat_priv->ppe_base[ppe_id] + PPE_FLOW_CFG,
+		    BIT_IPV4_NAPT_EN | BIT_IPV4_NAT_EN | BIT_IPV4_NAT_FRAG_EN |
+		    BIT_IPV6_HASH_GREK | BIT_IPV4_DSL_EN |
+		    BIT_IPV6_6RD_EN | BIT_IPV6_3T_ROUTE_EN |
+		    BIT_IPV6_5T_ROUTE_EN | BIT_FUC_FOE | BIT_FMC_FOE);
+
+	if (hnat_priv->data->version == MTK_HNAT_V2 ||
+	    hnat_priv->data->version == MTK_HNAT_V3)
+		cr_clr_bits(hnat_priv->ppe_base[ppe_id] + PPE_FLOW_CFG,
+			    BIT_IPV4_MAPE_EN | BIT_IPV4_MAPT_EN);
+
+	if (hnat_priv->data->version == MTK_HNAT_V3)
+		cr_clr_bits(hnat_priv->ppe_base[ppe_id] + PPE_FLOW_CFG,
+			    BIT_IPV6_NAT_EN | BIT_IPV6_NAPT_EN |
+			    BIT_CS0_RM_ALL_IP6_IP_EN);
+
+	/* disable FOE aging */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, NTU_AGE, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, UNBD_AGE, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, TCP_AGE, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, UDP_AGE, 0);
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_TB_CFG, FIN_AGE, 0);
+
+	/* free the FOE table */
+	foe_table_sz = hnat_priv->foe_etry_num * sizeof(struct foe_entry);
+	if (hnat_priv->foe_table_cpu[ppe_id])
+		dma_free_coherent(hnat_priv->dev, foe_table_sz,
+				  hnat_priv->foe_table_cpu[ppe_id],
+				  hnat_priv->foe_table_dev[ppe_id]);
+	hnat_priv->foe_table_cpu[ppe_id] = NULL;
+	writel(0, hnat_priv->ppe_base[ppe_id] + PPE_TB_BASE);
+
+	if (hnat_priv->data->per_flow_accounting) {
+		foe_mib_tb_sz = hnat_priv->foe_etry_num * sizeof(struct mib_entry);
+		if (hnat_priv->foe_mib_cpu[ppe_id])
+			dma_free_coherent(hnat_priv->dev, foe_mib_tb_sz,
+					  hnat_priv->foe_mib_cpu[ppe_id],
+					  hnat_priv->foe_mib_dev[ppe_id]);
+		writel(0, hnat_priv->ppe_base[ppe_id] + PPE_MIB_TB_BASE);
+		kfree(hnat_priv->acct[ppe_id]);
+	}
+}
+
+static void hnat_release_netdev(void)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (ext_entry->dev)
+			dev_put(ext_entry->dev);
+		ext_if_del(ext_entry);
+		kfree(ext_entry);
+	}
+
+	if (hnat_priv->g_ppdev)
+		dev_put(hnat_priv->g_ppdev);
+
+	if (hnat_priv->g_wandev)
+		dev_put(hnat_priv->g_wandev);
+}
+
+static struct notifier_block nf_hnat_netdevice_nb __read_mostly = {
+	.notifier_call = nf_hnat_netdevice_event,
+};
+
+static struct notifier_block nf_hnat_netevent_nb __read_mostly = {
+	.notifier_call = nf_hnat_netevent_handler,
+};
+
+int hnat_enable_hook(void)
+{
+	/* register hook functions used by WHNAT module.
+	 */
+	if (hnat_priv->data->whnat) {
+		ra_sw_nat_hook_rx =
+			(hnat_priv->data->version == MTK_HNAT_V2 ||
+			 hnat_priv->data->version == MTK_HNAT_V3) ?
+			 mtk_sw_nat_hook_rx : NULL;
+		ra_sw_nat_hook_tx = mtk_sw_nat_hook_tx;
+		ppe_dev_register_hook = mtk_ppe_dev_register_hook;
+		ppe_dev_unregister_hook = mtk_ppe_dev_unregister_hook;
+		ra_sw_nat_clear_bind_entries = foe_clear_all_bind_entries;
+	}
+
+	if (hnat_register_nf_hooks())
+		return -1;
+
+	ppe_del_entry_by_mac = entry_delete_by_mac;
+	hook_toggle = 1;
+
+	return 0;
+}
+
+int hnat_disable_hook(void)
+{
+	int i, hash_index;
+	struct foe_entry *entry;
+
+	ra_sw_nat_hook_tx = NULL;
+	ra_sw_nat_hook_rx = NULL;
+	ra_sw_nat_clear_bind_entries = NULL;
+	hnat_unregister_nf_hooks();
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_TB_CFG,
+			     SMA, SMA_ONLY_FWD_CPU);
+
+		for (hash_index = 0; hash_index < hnat_priv->foe_etry_num; hash_index++) {
+			entry = hnat_priv->foe_table_cpu[i] + hash_index;
+			if (entry->bfib1.state == BIND) {
+				entry->ipv4_hnapt.udib1.state = INVALID;
+				entry->ipv4_hnapt.udib1.time_stamp =
+					readl((hnat_priv->fe_base + 0x0010)) & 0xFF;
+			}
+		}
+	}
+
+	/* clear HWNAT cache */
+	hnat_cache_ebl(1);
+
+	mod_timer(&hnat_priv->hnat_sma_build_entry_timer, jiffies + 3 * HZ);
+	ppe_del_entry_by_mac = NULL;
+	hook_toggle = 0;
+
+	return 0;
+}
+
+int hnat_warm_init(void)
+{
+	u32 foe_table_sz, foe_mib_tb_sz, ppe_id = 0;
+
+	unregister_netevent_notifier(&nf_hnat_netevent_nb);
+
+	for (ppe_id = 0; ppe_id < CFG_PPE_NUM; ppe_id++) {
+		foe_table_sz =
+			hnat_priv->foe_etry_num * sizeof(struct foe_entry);
+		writel(hnat_priv->foe_table_dev[ppe_id],
+		       hnat_priv->ppe_base[ppe_id] + PPE_TB_BASE);
+		memset(hnat_priv->foe_table_cpu[ppe_id], 0, foe_table_sz);
+
+		if (hnat_priv->data->version == MTK_HNAT_V1_1)
+			exclude_boundary_entry(hnat_priv->foe_table_cpu[ppe_id]);
+
+		if (hnat_priv->data->per_flow_accounting) {
+			foe_mib_tb_sz =
+				hnat_priv->foe_etry_num * sizeof(struct mib_entry);
+			writel(hnat_priv->foe_mib_dev[ppe_id],
+			       hnat_priv->ppe_base[ppe_id] + PPE_MIB_TB_BASE);
+			memset(hnat_priv->foe_mib_cpu[ppe_id], 0,
+			       foe_mib_tb_sz);
+		}
+
+		hnat_hw_init(ppe_id);
+	}
+
+	set_gmac_ppe_fwd(NR_GMAC1_PORT, 1);
+	set_gmac_ppe_fwd(NR_GMAC2_PORT, 1);
+	set_gmac_ppe_fwd(NR_GMAC3_PORT, 1);
+	register_netevent_notifier(&nf_hnat_netevent_nb);
+
+	return 0;
+}
+
+static struct packet_type mtk_pack_type __read_mostly = {
+	.type   = HQOS_MAGIC_TAG,
+	.func   = mtk_hqos_ptype_cb,
+};
+
+static int hnat_probe(struct platform_device *pdev)
+{
+	int i;
+	int err = 0;
+	int index = 0;
+	struct resource *res;
+	const char *name;
+	struct device_node *np;
+	unsigned int val;
+	struct property *prop;
+	struct extdev_entry *ext_entry;
+	const struct of_device_id *match;
+
+	hnat_priv = devm_kzalloc(&pdev->dev, sizeof(struct mtk_hnat), GFP_KERNEL);
+	if (!hnat_priv)
+		return -ENOMEM;
+
+	hnat_priv->foe_etry_num = DEF_ETRY_NUM;
+
+	match = of_match_device(of_hnat_match, &pdev->dev);
+	if (unlikely(!match))
+		return -EINVAL;
+
+	hnat_priv->data = (struct mtk_hnat_data *)match->data;
+
+	hnat_priv->dev = &pdev->dev;
+	np = hnat_priv->dev->of_node;
+
+	err = of_property_read_string(np, "mtketh-wan", &name);
+	if (err < 0)
+		return -EINVAL;
+
+	strncpy(hnat_priv->wan, (char *)name, IFNAMSIZ - 1);
+	dev_info(&pdev->dev, "wan = %s\n", hnat_priv->wan);
+
+	err = of_property_read_string(np, "mtketh-lan", &name);
+	if (err < 0)
+		strncpy(hnat_priv->lan, "eth0", IFNAMSIZ);
+	else
+		strncpy(hnat_priv->lan, (char *)name, IFNAMSIZ - 1);
+	dev_info(&pdev->dev, "lan = %s\n", hnat_priv->lan);
+
+	err = of_property_read_string(np, "mtketh-lan2", &name);
+	if (err < 0)
+		strncpy(hnat_priv->lan2, "eth2", IFNAMSIZ);
+	else
+		strncpy(hnat_priv->lan2, (char *)name, IFNAMSIZ - 1);
+	dev_info(&pdev->dev, "lan2 = %s\n", hnat_priv->lan2);
+
+	err = of_property_read_string(np, "mtketh-ppd", &name);
+	if (err < 0)
+		strncpy(hnat_priv->ppd, "eth0", IFNAMSIZ);
+	else
+		strncpy(hnat_priv->ppd, (char *)name, IFNAMSIZ - 1);
+	dev_info(&pdev->dev, "ppd = %s\n", hnat_priv->ppd);
+
+	/*get total gmac num in hnat*/
+	err = of_property_read_u32_index(np, "mtketh-max-gmac", 0, &val);
+
+	if (err < 0)
+		return -EINVAL;
+
+	hnat_priv->gmac_num = val;
+
+	dev_info(&pdev->dev, "gmac num = %d\n", hnat_priv->gmac_num);
+
+	err = of_property_read_u32_index(np, "mtkdsa-wan-port", 0, &val);
+
+	if (err < 0) {
+		hnat_priv->wan_dsa_port = NONE_DSA_PORT;
+	} else {
+		hnat_priv->wan_dsa_port = val;
+		dev_info(&pdev->dev, "wan dsa port = %d\n", hnat_priv->wan_dsa_port);
+	}
+
+	err = of_property_read_u32_index(np, "mtketh-ppe-num", 0, &val);
+
+	if (err < 0)
+		hnat_priv->ppe_num = 1;
+	else
+		hnat_priv->ppe_num = val;
+
+	dev_info(&pdev->dev, "ppe num = %d\n", hnat_priv->ppe_num);
+
+	res = platform_get_resource(pdev, IORESOURCE_MEM, 0);
+	if (!res)
+		return -ENOENT;
+
+	hnat_priv->fe_base = devm_ioremap_nocache(&pdev->dev, res->start,
+					     res->end - res->start + 1);
+	if (!hnat_priv->fe_base)
+		return -EADDRNOTAVAIL;
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+	hnat_priv->ppe_base[0] = hnat_priv->fe_base + 0x2200;
+
+	if (CFG_PPE_NUM > 1)
+		hnat_priv->ppe_base[1] = hnat_priv->fe_base + 0x2600;
+
+	if (CFG_PPE_NUM > 2)
+		hnat_priv->ppe_base[2] = hnat_priv->fe_base + 0x2e00;
+#else
+	hnat_priv->ppe_base[0] = hnat_priv->fe_base + 0xe00;
+#endif
+
+	err = hnat_init_debugfs(hnat_priv);
+	if (err)
+		return err;
+
+	prop = of_find_property(np, "ext-devices", NULL);
+	for (name = of_prop_next_string(prop, NULL); name;
+	     name = of_prop_next_string(prop, name), index++) {
+		ext_entry = kzalloc(sizeof(*ext_entry), GFP_KERNEL);
+		if (!ext_entry) {
+			err = -ENOMEM;
+			goto err_out1;
+		}
+		strncpy(ext_entry->name, (char *)name, IFNAMSIZ - 1);
+		ext_if_add(ext_entry);
+	}
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		dev_info(&pdev->dev, "ext devices = %s\n", ext_entry->name);
+	}
+
+	hnat_priv->lvid = 1;
+	hnat_priv->wvid = 2;
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		err = hnat_start(i);
+		if (err)
+			goto err_out;
+	}
+
+	if (hnat_priv->data->whnat) {
+		err = whnat_adjust_nf_hooks();
+		if (err)
+			goto err_out;
+	}
+
+	err = hnat_enable_hook();
+	if (err)
+		goto err_out;
+
+	register_netdevice_notifier(&nf_hnat_netdevice_nb);
+	register_netevent_notifier(&nf_hnat_netevent_nb);
+
+	if (hnat_priv->data->mcast) {
+		for (i = 0; i < CFG_PPE_NUM; i++)
+			hnat_mcast_enable(i);
+	}
+
+	timer_setup(&hnat_priv->hnat_sma_build_entry_timer, hnat_sma_build_entry, 0);
+	if (hnat_priv->data->version == MTK_HNAT_V1_3) {
+		timer_setup(&hnat_priv->hnat_reset_timestamp_timer, hnat_reset_timestamp, 0);
+		hnat_priv->hnat_reset_timestamp_timer.expires = jiffies;
+		add_timer(&hnat_priv->hnat_reset_timestamp_timer);
+	}
+
+	if (IS_HQOS_MODE && IS_GMAC1_MODE)
+		dev_add_pack(&mtk_pack_type);
+
+	err = hnat_roaming_enable();
+	if (err)
+		pr_info("hnat roaming work fail\n");
+
+	INIT_LIST_HEAD(&hnat_priv->xlat.map_list);
+
+	return 0;
+
+err_out:
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		hnat_stop(i);
+err_out1:
+	hnat_deinit_debugfs(hnat_priv);
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		ext_if_del(ext_entry);
+		kfree(ext_entry);
+	}
+	return err;
+}
+
+static int hnat_remove(struct platform_device *pdev)
+{
+	int i;
+
+	hnat_roaming_disable();
+	unregister_netdevice_notifier(&nf_hnat_netdevice_nb);
+	unregister_netevent_notifier(&nf_hnat_netevent_nb);
+	hnat_disable_hook();
+
+	if (hnat_priv->data->mcast)
+		hnat_mcast_disable();
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		hnat_stop(i);
+
+	hnat_deinit_debugfs(hnat_priv);
+	hnat_release_netdev();
+	del_timer_sync(&hnat_priv->hnat_sma_build_entry_timer);
+	if (hnat_priv->data->version == MTK_HNAT_V1_3)
+		del_timer_sync(&hnat_priv->hnat_reset_timestamp_timer);
+
+	if (IS_HQOS_MODE && IS_GMAC1_MODE)
+		dev_remove_pack(&mtk_pack_type);
+
+	return 0;
+}
+
+static const struct mtk_hnat_data hnat_data_v1 = {
+	.num_of_sch = 2,
+	.whnat = false,
+	.per_flow_accounting = false,
+	.mcast = false,
+	.version = MTK_HNAT_V1_1,
+};
+
+static const struct mtk_hnat_data hnat_data_v2 = {
+	.num_of_sch = 2,
+	.whnat = true,
+	.per_flow_accounting = true,
+	.mcast = false,
+	.version = MTK_HNAT_V1_2,
+};
+
+static const struct mtk_hnat_data hnat_data_v3 = {
+	.num_of_sch = 4,
+	.whnat = false,
+	.per_flow_accounting = false,
+	.mcast = false,
+	.version = MTK_HNAT_V1_3,
+};
+
+static const struct mtk_hnat_data hnat_data_v4 = {
+	.num_of_sch = 4,
+	.whnat = true,
+	.per_flow_accounting = true,
+	.mcast = false,
+	.version = MTK_HNAT_V2,
+};
+
+static const struct mtk_hnat_data hnat_data_v5 = {
+	.num_of_sch = 4,
+	.whnat = true,
+	.per_flow_accounting = true,
+	.mcast = false,
+	.version = MTK_HNAT_V3,
+};
+
+const struct of_device_id of_hnat_match[] = {
+	{ .compatible = "mediatek,mtk-hnat", .data = &hnat_data_v3 },
+	{ .compatible = "mediatek,mtk-hnat_v1", .data = &hnat_data_v1 },
+	{ .compatible = "mediatek,mtk-hnat_v2", .data = &hnat_data_v2 },
+	{ .compatible = "mediatek,mtk-hnat_v3", .data = &hnat_data_v3 },
+	{ .compatible = "mediatek,mtk-hnat_v4", .data = &hnat_data_v4 },
+	{ .compatible = "mediatek,mtk-hnat_v5", .data = &hnat_data_v5 },
+	{},
+};
+MODULE_DEVICE_TABLE(of, of_hnat_match);
+
+static struct platform_driver hnat_driver = {
+	.probe = hnat_probe,
+	.remove = hnat_remove,
+	.driver = {
+		.name = "mediatek_soc_hnat",
+		.of_match_table = of_hnat_match,
+	},
+};
+
+module_platform_driver(hnat_driver);
+
+MODULE_LICENSE("GPL v2");
+MODULE_AUTHOR("Sean Wang <sean.wang@mediatek.com>");
+MODULE_AUTHOR("John Crispin <john@phrozen.org>");
+MODULE_DESCRIPTION("Mediatek Hardware NAT");
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.h b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.h
new file mode 100644
index 00000000..331fce9c
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat.h
@@ -0,0 +1,1264 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Sean Wang <sean.wang@mediatek.com>
+ *   Copyright (C) 2016-2017 John Crispin <blogic@openwrt.org>
+ */
+
+#include <linux/debugfs.h>
+#include <linux/string.h>
+#include <linux/if.h>
+#include <linux/if_ether.h>
+#include <net/netevent.h>
+#include <linux/mod_devicetable.h>
+#include "hnat_mcast.h"
+#include "nf_hnat_mtk.h"
+
+/*--------------------------------------------------------------------------*/
+/* Register Offset*/
+/*--------------------------------------------------------------------------*/
+#define PPE_GLO_CFG 0x00
+#define PPE_FLOW_CFG 0x04
+#define PPE_IP_PROT_CHK 0x08
+#define PPE_IP_PROT_0 0x0C
+#define PPE_IP_PROT_1 0x10
+#define PPE_IP_PROT_2 0x14
+#define PPE_IP_PROT_3 0x18
+#define PPE_TB_CFG 0x1C
+#define PPE_TB_BASE 0x20
+#define PPE_TB_USED 0x24
+#define PPE_BNDR 0x28
+#define PPE_BIND_LMT_0 0x2C
+#define PPE_BIND_LMT_1 0x30
+#define PPE_KA 0x34
+#define PPE_UNB_AGE 0x38
+#define PPE_BND_AGE_0 0x3C
+#define PPE_BND_AGE_1 0x40
+#define PPE_HASH_SEED 0x44
+#define PPE_DFT_CPORT 0x48
+#define PPE_DFT_CPORT1 0x4C
+#define PPE_MCAST_PPSE 0x84
+#define PPE_MCAST_L_0 0x88
+#define PPE_MCAST_H_0 0x8C
+#define PPE_MCAST_L_1 0x90
+#define PPE_MCAST_H_1 0x94
+#define PPE_MCAST_L_2 0x98
+#define PPE_MCAST_H_2 0x9C
+#define PPE_MCAST_L_3 0xA0
+#define PPE_MCAST_H_3 0xA4
+#define PPE_MCAST_L_4 0xA8
+#define PPE_MCAST_H_4 0xAC
+#define PPE_MCAST_L_5 0xB0
+#define PPE_MCAST_H_5 0xB4
+#define PPE_MCAST_L_6 0xBC
+#define PPE_MCAST_H_6 0xC0
+#define PPE_MCAST_L_7 0xC4
+#define PPE_MCAST_H_7 0xC8
+#define PPE_MCAST_L_8 0xCC
+#define PPE_MCAST_H_8 0xD0
+#define PPE_MCAST_L_9 0xD4
+#define PPE_MCAST_H_9 0xD8
+#define PPE_MCAST_L_A 0xDC
+#define PPE_MCAST_H_A 0xE0
+#define PPE_MCAST_L_B 0xE4
+#define PPE_MCAST_H_B 0xE8
+#define PPE_MCAST_L_C 0xEC
+#define PPE_MCAST_H_C 0xF0
+#define PPE_MCAST_L_D 0xF4
+#define PPE_MCAST_H_D 0xF8
+#define PPE_MCAST_L_E 0xFC
+#define PPE_MCAST_H_E 0xE0
+#define PPE_MCAST_L_F 0x100
+#define PPE_MCAST_H_F 0x104
+#define PPE_MCAST_L_10 0xC00
+#define PPE_MCAST_H_10 0xC04
+#define PPE_MTU_DRP 0x108
+#define PPE_MTU_VLYR_0 0x10C
+#define PPE_MTU_VLYR_1 0x110
+#define PPE_MTU_VLYR_2 0x114
+#define PPE_VPM_TPID 0x118
+#define PPE_CAH_CTRL 0x120
+#define PPE_CAH_TAG_SRH 0x124
+#define PPE_CAH_LINE_RW 0x128
+#define PPE_CAH_WDATA 0x12C
+#define PPE_CAH_RDATA 0x130
+
+#define PPE_MIB_CFG 0X134
+#define PPE_MIB_TB_BASE 0X138
+#define PPE_MIB_SER_CR 0X13C
+#define PPE_MIB_SER_R0 0X140
+#define PPE_MIB_SER_R1 0X144
+#define PPE_MIB_SER_R2 0X148
+#define PPE_MIB_SER_R3 0X14C
+#define PPE_MIB_CAH_CTRL 0X150
+#define PPE_MIB_CAH_TAG_SRH 0X154
+#define PPE_MIB_CAH_LINE_RW 0X158
+#define PPE_MIB_CAH_WDATA 0X15C
+#define PPE_MIB_CAH_RDATA 0X160
+#define PPE_SB_FIFO_DBG 0x170
+#define PPE_SBW_CTRL 0x174
+
+#define GDMA1_FWD_CFG 0x500
+#define GDMA2_FWD_CFG 0x1500
+#define GDMA3_FWD_CFG 0x540
+
+/* QDMA Tx queue configuration */
+#define QTX_CFG(x)			(QDMA_BASE + ((x) * 0x10))
+#define QTX_CFG_HW_RESV_CNT_OFFSET	(8)
+#define QTX_CFG_SW_RESV_CNT_OFFSET	(0)
+
+#define QTX_SCH(x)			(QDMA_BASE + 0x4 + ((x) * 0x10))
+#define QTX_SCH_MIN_RATE_EN		BIT(27)
+#define QTX_SCH_MAX_RATE_EN		BIT(11)
+#define QTX_SCH_MIN_RATE_MAN_OFFSET	(20)
+#define QTX_SCH_MIN_RATE_EXP_OFFSET	(16)
+#define QTX_SCH_MAX_RATE_WGHT_OFFSET	(12)
+#define QTX_SCH_MAX_RATE_MAN_OFFSET	(4)
+#define QTX_SCH_MAX_RATE_EXP_OFFSET	(0)
+
+/* QDMA Tx scheduler configuration */
+#define QDMA_PAGE			(QDMA_BASE + 0x1f0)
+#define QDMA_TX_2SCH_BASE		(QDMA_BASE + 0x214)
+#define QTX_MIB_IF			(QDMA_BASE + 0x2bc)
+#define QDMA_TX_4SCH_BASE(x)		(QDMA_BASE + 0x398 + (((x) >> 1) * 0x4))
+#define QDMA_TX_SCH_WFQ_EN		BIT(15)
+
+/*--------------------------------------------------------------------------*/
+/* Register Mask*/
+/*--------------------------------------------------------------------------*/
+/* PPE_TB_CFG mask */
+#define TB_ETRY_NUM (0x7 << 0) /* RW */
+#define TB_ENTRY_SIZE (0x1 << 3) /* RW */
+#define SMA (0x3 << 4) /* RW */
+#define NTU_AGE (0x1 << 7) /* RW */
+#define UNBD_AGE (0x1 << 8) /* RW */
+#define TCP_AGE (0x1 << 9) /* RW */
+#define UDP_AGE (0x1 << 10) /* RW */
+#define FIN_AGE (0x1 << 11) /* RW */
+#define KA_CFG (0x3 << 12)
+#define HASH_MODE (0x3 << 14) /* RW */
+#define SCAN_MODE (0x3 << 16) /* RW */
+#define XMODE (0x3 << 18) /* RW */
+#define TICK_SEL (0x1 << 24) /* RW */
+
+
+/*PPE_CAH_CTRL mask*/
+#define CAH_EN (0x1 << 0) /* RW */
+#define CAH_X_MODE (0x1 << 9) /* RW */
+
+/*PPE_UNB_AGE mask*/
+#define UNB_DLTA (0xff << 0) /* RW */
+#define UNB_MNP (0xffff << 16) /* RW */
+
+/*PPE_BND_AGE_0 mask*/
+#define UDP_DLTA (0xffff << 0) /* RW */
+#define NTU_DLTA (0xffff << 16) /* RW */
+
+/*PPE_BND_AGE_1 mask*/
+#define TCP_DLTA (0xffff << 0) /* RW */
+#define FIN_DLTA (0xffff << 16) /* RW */
+
+/*PPE_KA mask*/
+#define KA_T (0xffff << 0) /* RW */
+#define TCP_KA (0xff << 16) /* RW */
+#define UDP_KA (0xff << 24) /* RW */
+
+/*PPE_BIND_LMT_0 mask*/
+#define QURT_LMT (0x3ff << 0) /* RW */
+#define HALF_LMT (0x3ff << 16) /* RW */
+
+/*PPE_BIND_LMT_1 mask*/
+#define FULL_LMT (0x3fff << 0) /* RW */
+#define NTU_KA (0xff << 16) /* RW */
+
+/*PPE_BNDR mask*/
+#define BIND_RATE (0xffff << 0) /* RW */
+#define PBND_RD_PRD (0xffff << 16) /* RW */
+
+/*PPE_GLO_CFG mask*/
+#define PPE_EN (0x1 << 0) /* RW */
+#define TTL0_DRP (0x1 << 4) /* RW */
+#define MCAST_TB_EN (0x1 << 7) /* RW */
+#define MCAST_HASH (0x3 << 12) /* RW */
+
+#define MC_P3_PPSE (0xf << 12) /* RW */
+#define MC_P2_PPSE (0xf << 8) /* RW */
+#define MC_P1_PPSE (0xf << 4) /* RW */
+#define MC_P0_PPSE (0xf << 0) /* RW */
+
+#define MIB_EN (0x1 << 0) /* RW */
+#define MIB_READ_CLEAR (0X1 << 1) /* RW */
+#define MIB_CAH_EN (0X1 << 0) /* RW */
+
+/*GDMA_FWD_CFG mask */
+#define GDM_UFRC_MASK (0xF << 12) /* RW */
+#define GDM_BFRC_MASK (0xF << 8) /*RW*/
+#define GDM_MFRC_MASK (0xF << 4) /*RW*/
+#define GDM_OFRC_MASK (0xF << 0) /*RW*/
+#define GDM_ALL_FRC_MASK                                                      \
+	(GDM_UFRC_MASK | GDM_BFRC_MASK | GDM_MFRC_MASK | GDM_OFRC_MASK)
+
+/*QDMA_PAGE mask*/
+#define QTX_CFG_PAGE (0xf << 0) /* RW */
+
+/*QTX_MIB_IF mask*/
+#define MIB_ON_QTX_CFG (0x1 << 31) /* RW */
+#define VQTX_MIB_EN (0x1 << 28) /* RW */
+
+/* PPE Side Band FIFO Debug Mask */
+#define SB_MED_FULL_DRP_EN (0x1 << 11)
+
+/*--------------------------------------------------------------------------*/
+/* Descriptor Structure */
+/*--------------------------------------------------------------------------*/
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+struct hnat_unbind_info_blk {
+	u32 time_stamp : 8;
+	u32 sp : 4;
+	u32 pcnt : 8;
+	u32 ilgf : 1;
+	u32 mc : 1;
+	u32 preb : 1;
+	u32 pkt_type : 5;
+	u32 state : 2;
+	u32 udp : 1;
+	u32 sta : 1;		/* static entry */
+} __packed;
+
+struct hnat_bind_info_blk {
+	u32 time_stamp : 8;
+	u32 sp : 4;
+	u32 mc : 1;
+	u32 ka : 1;		/* keep alive */
+	u32 vlan_layer : 3;
+	u32 psn : 1;		/* egress packet has PPPoE session */
+	u32 vpm : 1;		/* 0:ethertype remark, 1:0x8100(CR default) */
+	u32 ps : 1;		/* packet sampling */
+	u32 cah : 1;		/* cacheable flag */
+	u32 rmt : 1;		/* remove tunnel ip header (6rd/dslite only) */
+	u32 ttl : 1;
+	u32 pkt_type : 5;
+	u32 state : 2;
+	u32 udp : 1;
+	u32 sta : 1;		/* static entry */
+} __packed;
+
+struct hnat_info_blk2 {
+	u32 qid : 7;		/* QID in Qos Port */
+	u32 port_mg : 1;
+	u32 fqos : 1;		/* force to PSE QoS port */
+	u32 dp : 4;		/* force to PSE port x */
+	u32 mcast : 1;		/* multicast this packet to CPU */
+	u32 pcpl : 1;		/* OSBN */
+	u32 mibf : 1;
+	u32 alen : 1;
+	u32 rxid : 2;
+	u32 winfoi : 1;
+	u32 port_ag : 4;
+	u32 dscp : 8;		/* DSCP value */
+} __packed;
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+struct hnat_winfo {
+	u32 wcid : 16;		/* WiFi wtable Idx */
+	u32 bssid : 8;		/* WiFi Bssidx */
+	u32 resv : 8;
+} __packed;
+
+struct hnat_winfo_pao {
+	u32 usr_info : 16;
+	u32 tid : 4;
+	u32 is_fixedrate : 1;
+	u32 is_prior : 1;
+	u32 is_sp : 1;
+	u32 hf : 1;
+	u32 amsdu : 1;
+	u32 resv : 7;
+} __packed;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+struct hnat_winfo {
+	u32 bssid : 6;		/* WiFi Bssidx */
+	u32 wcid : 10;		/* WiFi wtable Idx */
+} __packed;
+#endif
+
+#else
+struct hnat_unbind_info_blk {
+	u32 time_stamp : 8;
+	u32 pcnt : 16; /* packet count */
+	u32 preb : 1;
+	u32 pkt_type : 3;
+	u32 state : 2;
+	u32 udp : 1;
+	u32 sta : 1; /* static entry */
+} __packed;
+
+struct hnat_bind_info_blk {
+	u32 time_stamp : 15;
+	u32 ka : 1; /* keep alive */
+	u32 vlan_layer : 3;
+	u32 psn : 1; /* egress packet has PPPoE session */
+	u32 vpm : 1; /* 0:ethertype remark, 1:0x8100(CR default) */
+	u32 ps : 1; /* packet sampling */
+	u32 cah : 1; /* cacheable flag */
+	u32 rmt : 1; /* remove tunnel ip header (6rd/dslite only) */
+	u32 ttl : 1;
+	u32 pkt_type : 3;
+	u32 state : 2;
+	u32 udp : 1;
+	u32 sta : 1; /* static entry */
+} __packed;
+
+struct hnat_info_blk2 {
+	u32 qid : 4; /* QID in Qos Port */
+	u32 fqos : 1; /* force to PSE QoS port */
+	u32 dp : 3; /* force to PSE port x
+		     * 0:PSE,1:GSW, 2:GMAC,4:PPE,5:QDMA,7=DROP
+		     */
+	u32 mcast : 1; /* multicast this packet to CPU */
+	u32 pcpl : 1; /* OSBN */
+	u32 mibf : 1; /* 0:off 1:on PPE MIB counter */
+	u32 alen : 1; /* 0:post 1:pre packet length in accounting */
+	u32 port_mg : 6; /* port meter group */
+	u32 port_ag : 6; /* port account group */
+	u32 dscp : 8; /* DSCP value */
+} __packed;
+
+struct hnat_winfo {
+	u32 bssid : 6;		/* WiFi Bssidx */
+	u32 wcid : 8;		/* WiFi wtable Idx */
+	u32 rxid : 2;		/* WiFi Ring idx */
+} __packed;
+#endif
+
+/* info blk2 for WHNAT */
+struct hnat_info_blk2_whnat {
+	u32 qid : 4; /* QID[3:0] in Qos Port */
+	u32 fqos : 1; /* force to PSE QoS port */
+	u32 dp : 3; /* force to PSE port x
+		     * 0:PSE,1:GSW, 2:GMAC,4:PPE,5:QDMA,7=DROP
+		     */
+	u32 mcast : 1; /* multicast this packet to CPU */
+	u32 pcpl : 1; /* OSBN */
+	u32 mibf : 1; /* 0:off 1:on PPE MIB counter */
+	u32 alen : 1; /* 0:post 1:pre packet length in accounting */
+	u32 qid2 : 2; /* QID[5:4] in Qos Port */
+	u32 resv : 2;
+	u32 wdmaid : 1; /* 0:to pcie0 dev 1:to pcie1 dev */
+	u32 winfoi : 1; /* 0:off 1:on Wi-Fi hwnat support */
+	u32 port_ag : 6; /* port account group */
+	u32 dscp : 8; /* DSCP value */
+} __packed;
+
+struct hnat_ipv4_hnapt {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 sip;
+	u32 dip;
+	u16 dport;
+	u16 sport;
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+	u32 new_sip;
+	u32 new_dip;
+	u16 new_dport;
+	u16 new_sport;
+	u16 m_timestamp; /* For mcast*/
+	u16 resv1;
+	u32 resv2;
+	u32 resv3 : 26;
+	u32 act_dp : 6; /* UDF */
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	union {
+#if !defined(CONFIG_MEDIATEK_NETSYS_V2) && !defined(CONFIG_MEDIATEK_NETSYS_V3)
+		struct hnat_winfo winfo;
+#endif
+		u16 vlan2;
+	};
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv4;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv5 : 2;
+	u32 tport_id : 4;
+	u32 resv6 : 12;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+#endif
+} __packed;
+
+struct hnat_ipv4_dslite {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 sip;
+	u32 dip;
+	u16 dport;
+	u16 sport;
+
+	u32 tunnel_sipv6_0;
+	u32 tunnel_sipv6_1;
+	u32 tunnel_sipv6_2;
+	u32 tunnel_sipv6_3;
+
+	u32 tunnel_dipv6_0;
+	u32 tunnel_dipv6_1;
+	u32 tunnel_dipv6_2;
+	u32 tunnel_dipv6_3;
+
+	u8 flow_lbl[3]; /* in order to consist with Linux kernel (should be 20bits) */
+	u8 priority;    /* in order to consist with Linux kernel (should be 8bits) */
+	u32 hop_limit : 8;
+	u32 resv2 : 18;
+	u32 act_dp : 6; /* UDF */
+
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	union {
+#if !defined(CONFIG_MEDIATEK_NETSYS_V2) && !defined(CONFIG_MEDIATEK_NETSYS_V3)
+		struct hnat_winfo winfo;
+#endif
+		u16 vlan2;
+	};
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv3;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv4 : 2;
+	u32 tport_id : 4;
+	u32 resv5 : 12;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+#endif
+} __packed;
+
+struct hnat_ipv4_mape {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 sip;
+	u32 dip;
+	u16 dport;
+	u16 sport;
+
+	u32 tunnel_sipv6_0;
+	u32 tunnel_sipv6_1;
+	u32 tunnel_sipv6_2;
+	u32 tunnel_sipv6_3;
+
+	u32 tunnel_dipv6_0;
+	u32 tunnel_dipv6_1;
+	u32 tunnel_dipv6_2;
+	u32 tunnel_dipv6_3;
+
+	u8 flow_lbl[3]; /* in order to consist with Linux kernel (should be 20bits) */
+	u8 priority;    /* in order to consist with Linux kernel (should be 8bits) */
+	u32 hop_limit : 8;
+	u32 resv2 : 18;
+	u32 act_dp : 6; /* UDF */
+
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	union {
+#if !defined(CONFIG_MEDIATEK_NETSYS_V2) && !defined(CONFIG_MEDIATEK_NETSYS_V3)
+		struct hnat_winfo winfo;
+#endif
+		u16 vlan2;
+	};
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv3;
+	u32 new_sip;
+	u32 new_dip;
+	u16 new_dport;
+	u16 new_sport;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv4 : 2;
+	u32 tport_id : 4;
+	u32 resv5 : 12;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+	u32 new_sip;
+	u32 new_dip;
+	u16 new_dport;
+	u16 new_sport;
+#endif
+} __packed;
+
+struct hnat_ipv6_3t_route {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 ipv6_sip0;
+	u32 ipv6_sip1;
+	u32 ipv6_sip2;
+	u32 ipv6_sip3;
+	u32 ipv6_dip0;
+	u32 ipv6_dip1;
+	u32 ipv6_dip2;
+	u32 ipv6_dip3;
+	u32 prot : 8;
+	u32 hph : 24; /* hash placeholder */
+
+	u32 resv1;
+	u32 resv2;
+	u32 resv3;
+	u32 resv4 : 26;
+	u32 act_dp : 6; /* UDF */
+
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	union {
+#if !defined(CONFIG_MEDIATEK_NETSYS_V2) && !defined(CONFIG_MEDIATEK_NETSYS_V3)
+		struct hnat_winfo winfo;
+#endif
+		u16 vlan2;
+	};
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv5;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv6 : 2;
+	u32 tport_id : 4;
+	u32 resv7 : 12;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+#endif
+} __packed;
+
+struct hnat_ipv6_5t_route {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 ipv6_sip0;
+	u32 ipv6_sip1;
+	u32 ipv6_sip2;
+	u32 ipv6_sip3;
+	u32 ipv6_dip0;
+	u32 ipv6_dip1;
+	u32 ipv6_dip2;
+	u32 ipv6_dip3;
+	u16 dport;
+	u16 sport;
+
+	u32 resv1;
+	u32 resv2;
+	u32 resv3;
+	u32 resv4 : 26;
+	u32 act_dp : 6; /* UDF */
+
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	union {
+#if !defined(CONFIG_MEDIATEK_NETSYS_V2) && !defined(CONFIG_MEDIATEK_NETSYS_V3)
+		struct hnat_winfo winfo;
+#endif
+		u16 vlan2;
+	};
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv5;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv6 : 2;
+	u32 tport_id : 4;
+	u32 resv7 : 12;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+#endif
+} __packed;
+
+struct hnat_ipv6_6rd {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 ipv6_sip0;
+	u32 ipv6_sip1;
+	u32 ipv6_sip2;
+	u32 ipv6_sip3;
+	u32 ipv6_dip0;
+	u32 ipv6_dip1;
+	u32 ipv6_dip2;
+	u32 ipv6_dip3;
+	u16 dport;
+	u16 sport;
+
+	u32 tunnel_sipv4;
+	u32 tunnel_dipv4;
+	u32 hdr_chksum : 16;
+	u32 dscp : 8;
+	u32 ttl : 8;
+	u32 flag : 3;
+	u32 resv1 : 13;
+	u32 per_flow_6rd_id : 1;
+	u32 resv2 : 9;
+	u32 act_dp : 6; /* UDF */
+
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	union {
+#if !defined(CONFIG_MEDIATEK_NETSYS_V2) && !defined(CONFIG_MEDIATEK_NETSYS_V3)
+		struct hnat_winfo winfo;
+#endif
+		u16 vlan2;
+	};
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv3;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv4 : 2;
+	u32 tport_id : 4;
+	u32 resv5 : 12;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+#endif
+} __packed;
+
+struct hnat_ipv6_hnapt {
+	union {
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_unbind_info_blk udib1;
+		u32 info_blk1;
+	};
+	u32 ipv6_sip0;
+	u32 ipv6_sip1;
+	u32 ipv6_sip2;
+	u32 ipv6_sip3;
+	u32 ipv6_dip0;
+	u32 ipv6_dip1;
+	u32 ipv6_dip2;
+	u32 ipv6_dip3;
+	u16 dport;
+	u16 sport;
+
+	u32 resv1;
+	u32 resv2;
+	u32 resv3;
+	u32 resv4 : 8;
+	u32 eg_ipv6_dir : 1;
+	u32 eg_keep_ecn : 1;
+	u32 eg_keep_cls : 1;
+	u32 resv5 : 15;
+	u32 act_dp : 6; /* UDF */
+
+	union {
+		struct hnat_info_blk2 iblk2;
+		struct hnat_info_blk2_whnat iblk2w;
+		u32 info_blk2;
+	};
+
+	u16 vlan1;
+	u16 etype;
+	u32 dmac_hi;
+	u16 vlan2;
+	u16 dmac_lo;
+	u32 smac_hi;
+	u16 pppoe_id;
+	u16 smac_lo;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u16 minfo;
+	u16 resv6;
+	u32 new_ipv6_ip0;
+	u32 new_ipv6_ip1;
+	u32 new_ipv6_ip2;
+	u32 new_ipv6_ip3;
+	u16 new_dport;
+	u16 new_sport;
+	struct hnat_winfo winfo;
+	struct hnat_winfo_pao winfo_pao;
+	u32 cdrt_id : 8;
+	u32 tops_entry : 6;
+	u32 resv7 : 2;
+	u32 tport_id : 4;
+	u32 resv8 : 12;
+	u32 resv9;
+	u32 resv10;
+	u32 resv11;
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+	u16 minfo;
+	struct hnat_winfo winfo;
+#endif
+} __packed;
+
+struct foe_entry {
+	union {
+		struct hnat_unbind_info_blk udib1;
+		struct hnat_bind_info_blk bfib1;
+		struct hnat_ipv4_hnapt ipv4_hnapt;
+		struct hnat_ipv4_dslite ipv4_dslite;
+		struct hnat_ipv4_mape ipv4_mape;
+		struct hnat_ipv6_3t_route ipv6_3t_route;
+		struct hnat_ipv6_5t_route ipv6_5t_route;
+		struct hnat_ipv6_6rd ipv6_6rd;
+		struct hnat_ipv6_hnapt ipv6_hnapt;
+	};
+};
+
+/* If user wants to change default FOE entry number, both DEF_ETRY_NUM and
+ * DEF_ETRY_NUM_CFG need to be modified.
+ */
+#define DEF_ETRY_NUM		8192
+/* feasible values : 32768, 16384, 8192, 4096, 2048, 1024 */
+#define DEF_ETRY_NUM_CFG	TABLE_8K
+/* corresponding values : TABLE_32K, TABLE_16K, TABLE_8K, TABLE_4K, TABLE_2K,
+ * TABLE_1K
+ */
+#define MAX_EXT_DEVS		(0x3fU)
+#define MAX_IF_NUM		64
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+#define MAX_PPE_NUM		3
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+#define MAX_PPE_NUM		2
+#else
+#define MAX_PPE_NUM		1
+#endif
+#define CFG_PPE_NUM		(hnat_priv->ppe_num)
+
+struct mib_entry {
+	u32 byt_cnt_l;
+	u16 byt_cnt_h;
+	u32 pkt_cnt_l;
+	u8 pkt_cnt_h;
+	u8 resv0;
+	u32 resv1;
+} __packed;
+
+struct hnat_accounting {
+	u64 bytes;
+	u64 packets;
+};
+
+enum mtk_hnat_version {
+	MTK_HNAT_V1_1 = 1,	/* version 1.1: mt7621, mt7623	*/
+	MTK_HNAT_V1_2,		/* version 1.2: mt7622		*/
+	MTK_HNAT_V1_3,		/* version 1.3: mt7629		*/
+	MTK_HNAT_V2,		/* version 2:	mt7981, mt7986	*/
+	MTK_HNAT_V3,		/* version 3:	mt7988		*/
+};
+
+struct mtk_hnat_data {
+	u8 num_of_sch;
+	bool whnat;
+	bool per_flow_accounting;
+	bool mcast;
+	enum mtk_hnat_version version;
+};
+
+struct map46 {
+	u32 ipv4;
+	struct in6_addr ipv6;
+	struct list_head list;
+};
+
+struct xlat_conf {
+	struct list_head map_list;
+	struct in6_addr prefix;
+	int prefix_len;
+};
+
+struct mtk_hnat {
+	struct device *dev;
+	void __iomem *fe_base;
+	void __iomem *ppe_base[MAX_PPE_NUM];
+	struct foe_entry *foe_table_cpu[MAX_PPE_NUM];
+	dma_addr_t foe_table_dev[MAX_PPE_NUM];
+	u8 enable;
+	u8 enable1;
+	struct dentry *root;
+	struct debugfs_regset32 *regset[MAX_PPE_NUM];
+
+	struct mib_entry *foe_mib_cpu[MAX_PPE_NUM];
+	dma_addr_t foe_mib_dev[MAX_PPE_NUM];
+	struct hnat_accounting *acct[MAX_PPE_NUM];
+	const struct mtk_hnat_data *data;
+
+	/*devices we plays for*/
+	char wan[IFNAMSIZ];
+	char lan[IFNAMSIZ];
+	char lan2[IFNAMSIZ];
+	char ppd[IFNAMSIZ];
+	u16 lvid;
+	u16 wvid;
+
+	struct reset_control *rstc;
+
+	u8 ppe_num;
+	u8 gmac_num;
+	u8 wan_dsa_port;
+	struct ppe_mcast_table *pmcast;
+
+	u32 foe_etry_num;
+	u32 etry_num_cfg;
+	struct net_device *g_ppdev;
+	struct net_device *g_wandev;
+	struct net_device *wifi_hook_if[MAX_IF_NUM];
+	struct extdev_entry *ext_if[MAX_EXT_DEVS];
+	struct timer_list hnat_sma_build_entry_timer;
+	struct timer_list hnat_reset_timestamp_timer;
+	struct timer_list hnat_mcast_check_timer;
+	bool nf_stat_en;
+	struct xlat_conf xlat;
+};
+
+struct extdev_entry {
+	char name[IFNAMSIZ];
+	struct net_device *dev;
+};
+
+struct tcpudphdr {
+	__be16 src;
+	__be16 dst;
+};
+
+enum FoeEntryState { INVALID = 0, UNBIND = 1, BIND = 2, FIN = 3 };
+
+enum FoeIpAct {
+	IPV4_HNAPT = 0,
+	IPV4_HNAT = 1,
+	IPV4_DSLITE = 3,
+	IPV6_3T_ROUTE = 4,
+	IPV6_5T_ROUTE = 5,
+	IPV6_6RD = 7,
+	IPV4_MAP_T = 8,
+	IPV4_MAP_E = 9,
+	IPV6_HNAPT = 10,
+	IPV6_HNAT = 11,
+};
+
+/*--------------------------------------------------------------------------*/
+/* Common Definition*/
+/*--------------------------------------------------------------------------*/
+
+#define HNAT_SW_VER   "1.1.0"
+#define HASH_SEED_KEY 0x12345678
+
+/*PPE_TB_CFG value*/
+#define ENTRY_128B 0
+#define ENTRY_96B 1
+#define ENTRY_80B 1
+#define TABLE_1K 0
+#define TABLE_2K 1
+#define TABLE_4K 2
+#define TABLE_8K 3
+#define TABLE_16K 4
+#define TABLE_32K 5
+#define SMA_DROP 0 /* Drop the packet */
+#define SMA_DROP2 1 /* Drop the packet */
+#define SMA_ONLY_FWD_CPU 2 /* Only Forward to CPU */
+#define SMA_FWD_CPU_BUILD_ENTRY 3 /* Forward to CPU and build new FOE entry */
+#define HASH_MODE_0 0
+#define HASH_MODE_1 1
+#define HASH_MODE_2 2
+#define HASH_MODE_3 3
+
+/*PPE_FLOW_CFG*/
+#define BIT_FUC_FOE BIT(2)
+#define BIT_FMC_FOE BIT(1)
+#define BIT_FBC_FOE BIT(0)
+#define BIT_UDP_IP4F_NAT_EN BIT(7) /*Enable IPv4 fragment + UDP packet NAT*/
+#define BIT_IPV6_3T_ROUTE_EN BIT(8)
+#define BIT_IPV6_5T_ROUTE_EN BIT(9)
+#define BIT_IPV6_6RD_EN BIT(10)
+#define BIT_IPV6_464XLAT_EN BIT(11)
+#define BIT_IPV4_NAT_EN BIT(12)
+#define BIT_IPV4_NAPT_EN BIT(13)
+#define BIT_IPV4_DSL_EN BIT(14)
+#define BIT_MIB_BUSY BIT(16)
+#define BIT_IPV4_NAT_FRAG_EN BIT(17)
+#define BIT_IPV4_HASH_GREK BIT(19)
+#define BIT_IPV6_HASH_GREK BIT(20)
+#define BIT_IPV4_MAPE_EN BIT(21)
+#define BIT_IPV4_MAPT_EN BIT(22)
+#define BIT_IPV6_NAT_EN BIT(23)
+#define BIT_IPV6_NAPT_EN BIT(24)
+#define BIT_CS0_RM_ALL_IP6_IP_EN BIT(25)
+
+/*GDMA_FWD_CFG value*/
+#define BITS_GDM_UFRC_P_PPE (NR_PPE0_PORT << 12)
+#define BITS_GDM_BFRC_P_PPE (NR_PPE0_PORT << 8)
+#define BITS_GDM_MFRC_P_PPE (NR_PPE0_PORT << 4)
+#define BITS_GDM_OFRC_P_PPE (NR_PPE0_PORT << 0)
+#define BITS_GDM_ALL_FRC_P_PPE                                              \
+	(BITS_GDM_UFRC_P_PPE | BITS_GDM_BFRC_P_PPE | BITS_GDM_MFRC_P_PPE |  \
+	 BITS_GDM_OFRC_P_PPE)
+
+#define BITS_GDM_UFRC_P_PPE1 (NR_PPE1_PORT << 12)
+#define BITS_GDM_BFRC_P_PPE1 (NR_PPE1_PORT << 8)
+#define BITS_GDM_MFRC_P_PPE1 (NR_PPE1_PORT << 4)
+#define BITS_GDM_OFRC_P_PPE1 (NR_PPE1_PORT << 0)
+#define BITS_GDM_ALL_FRC_P_PPE1					\
+	(BITS_GDM_UFRC_P_PPE1 | BITS_GDM_BFRC_P_PPE1 |		\
+	 BITS_GDM_MFRC_P_PPE1 | BITS_GDM_OFRC_P_PPE1)
+
+#define BITS_GDM_UFRC_P_PPE2 (NR_PPE2_PORT << 12)
+#define BITS_GDM_BFRC_P_PPE2 (NR_PPE2_PORT << 8)
+#define BITS_GDM_MFRC_P_PPE2 (NR_PPE2_PORT << 4)
+#define BITS_GDM_OFRC_P_PPE2 (NR_PPE2_PORT << 0)
+#define BITS_GDM_ALL_FRC_P_PPE2					\
+	(BITS_GDM_UFRC_P_PPE2 | BITS_GDM_BFRC_P_PPE2 |		\
+	 BITS_GDM_MFRC_P_PPE2 | BITS_GDM_OFRC_P_PPE2)
+
+#define BITS_GDM_UFRC_P_CPU_PDMA (NR_PDMA_PORT << 12)
+#define BITS_GDM_BFRC_P_CPU_PDMA (NR_PDMA_PORT << 8)
+#define BITS_GDM_MFRC_P_CPU_PDMA (NR_PDMA_PORT << 4)
+#define BITS_GDM_OFRC_P_CPU_PDMA (NR_PDMA_PORT << 0)
+#define BITS_GDM_ALL_FRC_P_CPU_PDMA                                           \
+	(BITS_GDM_UFRC_P_CPU_PDMA | BITS_GDM_BFRC_P_CPU_PDMA |               \
+	 BITS_GDM_MFRC_P_CPU_PDMA | BITS_GDM_OFRC_P_CPU_PDMA)
+
+#define BITS_GDM_UFRC_P_CPU_QDMA (NR_QDMA_PORT << 12)
+#define BITS_GDM_BFRC_P_CPU_QDMA (NR_QDMA_PORT << 8)
+#define BITS_GDM_MFRC_P_CPU_QDMA (NR_QDMA_PORT << 4)
+#define BITS_GDM_OFRC_P_CPU_QDMA (NR_QDMA_PORT << 0)
+#define BITS_GDM_ALL_FRC_P_CPU_QDMA                                           \
+	(BITS_GDM_UFRC_P_CPU_QDMA | BITS_GDM_BFRC_P_CPU_QDMA |               \
+	 BITS_GDM_MFRC_P_CPU_QDMA | BITS_GDM_OFRC_P_CPU_QDMA)
+
+#define BITS_GDM_UFRC_P_DISCARD (NR_DISCARD << 12)
+#define BITS_GDM_BFRC_P_DISCARD (NR_DISCARD << 8)
+#define BITS_GDM_MFRC_P_DISCARD (NR_DISCARD << 4)
+#define BITS_GDM_OFRC_P_DISCARD (NR_DISCARD << 0)
+#define BITS_GDM_ALL_FRC_P_DISCARD                                            \
+	(BITS_GDM_UFRC_P_DISCARD | BITS_GDM_BFRC_P_DISCARD |                 \
+	 BITS_GDM_MFRC_P_DISCARD | BITS_GDM_OFRC_P_DISCARD)
+
+#define hnat_is_enabled(hnat_priv) (hnat_priv->enable)
+#define hnat_enabled(hnat_priv) (hnat_priv->enable = 1)
+#define hnat_disabled(hnat_priv) (hnat_priv->enable = 0)
+#define hnat_is_enabled1(hnat_priv) (hnat_priv->enable1)
+#define hnat_enabled1(hnat_priv) (hnat_priv->enable1 = 1)
+#define hnat_disabled1(hnat_priv) (hnat_priv->enable1 = 0)
+
+#define entry_hnat_is_bound(e) (e->bfib1.state == BIND)
+#define entry_hnat_state(e) (e->bfib1.state)
+
+#define skb_hnat_is_hashed(skb)                                                \
+	(skb_hnat_entry(skb) != 0x3fff && skb_hnat_entry(skb) < hnat_priv->foe_etry_num)
+#define FROM_GE_LAN_GRP(skb) (FROM_GE_LAN(skb) | FROM_GE_LAN2(skb))
+#define FROM_GE_LAN(skb) (skb_hnat_iface(skb) == FOE_MAGIC_GE_LAN)
+#define FROM_GE_LAN2(skb) (skb_hnat_iface(skb) == FOE_MAGIC_GE_LAN2)
+#define FROM_GE_WAN(skb) (skb_hnat_iface(skb) == FOE_MAGIC_GE_WAN)
+#define FROM_GE_PPD(skb) (skb_hnat_iface(skb) == FOE_MAGIC_GE_PPD)
+#define FROM_GE_VIRTUAL(skb) (skb_hnat_iface(skb) == FOE_MAGIC_GE_VIRTUAL)
+#define FROM_EXT(skb) (skb_hnat_iface(skb) == FOE_MAGIC_EXT)
+#define FROM_WED(skb) ((skb_hnat_iface(skb) == FOE_MAGIC_WED0) ||		\
+		       (skb_hnat_iface(skb) == FOE_MAGIC_WED1) ||		\
+		       (skb_hnat_iface(skb) == FOE_MAGIC_WED2))
+#define FOE_MAGIC_GE_LAN 0x1
+#define FOE_MAGIC_GE_WAN 0x2
+#define FOE_MAGIC_EXT 0x3
+#define FOE_MAGIC_GE_VIRTUAL 0x4
+#define FOE_MAGIC_GE_PPD 0x5
+#define FOE_MAGIC_GE_LAN2 0x6
+#define FOE_MAGIC_WED0 0x78
+#define FOE_MAGIC_WED1 0x79
+#define FOE_MAGIC_WED2 0x7A
+#define FOE_INVALID 0xf
+#define index6b(i) (0x3fU - i)
+
+#define IPV4_HNAPT 0
+#define IPV4_HNAT 1
+#define IP_FORMAT(addr)                                                        \
+	(((unsigned char *)&addr)[3], ((unsigned char *)&addr)[2],              \
+	((unsigned char *)&addr)[1], ((unsigned char *)&addr)[0])
+
+/*PSE Ports*/
+#define NR_PDMA_PORT 0
+#define NR_GMAC1_PORT 1
+#define NR_GMAC2_PORT 2
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+#define NR_WHNAT_WDMA_PORT EINVAL
+#define NR_PPE0_PORT 3
+#define NR_PPE1_PORT 4
+#define NR_PPE2_PORT 0xC
+#else
+#define NR_WHNAT_WDMA_PORT 3
+#define NR_PPE0_PORT 4
+#endif
+#define NR_QDMA_PORT 5
+#define NR_DISCARD 7
+#define NR_WDMA0_PORT 8
+#define NR_WDMA1_PORT 9
+#define NR_WDMA2_PORT 13
+#define NR_GMAC3_PORT 15
+#define LAN_DEV_NAME hnat_priv->lan
+#define LAN2_DEV_NAME hnat_priv->lan2
+#define IS_WAN(dev)                                                            \
+	(!strncmp((dev)->name, hnat_priv->wan, strlen(hnat_priv->wan)))
+#define IS_LAN_GRP(dev) (IS_LAN(dev) | IS_LAN2(dev))
+#define IS_LAN(dev) (!strncmp(dev->name, LAN_DEV_NAME, strlen(LAN_DEV_NAME)))
+#define IS_LAN2(dev) (!strncmp(dev->name, LAN2_DEV_NAME,			\
+		      strlen(LAN2_DEV_NAME)))
+#define IS_BR(dev) (!strncmp(dev->name, "br", 2))
+#define IS_WHNAT(dev)								\
+	((hnat_priv->data->whnat &&						\
+	 (get_wifi_hook_if_index_from_dev(dev) != 0)) ? 1 : 0)
+#define IS_EXT(dev) ((get_index_from_dev(dev) != 0) ? 1 : 0)
+#define IS_PPD(dev) (!strcmp(dev->name, hnat_priv->ppd))
+#define IS_IPV4_HNAPT(x) (((x)->bfib1.pkt_type == IPV4_HNAPT) ? 1 : 0)
+#define IS_IPV4_HNAT(x) (((x)->bfib1.pkt_type == IPV4_HNAT) ? 1 : 0)
+#define IS_IPV4_GRP(x) (IS_IPV4_HNAPT(x) | IS_IPV4_HNAT(x))
+#define IS_IPV4_DSLITE(x) (((x)->bfib1.pkt_type == IPV4_DSLITE) ? 1 : 0)
+#define IS_IPV4_MAPE(x) (((x)->bfib1.pkt_type == IPV4_MAP_E) ? 1 : 0)
+#define IS_IPV4_MAPT(x) (((x)->bfib1.pkt_type == IPV4_MAP_T) ? 1 : 0)
+#define IS_IPV6_3T_ROUTE(x) (((x)->bfib1.pkt_type == IPV6_3T_ROUTE) ? 1 : 0)
+#define IS_IPV6_5T_ROUTE(x) (((x)->bfib1.pkt_type == IPV6_5T_ROUTE) ? 1 : 0)
+#define IS_IPV6_6RD(x) (((x)->bfib1.pkt_type == IPV6_6RD) ? 1 : 0)
+#define IS_IPV6_HNAPT(x) (((x)->bfib1.pkt_type == IPV6_HNAPT) ? 1 : 0)
+#define IS_IPV6_HNAT(x) (((x)->bfib1.pkt_type == IPV6_HNAT) ? 1 : 0)
+#define IS_IPV6_GRP(x)                                                         \
+	(IS_IPV6_3T_ROUTE(x) | IS_IPV6_5T_ROUTE(x) | IS_IPV6_6RD(x) |          \
+	 IS_IPV4_DSLITE(x) | IS_IPV4_MAPE(x) | IS_IPV4_MAPT(x) |	       \
+	 IS_IPV6_HNAPT(x) | IS_IPV6_HNAT(x))
+#define IS_BOND_MODE (!strncmp(LAN_DEV_NAME, "bond", 4))
+#define IS_GMAC1_MODE ((hnat_priv->gmac_num == 1) ? 1 : 0)
+#define IS_HQOS_MODE (qos_toggle == 1)
+#define IS_PPPQ_MODE (qos_toggle == 2)		/* Per Port Per Queue */
+#define IS_PPPQ_PATH(dev, skb) \
+	((IS_DSA_1G_LAN(dev) || IS_DSA_WAN(dev)) || \
+	 (FROM_WED(skb) && IS_DSA_LAN(dev)))
+#define IS_HQOS_DL_MODE (IS_HQOS_MODE && qos_dl_toggle)
+#define IS_HQOS_UL_MODE (IS_HQOS_MODE && qos_ul_toggle)
+#define MAX_PPPQ_PORT_NUM	6
+
+#define es(entry) (entry_state[entry->bfib1.state])
+#define ei(entry, end) (hnat_priv->foe_etry_num - (int)(end - entry))
+#define pt(entry) (packet_type[entry->ipv4_hnapt.bfib1.pkt_type])
+#define ipv4_smac(mac, e)                                                      \
+	({                                                                     \
+		mac[0] = e->ipv4_hnapt.smac_hi[3];                             \
+		mac[1] = e->ipv4_hnapt.smac_hi[2];                             \
+		mac[2] = e->ipv4_hnapt.smac_hi[1];                             \
+		mac[3] = e->ipv4_hnapt.smac_hi[0];                             \
+		mac[4] = e->ipv4_hnapt.smac_lo[1];                             \
+		mac[5] = e->ipv4_hnapt.smac_lo[0];                             \
+	})
+#define ipv4_dmac(mac, e)                                                      \
+	({                                                                     \
+		mac[0] = e->ipv4_hnapt.dmac_hi[3];                             \
+		mac[1] = e->ipv4_hnapt.dmac_hi[2];                             \
+		mac[2] = e->ipv4_hnapt.dmac_hi[1];                             \
+		mac[3] = e->ipv4_hnapt.dmac_hi[0];                             \
+		mac[4] = e->ipv4_hnapt.dmac_lo[1];                             \
+		mac[5] = e->ipv4_hnapt.dmac_lo[0];                             \
+	})
+
+#define IS_DSA_LAN(dev) (!strncmp(dev->name, "lan", 3))
+#define IS_DSA_1G_LAN(dev) (!strncmp(dev->name, "lan", 3) &&		       \
+			    strcmp(dev->name, "lan5"))
+#define IS_DSA_WAN(dev) (!strncmp(dev->name, "wan", 3))
+#define NONE_DSA_PORT 0xff
+#define MAX_CRSN_NUM 32
+#define IPV6_HDR_LEN 40
+#define IPV6_SNAT	0
+#define IPV6_DNAT	1
+
+/*QDMA_PAGE value*/
+#define NUM_OF_Q_PER_PAGE 16
+
+/*IPv6 Header*/
+#ifndef NEXTHDR_IPIP
+#define NEXTHDR_IPIP 4
+#endif
+
+#define UDF_PINGPONG_IFIDX GENMASK(3, 0)
+#define UDF_HNAT_PRE_FILLED BIT(4)
+
+extern const struct of_device_id of_hnat_match[];
+extern struct mtk_hnat *hnat_priv;
+
+static inline int is_hnat_pre_filled(struct foe_entry *entry)
+{
+	u32 udf = 0;
+
+	if (IS_IPV4_GRP(entry))
+		udf = entry->ipv4_hnapt.act_dp;
+	else
+		udf = entry->ipv6_5t_route.act_dp;
+
+	return !!(udf & UDF_HNAT_PRE_FILLED);
+}
+
+#if defined(CONFIG_NET_DSA_MT7530)
+u32 hnat_dsa_fill_stag(const struct net_device *netdev,
+		       struct foe_entry *entry,
+		       struct flow_offload_hw_path *hw_path,
+		       u16 eth_proto, int mape);
+
+static inline bool hnat_dsa_is_enable(struct mtk_hnat *priv)
+{
+	return (priv->wan_dsa_port != NONE_DSA_PORT);
+}
+#else
+static inline u32 hnat_dsa_fill_stag(const struct net_device *netdev,
+				     struct foe_entry *entry,
+				     struct flow_offload_hw_path *hw_path,
+				     u16 eth_proto, int mape)
+{
+	return 0;
+}
+
+static inline bool hnat_dsa_is_enable(struct mtk_hnat *priv)
+{
+	return false;
+}
+#endif
+
+void hnat_deinit_debugfs(struct mtk_hnat *h);
+int hnat_init_debugfs(struct mtk_hnat *h);
+int hnat_register_nf_hooks(void);
+void hnat_unregister_nf_hooks(void);
+int whnat_adjust_nf_hooks(void);
+int mtk_hqos_ptype_cb(struct sk_buff *skb, struct net_device *dev,
+		      struct packet_type *pt, struct net_device *unused);
+extern int dbg_cpu_reason;
+extern int debug_level;
+extern int xlat_toggle;
+extern struct hnat_desc headroom[DEF_ETRY_NUM];
+extern int qos_dl_toggle;
+extern int qos_ul_toggle;
+extern int hook_toggle;
+extern int mape_toggle;
+extern int qos_toggle;
+
+int ext_if_add(struct extdev_entry *ext_entry);
+int ext_if_del(struct extdev_entry *ext_entry);
+void cr_set_field(void __iomem *reg, u32 field, u32 val);
+int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no);
+int mtk_sw_nat_hook_rx(struct sk_buff *skb);
+void foe_clear_all_bind_entries(void);
+void mtk_ppe_dev_register_hook(struct net_device *dev);
+void mtk_ppe_dev_unregister_hook(struct net_device *dev);
+int nf_hnat_netdevice_event(struct notifier_block *unused, unsigned long event,
+			    void *ptr);
+int nf_hnat_netevent_handler(struct notifier_block *unused, unsigned long event,
+			     void *ptr);
+uint32_t foe_dump_pkt(struct sk_buff *skb);
+uint32_t hnat_cpu_reason_cnt(struct sk_buff *skb);
+int hnat_enable_hook(void);
+int hnat_disable_hook(void);
+void hnat_cache_ebl(int enable);
+void hnat_qos_shaper_ebl(u32 id, u32 enable);
+void set_gmac_ppe_fwd(int gmac_no, int enable);
+int entry_detail(u32 ppe_id, int index);
+int entry_delete_by_mac(u8 *mac);
+int entry_delete(u32 ppe_id, int index);
+int hnat_warm_init(void);
+u32 hnat_get_ppe_hash(struct foe_entry *entry);
+int mtk_ppe_get_xlat_v4_by_v6(struct in6_addr *ipv6, u32 *ipv4);
+int mtk_ppe_get_xlat_v6_by_v4(u32 *ipv4, struct in6_addr *ipv6,
+			      struct in6_addr *prefix);
+
+struct hnat_accounting *hnat_get_count(struct mtk_hnat *h, u32 ppe_id,
+				       u32 index, struct hnat_accounting *diff);
+
+static inline u16 foe_timestamp(struct mtk_hnat *h)
+{
+	return (readl(hnat_priv->fe_base + 0x0010)) & 0xffff;
+}
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_debugfs.c b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_debugfs.c
new file mode 100644
index 00000000..f6483869
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_debugfs.c
@@ -0,0 +1,3233 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Sean Wang <sean.wang@mediatek.com>
+ *   Copyright (C) 2016-2017 John Crispin <blogic@openwrt.org>
+ */
+
+#include <linux/kernel.h>
+#include <linux/slab.h>
+#include <linux/dma-mapping.h>
+#include <linux/netdevice.h>
+#include <linux/iopoll.h>
+#include <linux/inet.h>
+#include <net/ipv6.h>
+
+#include "hnat.h"
+#include "nf_hnat_mtk.h"
+#include "../mtk_eth_soc.h"
+
+int dbg_entry_state = BIND;
+typedef int (*debugfs_write_func)(int par1);
+int debug_level;
+int dbg_cpu_reason;
+int hook_toggle;
+int mape_toggle;
+int qos_toggle;
+int qos_dl_toggle = 1;
+int qos_ul_toggle = 1;
+int xlat_toggle;
+struct hnat_desc headroom[DEF_ETRY_NUM];
+unsigned int dbg_cpu_reason_cnt[MAX_CRSN_NUM];
+
+static const char * const entry_state[] = { "INVALID", "UNBIND", "BIND", "FIN" };
+
+static const char * const packet_type[] = {
+	"IPV4_HNAPT",    "IPV4_HNAT",     "IPV6_1T_ROUTE", "IPV4_DSLITE",
+	"IPV6_3T_ROUTE", "IPV6_5T_ROUTE", "REV",	   "IPV6_6RD",
+	"IPV4_MAP_T",    "IPV4_MAP_E",    "IPV6_HNAPT",    "IPV6_HNAT",
+};
+
+static uint8_t *show_cpu_reason(struct sk_buff *skb)
+{
+	static u8 buf[32];
+	int ret;
+
+	switch (skb_hnat_reason(skb)) {
+	case TTL_0:
+		return "IPv4(IPv6) TTL(hop limit)\n";
+	case HAS_OPTION_HEADER:
+		return "Ipv4(IPv6) has option(extension) header\n";
+	case NO_FLOW_IS_ASSIGNED:
+		return "No flow is assigned\n";
+	case IPV4_WITH_FRAGMENT:
+		return "IPv4 HNAT doesn't support IPv4 /w fragment\n";
+	case IPV4_HNAPT_DSLITE_WITH_FRAGMENT:
+		return "IPv4 HNAPT/DS-Lite doesn't support IPv4 /w fragment\n";
+	case IPV4_HNAPT_DSLITE_WITHOUT_TCP_UDP:
+		return "IPv4 HNAPT/DS-Lite can't find TCP/UDP sport/dport\n";
+	case IPV6_5T_6RD_WITHOUT_TCP_UDP:
+		return "IPv6 5T-route/6RD can't find TCP/UDP sport/dport\n";
+	case TCP_FIN_SYN_RST:
+		return "Ingress packet is TCP fin/syn/rst\n";
+	case UN_HIT:
+		return "FOE Un-hit\n";
+	case HIT_UNBIND:
+		return "FOE Hit unbind\n";
+	case HIT_UNBIND_RATE_REACH:
+		return "FOE Hit unbind & rate reach\n";
+	case HIT_BIND_TCP_FIN:
+		return "Hit bind PPE TCP FIN entry\n";
+	case HIT_BIND_TTL_1:
+		return "Hit bind PPE entry and TTL(hop limit) = 1 and TTL(hot limit) - 1\n";
+	case HIT_BIND_WITH_VLAN_VIOLATION:
+		return "Hit bind and VLAN replacement violation\n";
+	case HIT_BIND_KEEPALIVE_UC_OLD_HDR:
+		return "Hit bind and keep alive with unicast old-header packet\n";
+	case HIT_BIND_KEEPALIVE_MC_NEW_HDR:
+		return "Hit bind and keep alive with multicast new-header packet\n";
+	case HIT_BIND_KEEPALIVE_DUP_OLD_HDR:
+		return "Hit bind and keep alive with duplicate old-header packet\n";
+	case HIT_BIND_FORCE_TO_CPU:
+		return "FOE Hit bind & force to CPU\n";
+	case HIT_BIND_EXCEED_MTU:
+		return "Hit bind and exceed MTU\n";
+	case HIT_BIND_MULTICAST_TO_CPU:
+		return "Hit bind multicast packet to CPU\n";
+	case HIT_BIND_MULTICAST_TO_GMAC_CPU:
+		return "Hit bind multicast packet to GMAC & CPU\n";
+	case HIT_PRE_BIND:
+		return "Pre bind\n";
+	}
+
+	ret = snprintf(buf, sizeof(buf), "CPU Reason Error - %X\n",
+		       skb_hnat_entry(skb));
+	if (ret == strlen(buf))
+		return buf;
+	else
+		return "CPU Reason Error\n";
+}
+
+uint32_t foe_dump_pkt(struct sk_buff *skb)
+{
+	struct foe_entry *entry;
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return 1;
+
+	entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+	pr_info("\nRx===<FOE_Entry=%d>=====\n", skb_hnat_entry(skb));
+	pr_info("RcvIF=%s\n", skb->dev->name);
+	pr_info("PPE_ID=%d\n", skb_hnat_ppe(skb));
+	pr_info("FOE_Entry=%d\n", skb_hnat_entry(skb));
+	pr_info("CPU Reason=%s", show_cpu_reason(skb));
+	pr_info("ALG=%d\n", skb_hnat_alg(skb));
+	pr_info("SP=%d\n", skb_hnat_sport(skb));
+
+	/* some special alert occurred, so entry_num is useless (just skip it) */
+	if (skb_hnat_entry(skb) == 0x3fff)
+		return 1;
+
+	/* PPE: IPv4 packet=IPV4_HNAT IPv6 packet=IPV6_ROUTE */
+	if (IS_IPV4_GRP(entry)) {
+		__be32 saddr = htonl(entry->ipv4_hnapt.sip);
+		__be32 daddr = htonl(entry->ipv4_hnapt.dip);
+
+		pr_info("Information Block 1=%x\n",
+			entry->ipv4_hnapt.info_blk1);
+		pr_info("SIP=%pI4\n", &saddr);
+		pr_info("DIP=%pI4\n", &daddr);
+		pr_info("SPORT=%d\n", entry->ipv4_hnapt.sport);
+		pr_info("DPORT=%d\n", entry->ipv4_hnapt.dport);
+		pr_info("Information Block 2=%x\n",
+			entry->ipv4_hnapt.info_blk2);
+		pr_info("State = %s, proto = %s\n", entry->bfib1.state == 0 ?
+			"Invalid" : entry->bfib1.state == 1 ?
+			"Unbind" : entry->bfib1.state == 2 ?
+			"BIND" : entry->bfib1.state == 3 ?
+			"FIN" : "Unknown",
+			entry->ipv4_hnapt.bfib1.udp == 0 ?
+			"TCP" : entry->ipv4_hnapt.bfib1.udp == 1 ?
+			"UDP" : "Unknown");
+	} else if (IS_IPV6_GRP(entry)) {
+		pr_info("Information Block 1=%x\n",
+			entry->ipv6_5t_route.info_blk1);
+		pr_info("IPv6_SIP=%08X:%08X:%08X:%08X\n",
+			entry->ipv6_5t_route.ipv6_sip0,
+			entry->ipv6_5t_route.ipv6_sip1,
+			entry->ipv6_5t_route.ipv6_sip2,
+			entry->ipv6_5t_route.ipv6_sip3);
+		pr_info("IPv6_DIP=%08X:%08X:%08X:%08X\n",
+			entry->ipv6_5t_route.ipv6_dip0,
+			entry->ipv6_5t_route.ipv6_dip1,
+			entry->ipv6_5t_route.ipv6_dip2,
+			entry->ipv6_5t_route.ipv6_dip3);
+		pr_info("SPORT=%d\n", entry->ipv6_5t_route.sport);
+		pr_info("DPORT=%d\n", entry->ipv6_5t_route.dport);
+		pr_info("Information Block 2=%x\n",
+			entry->ipv6_5t_route.info_blk2);
+		pr_info("State = %s, proto = %s\n", entry->bfib1.state == 0 ?
+			"Invalid" : entry->bfib1.state == 1 ?
+			"Unbind" : entry->bfib1.state == 2 ?
+			"BIND" : entry->bfib1.state == 3 ?
+			"FIN" : "Unknown",
+			entry->ipv6_5t_route.bfib1.udp == 0 ?
+			"TCP" : entry->ipv6_5t_route.bfib1.udp == 1 ?
+			"UDP" :	"Unknown");
+	} else {
+		pr_info("unknown Pkt_type=%d\n", entry->bfib1.pkt_type);
+	}
+
+	pr_info("==================================\n");
+	return 1;
+}
+
+uint32_t hnat_cpu_reason_cnt(struct sk_buff *skb)
+{
+	switch (skb_hnat_reason(skb)) {
+	case TTL_0:
+		dbg_cpu_reason_cnt[0]++;
+		return 0;
+	case HAS_OPTION_HEADER:
+		dbg_cpu_reason_cnt[1]++;
+		return 0;
+	case NO_FLOW_IS_ASSIGNED:
+		dbg_cpu_reason_cnt[2]++;
+		return 0;
+	case IPV4_WITH_FRAGMENT:
+		dbg_cpu_reason_cnt[3]++;
+		return 0;
+	case IPV4_HNAPT_DSLITE_WITH_FRAGMENT:
+		dbg_cpu_reason_cnt[4]++;
+		return 0;
+	case IPV4_HNAPT_DSLITE_WITHOUT_TCP_UDP:
+		dbg_cpu_reason_cnt[5]++;
+		return 0;
+	case IPV6_5T_6RD_WITHOUT_TCP_UDP:
+		dbg_cpu_reason_cnt[6]++;
+		return 0;
+	case TCP_FIN_SYN_RST:
+		dbg_cpu_reason_cnt[7]++;
+		return 0;
+	case UN_HIT:
+		dbg_cpu_reason_cnt[8]++;
+		return 0;
+	case HIT_UNBIND:
+		dbg_cpu_reason_cnt[9]++;
+		return 0;
+	case HIT_UNBIND_RATE_REACH:
+		dbg_cpu_reason_cnt[10]++;
+		return 0;
+	case HIT_BIND_TCP_FIN:
+		dbg_cpu_reason_cnt[11]++;
+		return 0;
+	case HIT_BIND_TTL_1:
+		dbg_cpu_reason_cnt[12]++;
+		return 0;
+	case HIT_BIND_WITH_VLAN_VIOLATION:
+		dbg_cpu_reason_cnt[13]++;
+		return 0;
+	case HIT_BIND_KEEPALIVE_UC_OLD_HDR:
+		dbg_cpu_reason_cnt[14]++;
+		return 0;
+	case HIT_BIND_KEEPALIVE_MC_NEW_HDR:
+		dbg_cpu_reason_cnt[15]++;
+		return 0;
+	case HIT_BIND_KEEPALIVE_DUP_OLD_HDR:
+		dbg_cpu_reason_cnt[16]++;
+		return 0;
+	case HIT_BIND_FORCE_TO_CPU:
+		dbg_cpu_reason_cnt[17]++;
+		return 0;
+	case HIT_BIND_EXCEED_MTU:
+		dbg_cpu_reason_cnt[18]++;
+		return 0;
+	case HIT_BIND_MULTICAST_TO_CPU:
+		dbg_cpu_reason_cnt[19]++;
+		return 0;
+	case HIT_BIND_MULTICAST_TO_GMAC_CPU:
+		dbg_cpu_reason_cnt[20]++;
+		return 0;
+	case HIT_PRE_BIND:
+		dbg_cpu_reason_cnt[21]++;
+		return 0;
+	}
+
+	return 0;
+}
+
+int hnat_set_usage(int level)
+{
+	debug_level = level;
+	pr_info("Read cpu_reason count: cat /sys/kernel/debug/hnat/cpu_reason\n\n");
+	pr_info("====================Advanced Settings====================\n");
+	pr_info("Usage: echo [type] [option] > /sys/kernel/debug/hnat/cpu_reason\n\n");
+	pr_info("Commands:   [type] [option]\n");
+	pr_info("              0       0~7      Set debug_level(0~7), current debug_level=%d\n",
+		debug_level);
+	pr_info("              1    cpu_reason  Track entries of the set cpu_reason\n");
+	pr_info("                               Set type=1 will change debug_level=7\n");
+	pr_info("cpu_reason list:\n");
+	pr_info("                       2       IPv4(IPv6) TTL(hop limit) = 0\n");
+	pr_info("                       3       IPv4(IPv6) has option(extension) header\n");
+	pr_info("                       7       No flow is assigned\n");
+	pr_info("                       8       IPv4 HNAT doesn't support IPv4 /w fragment\n");
+	pr_info("                       9       IPv4 HNAPT/DS-Lite doesn't support IPv4 /w fragment\n");
+	pr_info("                      10       IPv4 HNAPT/DS-Lite can't find TCP/UDP sport/dport\n");
+	pr_info("                      11       IPv6 5T-route/6RD can't find TCP/UDP sport/dport\n");
+	pr_info("                      12       Ingress packet is TCP fin/syn/rst\n");
+	pr_info("                      13       FOE Un-hit\n");
+	pr_info("                      14       FOE Hit unbind\n");
+	pr_info("                      15       FOE Hit unbind & rate reach\n");
+	pr_info("                      16       Hit bind PPE TCP FIN entry\n");
+	pr_info("                      17       Hit bind PPE entry and TTL(hop limit) = 1\n");
+	pr_info("                      18       Hit bind and VLAN replacement violation\n");
+	pr_info("                      19       Hit bind and keep alive with unicast old-header packet\n");
+	pr_info("                      20       Hit bind and keep alive with multicast new-header packet\n");
+	pr_info("                      21       Hit bind and keep alive with duplicate old-header packet\n");
+	pr_info("                      22       FOE Hit bind & force to CPU\n");
+	pr_info("                      23       HIT_BIND_WITH_OPTION_HEADER\n");
+	pr_info("                      24       Switch clone multicast packet to CPU\n");
+	pr_info("                      25       Switch clone multicast packet to GMAC1 & CPU\n");
+	pr_info("                      26       HIT_PRE_BIND\n");
+	pr_info("                      27       HIT_BIND_PACKET_SAMPLING\n");
+	pr_info("                      28       Hit bind and exceed MTU\n");
+
+	return 0;
+}
+
+int hnat_cpu_reason(int cpu_reason)
+{
+	dbg_cpu_reason = cpu_reason;
+	debug_level = 7;
+	pr_info("show cpu reason = %d\n", cpu_reason);
+
+	return 0;
+}
+
+int entry_set_usage(int level)
+{
+	debug_level = level;
+	pr_info("Show all entries(default state=bind): cat /sys/kernel/debug/hnat/hnat_entry\n\n");
+	pr_info("====================Advanced Settings====================\n");
+	pr_info("Usage: echo [type] [option] > /sys/kernel/debug/hnat/hnat_entry\n\n");
+	pr_info("Commands:   [type] [option]\n");
+	pr_info("              0       0~7      Set debug_level(0~7), current debug_level=%d\n",
+		debug_level);
+	pr_info("              1       0~3      Change tracking state\n");
+	pr_info("                               (0:invalid; 1:unbind; 2:bind; 3:fin)\n");
+	pr_info("              2   <entry_idx>  Show PPE0 specific foe entry info. of assigned <entry_idx>\n");
+	pr_info("              3   <entry_idx>  Delete PPE0 specific foe entry of assigned <entry_idx>\n");
+	pr_info("              4   <entry_idx>  Show PPE1 specific foe entry info. of assigned <entry_idx>\n");
+	pr_info("              5   <entry_idx>  Delete PPE1 specific foe entry of assigned <entry_idx>\n");
+	pr_info("                               When entry_idx is -1, clear all entries\n");
+
+	return 0;
+}
+
+int entry_set_state(int state)
+{
+	dbg_entry_state = state;
+	pr_info("ENTRY STATE = %s\n", dbg_entry_state == 0 ?
+		"Invalid" : dbg_entry_state == 1 ?
+		"Unbind" : dbg_entry_state == 2 ?
+		"BIND" : dbg_entry_state == 3 ?
+		"FIN" : "Unknown");
+	return 0;
+}
+
+int wrapped_ppe0_entry_detail(int index) {
+	entry_detail(0, index);
+	return 0;
+}
+
+int wrapped_ppe1_entry_detail(int index) {
+	entry_detail(1, index);
+	return 0;
+}
+
+int entry_detail(u32 ppe_id, int index)
+{
+	struct foe_entry *entry;
+	struct mtk_hnat *h = hnat_priv;
+	u32 *p;
+	u32 i = 0;
+	u32 print_cnt;
+	unsigned char h_dest[ETH_ALEN];
+	unsigned char h_source[ETH_ALEN];
+	__be32 saddr, daddr, nsaddr, ndaddr;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	if (index < 0 || index >= h->foe_etry_num) {
+		pr_info("Invalid entry index\n");
+		return -EINVAL;
+	}
+
+	entry = h->foe_table_cpu[ppe_id] + index;
+	saddr = htonl(entry->ipv4_hnapt.sip);
+	daddr = htonl(entry->ipv4_hnapt.dip);
+	nsaddr = htonl(entry->ipv4_hnapt.new_sip);
+	ndaddr = htonl(entry->ipv4_hnapt.new_dip);
+	p = (uint32_t *)entry;
+	pr_info("==========<PPE_ID=%d, Flow Table Entry=%d (%p)>===============\n",
+		ppe_id, index, entry);
+	if (debug_level >= 2) {
+		if (hnat_priv->data->version == MTK_HNAT_V3)
+			print_cnt = 28;
+		else
+			print_cnt = 20;
+
+		for (i = 0; i < print_cnt; i++)
+			pr_info("%02d: %08X\n", i, *(p + i));
+	}
+	pr_info("-----------------<Flow Info>------------------\n");
+	pr_info("Information Block 1: %08X\n", entry->ipv4_hnapt.info_blk1);
+
+	if (IS_IPV4_HNAPT(entry)) {
+		pr_info("Information Block 2: %08X (FP=%d FQOS=%d QID=%d)",
+			entry->ipv4_hnapt.info_blk2,
+			entry->ipv4_hnapt.iblk2.dp,
+			entry->ipv4_hnapt.iblk2.fqos,
+			entry->ipv4_hnapt.iblk2.qid);
+		pr_info("Create IPv4 HNAPT entry\n");
+		pr_info("IPv4 Org IP/Port: %pI4:%d->%pI4:%d\n", &saddr,
+			entry->ipv4_hnapt.sport, &daddr,
+			entry->ipv4_hnapt.dport);
+		pr_info("IPv4 New IP/Port: %pI4:%d->%pI4:%d\n", &nsaddr,
+			entry->ipv4_hnapt.new_sport, &ndaddr,
+			entry->ipv4_hnapt.new_dport);
+	} else if (IS_IPV4_HNAT(entry)) {
+		pr_info("Information Block 2: %08X\n",
+			entry->ipv4_hnapt.info_blk2);
+		pr_info("Create IPv4 HNAT entry\n");
+		pr_info("IPv4 Org IP: %pI4->%pI4\n", &saddr, &daddr);
+		pr_info("IPv4 New IP: %pI4->%pI4\n", &nsaddr, &ndaddr);
+	} else if (IS_IPV4_DSLITE(entry)) {
+		pr_info("Information Block 2: %08X (FP=%d FQOS=%d QID=%d)",
+			entry->ipv4_dslite.info_blk2,
+			entry->ipv4_dslite.iblk2.dp,
+			entry->ipv4_dslite.iblk2.fqos,
+			entry->ipv4_dslite.iblk2.qid);
+		pr_info("Create IPv4 Ds-Lite entry\n");
+		pr_info("IPv4 Ds-Lite: %pI4:%d->%pI4:%d\n", &saddr,
+			entry->ipv4_dslite.sport, &daddr,
+			entry->ipv4_dslite.dport);
+		pr_info("EG DIPv6: %08X:%08X:%08X:%08X->%08X:%08X:%08X:%08X\n",
+			entry->ipv4_dslite.tunnel_sipv6_0,
+			entry->ipv4_dslite.tunnel_sipv6_1,
+			entry->ipv4_dslite.tunnel_sipv6_2,
+			entry->ipv4_dslite.tunnel_sipv6_3,
+			entry->ipv4_dslite.tunnel_dipv6_0,
+			entry->ipv4_dslite.tunnel_dipv6_1,
+			entry->ipv4_dslite.tunnel_dipv6_2,
+			entry->ipv4_dslite.tunnel_dipv6_3);
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+	} else if (IS_IPV4_MAPE(entry)) {
+		nsaddr = htonl(entry->ipv4_mape.new_sip);
+		ndaddr = htonl(entry->ipv4_mape.new_dip);
+
+		pr_info("Information Block 2: %08X\n",
+			entry->ipv4_dslite.info_blk2);
+		pr_info("Create IPv4 MAP-E entry\n");
+		pr_info("IPv4 MAP-E Org IP/Port: %pI4:%d->%pI4:%d\n",
+			&saddr,	entry->ipv4_dslite.sport,
+			&daddr,	entry->ipv4_dslite.dport);
+		pr_info("IPv4 MAP-E New IP/Port: %pI4:%d->%pI4:%d\n",
+			&nsaddr, entry->ipv4_mape.new_sport,
+			&ndaddr, entry->ipv4_mape.new_dport);
+		pr_info("EG DIPv6: %08X:%08X:%08X:%08X->%08X:%08X:%08X:%08X\n",
+			entry->ipv4_dslite.tunnel_sipv6_0,
+			entry->ipv4_dslite.tunnel_sipv6_1,
+			entry->ipv4_dslite.tunnel_sipv6_2,
+			entry->ipv4_dslite.tunnel_sipv6_3,
+			entry->ipv4_dslite.tunnel_dipv6_0,
+			entry->ipv4_dslite.tunnel_dipv6_1,
+			entry->ipv4_dslite.tunnel_dipv6_2,
+			entry->ipv4_dslite.tunnel_dipv6_3);
+#endif
+	} else if (IS_IPV6_3T_ROUTE(entry)) {
+		pr_info("Information Block 2: %08X\n",
+			entry->ipv6_3t_route.info_blk2);
+		pr_info("Create IPv6 3-Tuple entry\n");
+		pr_info("ING SIPv6->DIPv6: %08X:%08X:%08X:%08X-> %08X:%08X:%08X:%08X (Prot=%d)\n",
+			entry->ipv6_3t_route.ipv6_sip0,
+			entry->ipv6_3t_route.ipv6_sip1,
+			entry->ipv6_3t_route.ipv6_sip2,
+			entry->ipv6_3t_route.ipv6_sip3,
+			entry->ipv6_3t_route.ipv6_dip0,
+			entry->ipv6_3t_route.ipv6_dip1,
+			entry->ipv6_3t_route.ipv6_dip2,
+			entry->ipv6_3t_route.ipv6_dip3,
+			entry->ipv6_3t_route.prot);
+	} else if (IS_IPV6_5T_ROUTE(entry)) {
+		pr_info("Information Block 2: %08X\n",
+			entry->ipv6_5t_route.info_blk2);
+		pr_info("Create IPv6 5-Tuple entry\n");
+		pr_info("ING SIPv6->DIPv6: %08X:%08X:%08X:%08X:%d-> %08X:%08X:%08X:%08X:%d\n",
+			entry->ipv6_5t_route.ipv6_sip0,
+			entry->ipv6_5t_route.ipv6_sip1,
+			entry->ipv6_5t_route.ipv6_sip2,
+			entry->ipv6_5t_route.ipv6_sip3,
+			entry->ipv6_5t_route.sport,
+			entry->ipv6_5t_route.ipv6_dip0,
+			entry->ipv6_5t_route.ipv6_dip1,
+			entry->ipv6_5t_route.ipv6_dip2,
+			entry->ipv6_5t_route.ipv6_dip3,
+			entry->ipv6_5t_route.dport);
+	} else if (IS_IPV6_6RD(entry)) {
+		pr_info("Information Block 2: %08X (FP=%d FQOS=%d QID=%d)",
+			entry->ipv6_6rd.info_blk2,
+			entry->ipv6_6rd.iblk2.dp,
+			entry->ipv6_6rd.iblk2.fqos,
+			entry->ipv6_6rd.iblk2.qid);
+		pr_info("Create IPv6 6RD entry\n");
+		pr_info("ING SIPv6->DIPv6: %08X:%08X:%08X:%08X:%d-> %08X:%08X:%08X:%08X:%d\n",
+			entry->ipv6_6rd.ipv6_sip0, entry->ipv6_6rd.ipv6_sip1,
+			entry->ipv6_6rd.ipv6_sip2, entry->ipv6_6rd.ipv6_sip3,
+			entry->ipv6_6rd.sport, entry->ipv6_6rd.ipv6_dip0,
+			entry->ipv6_6rd.ipv6_dip1, entry->ipv6_6rd.ipv6_dip2,
+			entry->ipv6_6rd.ipv6_dip3, entry->ipv6_6rd.dport);
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	} else if (IS_IPV6_HNAPT(entry)) {
+		pr_info("Information Block 2: %08X (FP=%d FQOS=%d QID=%d)",
+			entry->ipv6_hnapt.info_blk2,
+			entry->ipv6_hnapt.iblk2.dp,
+			entry->ipv6_hnapt.iblk2.fqos,
+			entry->ipv6_hnapt.iblk2.qid);
+		pr_info("Create IPv6 HNAPT entry\n");
+		pr_info("IPv6 Org IP/Port: %08X:%08X:%08X:%08X:%d -> %08X:%08X:%08X:%08X:%d",
+			entry->ipv6_hnapt.ipv6_sip0,
+			entry->ipv6_hnapt.ipv6_sip1,
+			entry->ipv6_hnapt.ipv6_sip2,
+			entry->ipv6_hnapt.ipv6_sip3,
+			entry->ipv6_hnapt.sport,
+			entry->ipv6_hnapt.ipv6_dip0,
+			entry->ipv6_hnapt.ipv6_dip1,
+			entry->ipv6_hnapt.ipv6_dip2,
+			entry->ipv6_hnapt.ipv6_dip3,
+			entry->ipv6_hnapt.dport);
+
+		if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_SNAT) {
+			pr_info("IPv6 New IP/Port: %08X:%08X:%08X:%08X:%d -> %08X:%08X:%08X:%08X:%d\n",
+				entry->ipv6_hnapt.new_ipv6_ip0,
+				entry->ipv6_hnapt.new_ipv6_ip1,
+				entry->ipv6_hnapt.new_ipv6_ip2,
+				entry->ipv6_hnapt.new_ipv6_ip3,
+				entry->ipv6_hnapt.new_sport,
+				entry->ipv6_hnapt.ipv6_dip0,
+				entry->ipv6_hnapt.ipv6_dip1,
+				entry->ipv6_hnapt.ipv6_dip2,
+				entry->ipv6_hnapt.ipv6_dip3,
+				entry->ipv6_hnapt.new_dport);
+		} else if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_DNAT) {
+			pr_info("IPv6 New IP/Port: %08X:%08X:%08X:%08X:%d -> %08X:%08X:%08X:%08X:%d\n",
+				entry->ipv6_hnapt.ipv6_sip0,
+				entry->ipv6_hnapt.ipv6_sip1,
+				entry->ipv6_hnapt.ipv6_sip2,
+				entry->ipv6_hnapt.ipv6_sip3,
+				entry->ipv6_hnapt.new_sport,
+				entry->ipv6_hnapt.new_ipv6_ip0,
+				entry->ipv6_hnapt.new_ipv6_ip1,
+				entry->ipv6_hnapt.new_ipv6_ip2,
+				entry->ipv6_hnapt.new_ipv6_ip3,
+				entry->ipv6_hnapt.new_dport);
+		}
+	} else if (IS_IPV6_HNAT(entry)) {
+		pr_info("Information Block 2: %08X (FP=%d FQOS=%d QID=%d)",
+			entry->ipv6_hnapt.info_blk2,
+			entry->ipv6_hnapt.iblk2.dp,
+			entry->ipv6_hnapt.iblk2.fqos,
+			entry->ipv6_hnapt.iblk2.qid);
+		pr_info("Create IPv6 HNAT entry\n");
+		pr_info("IPv6 Org IP: %08X:%08X:%08X:%08X -> %08X:%08X:%08X:%08X",
+			entry->ipv6_hnapt.ipv6_sip0,
+			entry->ipv6_hnapt.ipv6_sip1,
+			entry->ipv6_hnapt.ipv6_sip2,
+			entry->ipv6_hnapt.ipv6_sip3,
+			entry->ipv6_hnapt.ipv6_dip0,
+			entry->ipv6_hnapt.ipv6_dip1,
+			entry->ipv6_hnapt.ipv6_dip2,
+			entry->ipv6_hnapt.ipv6_dip3);
+
+		if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_SNAT) {
+			pr_info("IPv6 New IP: %08X:%08X:%08X:%08X -> %08X:%08X:%08X:%08X\n",
+				entry->ipv6_hnapt.new_ipv6_ip0,
+				entry->ipv6_hnapt.new_ipv6_ip1,
+				entry->ipv6_hnapt.new_ipv6_ip2,
+				entry->ipv6_hnapt.new_ipv6_ip3,
+				entry->ipv6_hnapt.ipv6_dip0,
+				entry->ipv6_hnapt.ipv6_dip1,
+				entry->ipv6_hnapt.ipv6_dip2,
+				entry->ipv6_hnapt.ipv6_dip3);
+		} else if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_DNAT) {
+			pr_info("IPv6 New IP: %08X:%08X:%08X:%08X -> %08X:%08X:%08X:%08X\n",
+				entry->ipv6_hnapt.ipv6_sip0,
+				entry->ipv6_hnapt.ipv6_sip1,
+				entry->ipv6_hnapt.ipv6_sip2,
+				entry->ipv6_hnapt.ipv6_sip3,
+				entry->ipv6_hnapt.new_ipv6_ip0,
+				entry->ipv6_hnapt.new_ipv6_ip1,
+				entry->ipv6_hnapt.new_ipv6_ip2,
+				entry->ipv6_hnapt.new_ipv6_ip3);
+		}
+#endif
+	}
+
+	if (IS_IPV4_HNAPT(entry) || IS_IPV4_HNAT(entry)) {
+		*((u32 *)h_source) = swab32(entry->ipv4_hnapt.smac_hi);
+		*((u16 *)&h_source[4]) = swab16(entry->ipv4_hnapt.smac_lo);
+		*((u32 *)h_dest) = swab32(entry->ipv4_hnapt.dmac_hi);
+		*((u16 *)&h_dest[4]) = swab16(entry->ipv4_hnapt.dmac_lo);
+		pr_info("SMAC=%pM => DMAC=%pM\n", h_source, h_dest);
+		pr_info("State = %s, ",	entry->bfib1.state == 0 ?
+			"Invalid" : entry->bfib1.state == 1 ?
+			"Unbind" : entry->bfib1.state == 2 ?
+			"BIND" : entry->bfib1.state == 3 ?
+			"FIN" : "Unknown");
+		pr_info("Vlan_Layer = %u, ", entry->bfib1.vlan_layer);
+		pr_info("Eth_type = 0x%x, Vid1 = 0x%x, Vid2 = 0x%x\n",
+			entry->ipv4_hnapt.etype, entry->ipv4_hnapt.vlan1,
+			entry->ipv4_hnapt.vlan2);
+		pr_info("multicast = %d, pppoe = %d, proto = %s\n",
+			entry->ipv4_hnapt.iblk2.mcast,
+			entry->ipv4_hnapt.bfib1.psn,
+			entry->ipv4_hnapt.bfib1.udp == 0 ?
+			"TCP" :	entry->ipv4_hnapt.bfib1.udp == 1 ?
+			"UDP" : "Unknown");
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+		pr_info("tport_id = %d, tops_entry = %d, cdrt_id = %d\n",
+			entry->ipv4_hnapt.tport_id,
+			entry->ipv4_hnapt.tops_entry,
+			entry->ipv4_hnapt.cdrt_id);
+#endif
+		pr_info("=========================================\n\n");
+	} else {
+		*((u32 *)h_source) = swab32(entry->ipv6_5t_route.smac_hi);
+		*((u16 *)&h_source[4]) = swab16(entry->ipv6_5t_route.smac_lo);
+		*((u32 *)h_dest) = swab32(entry->ipv6_5t_route.dmac_hi);
+		*((u16 *)&h_dest[4]) = swab16(entry->ipv6_5t_route.dmac_lo);
+		pr_info("SMAC=%pM => DMAC=%pM\n", h_source, h_dest);
+		pr_info("State = %s, ",	entry->bfib1.state == 0 ?
+			"Invalid" : entry->bfib1.state == 1 ?
+			"Unbind" : entry->bfib1.state == 2 ?
+			"BIND" : entry->bfib1.state == 3 ?
+			"FIN" : "Unknown");
+
+		pr_info("Vlan_Layer = %u, ", entry->bfib1.vlan_layer);
+		pr_info("Eth_type = 0x%x, Vid1 = 0x%x, Vid2 = 0x%x\n",
+			entry->ipv6_5t_route.etype, entry->ipv6_5t_route.vlan1,
+			entry->ipv6_5t_route.vlan2);
+		pr_info("multicast = %d, pppoe = %d, proto = %s\n",
+			entry->ipv6_5t_route.iblk2.mcast,
+			entry->ipv6_5t_route.bfib1.psn,
+			entry->ipv6_5t_route.bfib1.udp == 0 ?
+			"TCP" :	entry->ipv6_5t_route.bfib1.udp == 1 ?
+			"UDP" :	"Unknown");
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+		if (IS_IPV6_HNAT(entry) || IS_IPV6_HNAPT(entry)) {
+			pr_info("tport_id = %d, tops_entry = %d, cdrt_id = %d\n",
+				entry->ipv6_hnapt.tport_id,
+				entry->ipv6_hnapt.tops_entry,
+				entry->ipv6_hnapt.cdrt_id);
+
+		} else if (IS_IPV4_MAPE(entry) || IS_IPV4_MAPT(entry)) {
+			pr_info("tport_id = %d, tops_entry = %d, cdrt_id = %d\n",
+				entry->ipv4_mape.tport_id,
+				entry->ipv4_mape.tops_entry,
+				entry->ipv4_mape.cdrt_id);
+		} else {
+			pr_info("tport_id = %d, tops_entry = %d, cdrt_id = %d\n",
+				entry->ipv6_5t_route.tport_id,
+				entry->ipv6_5t_route.tops_entry,
+				entry->ipv6_5t_route.cdrt_id);
+		}
+#endif
+		pr_info("=========================================\n\n");
+	}
+	return 0;
+}
+
+int wrapped_ppe0_entry_delete(int index) {
+	entry_delete(0, index);
+	return 0;
+}
+
+int wrapped_ppe1_entry_delete(int index) {
+	entry_delete(1, index);
+	return 0;
+}
+
+int entry_delete(u32 ppe_id, int index)
+{
+	struct foe_entry *entry;
+	struct mtk_hnat *h = hnat_priv;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	if (index < -1 || index >= (int)h->foe_etry_num) {
+		pr_info("Invalid entry index\n");
+		return -EINVAL;
+	}
+
+	if (index == -1) {
+		memset(h->foe_table_cpu[ppe_id], 0, h->foe_etry_num * sizeof(struct foe_entry));
+		pr_info("clear all foe entry\n");
+	} else {
+
+		entry = h->foe_table_cpu[ppe_id] + index;
+		memset(entry, 0, sizeof(struct foe_entry));
+		pr_info("delete ppe id = %d, entry idx = %d\n", ppe_id, index);
+	}
+
+	/* clear HWNAT cache */
+	hnat_cache_ebl(1);
+
+	return 0;
+}
+EXPORT_SYMBOL(entry_delete);
+
+int cr_set_usage(int level)
+{
+	debug_level = level;
+	pr_info("Dump hnat CR: cat /sys/kernel/debug/hnat/hnat_setting\n\n");
+	pr_info("====================Advanced Settings====================\n");
+	pr_info("Usage: echo [type] [option] > /sys/kernel/debug/hnat/hnat_setting\n\n");
+	pr_info("Commands:   [type] [option]\n");
+	pr_info("              0     0~7        Set debug_level(0~7), current debug_level=%d\n",
+		debug_level);
+	pr_info("              1     0~65535    Set binding threshold\n");
+	pr_info("              2     0~65535    Set TCP bind lifetime\n");
+	pr_info("              3     0~65535    Set FIN bind lifetime\n");
+	pr_info("              4     0~65535    Set UDP bind lifetime\n");
+	pr_info("              5     0~255      Set TCP keep alive interval\n");
+	pr_info("              6     0~255      Set UDP keep alive interval\n");
+	pr_info("              7     0~1        Set hnat counter update to nf_conntrack\n");
+
+	return 0;
+}
+
+int binding_threshold(int threshold)
+{
+	int i;
+
+	pr_info("Binding Threshold =%d\n", threshold);
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		writel(threshold, hnat_priv->ppe_base[i] + PPE_BNDR);
+
+	return 0;
+}
+
+int tcp_bind_lifetime(int tcp_life)
+{
+	int i;
+
+	pr_info("tcp_life = %d\n", tcp_life);
+
+	/* set Delta time for aging out an bind TCP FOE entry */
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_BND_AGE_1,
+			     TCP_DLTA, tcp_life);
+
+	return 0;
+}
+
+int fin_bind_lifetime(int fin_life)
+{
+	int i;
+
+	pr_info("fin_life = %d\n", fin_life);
+
+	/* set Delta time for aging out an bind TCP FIN FOE entry */
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_BND_AGE_1,
+			     FIN_DLTA, fin_life);
+
+	return 0;
+}
+
+int udp_bind_lifetime(int udp_life)
+{
+	int i;
+
+	pr_info("udp_life = %d\n", udp_life);
+
+	/* set Delta time for aging out an bind UDP FOE entry */
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_BND_AGE_0,
+			     UDP_DLTA, udp_life);
+
+	return 0;
+}
+
+int tcp_keep_alive(int tcp_interval)
+{
+	int i;
+
+	if (tcp_interval > 255) {
+		tcp_interval = 255;
+		pr_info("TCP keep alive max interval = 255\n");
+	} else {
+		pr_info("tcp_interval = %d\n", tcp_interval);
+	}
+
+	/* Keep alive time for bind FOE TCP entry */
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_KA,
+			     TCP_KA, tcp_interval);
+
+	return 0;
+}
+
+int udp_keep_alive(int udp_interval)
+{
+	int i;
+
+	if (udp_interval > 255) {
+		udp_interval = 255;
+		pr_info("TCP/UDP keep alive max interval = 255\n");
+	} else {
+		pr_info("udp_interval = %d\n", udp_interval);
+	}
+
+	/* Keep alive timer for bind FOE UDP entry */
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_KA,
+			     UDP_KA, udp_interval);
+
+	return 0;
+}
+
+int set_nf_update_toggle(int toggle)
+{
+	struct mtk_hnat *h = hnat_priv;
+
+	if (toggle == 1)
+		pr_info("Enable hnat counter update to nf_conntrack\n");
+	else if (toggle == 0)
+		pr_info("Disable hnat counter update to nf_conntrack\n");
+	else
+		pr_info("input error\n");
+	h->nf_stat_en = toggle;
+
+	return 0;
+}
+
+static const debugfs_write_func hnat_set_func[] = {
+	[0] = hnat_set_usage,
+	[1] = hnat_cpu_reason,
+};
+
+static const debugfs_write_func entry_set_func[] = {
+	[0] = entry_set_usage,
+	[1] = entry_set_state,
+	[2] = wrapped_ppe0_entry_detail,
+	[3] = wrapped_ppe0_entry_delete,
+	[4] = wrapped_ppe1_entry_detail,
+	[5] = wrapped_ppe1_entry_delete,
+};
+
+static const debugfs_write_func cr_set_func[] = {
+	[0] = cr_set_usage,      [1] = binding_threshold,
+	[2] = tcp_bind_lifetime, [3] = fin_bind_lifetime,
+	[4] = udp_bind_lifetime, [5] = tcp_keep_alive,
+	[6] = udp_keep_alive,    [7] = set_nf_update_toggle,
+};
+
+int read_mib(struct mtk_hnat *h, u32 ppe_id,
+	     u32 index, u64 *bytes, u64 *packets)
+{
+	int ret;
+	u32 val, cnt_r0, cnt_r1, cnt_r2, cnt_r3;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	writel(index | (1 << 16), h->ppe_base[ppe_id] + PPE_MIB_SER_CR);
+	ret = readx_poll_timeout_atomic(readl, h->ppe_base[ppe_id] + PPE_MIB_SER_CR, val,
+					!(val & BIT_MIB_BUSY), 20, 10000);
+
+	if (ret < 0) {
+		pr_notice("mib busy, please check later\n");
+		return ret;
+	}
+	cnt_r0 = readl(h->ppe_base[ppe_id] + PPE_MIB_SER_R0);
+	cnt_r1 = readl(h->ppe_base[ppe_id] + PPE_MIB_SER_R1);
+	cnt_r2 = readl(h->ppe_base[ppe_id] + PPE_MIB_SER_R2);
+
+	if (hnat_priv->data->version == MTK_HNAT_V3) {
+		cnt_r3 = readl(h->ppe_base[ppe_id] + PPE_MIB_SER_R3);
+		*bytes = cnt_r0 + ((u64)cnt_r1 << 32);
+		*packets = cnt_r2 + ((u64)cnt_r3 << 32);
+	} else {
+		*bytes = cnt_r0 + ((u64)(cnt_r1 & 0xffff) << 32);
+		*packets = ((cnt_r1 & 0xffff0000) >> 16) +
+			   ((u64)(cnt_r2 & 0xffffff) << 16);
+	}
+
+	return 0;
+
+}
+
+struct hnat_accounting *hnat_get_count(struct mtk_hnat *h, u32 ppe_id,
+				       u32 index, struct hnat_accounting *diff)
+
+{
+	u64 bytes, packets;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return NULL;
+
+	if (index >= hnat_priv->foe_etry_num)
+		return NULL;
+
+	if (!hnat_priv->data->per_flow_accounting)
+		return NULL;
+
+	if (read_mib(h, ppe_id, index, &bytes, &packets))
+		return NULL;
+
+	h->acct[ppe_id][index].bytes += bytes;
+	h->acct[ppe_id][index].packets += packets;
+
+	if (diff) {
+		diff->bytes = bytes;
+		diff->packets = packets;
+	}
+
+	return &h->acct[ppe_id][index];
+}
+EXPORT_SYMBOL(hnat_get_count);
+
+#define PRINT_COUNT(m, acct) {if (acct) \
+		seq_printf(m, "bytes=%llu|packets=%llu|", \
+			   acct->bytes, acct->packets); }
+static int __hnat_debug_show(struct seq_file *m, void *private, u32 ppe_id)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct foe_entry *entry, *end;
+	unsigned char h_dest[ETH_ALEN];
+	unsigned char h_source[ETH_ALEN];
+	struct hnat_accounting *acct;
+	u32 entry_index = 0;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	entry = h->foe_table_cpu[ppe_id];
+	end = h->foe_table_cpu[ppe_id] + hnat_priv->foe_etry_num;
+	while (entry < end) {
+		if (!entry->bfib1.state) {
+			entry++;
+			entry_index++;
+			continue;
+		}
+		acct = hnat_get_count(h, ppe_id, entry_index, NULL);
+		if (IS_IPV4_HNAPT(entry)) {
+			__be32 saddr = htonl(entry->ipv4_hnapt.sip);
+			__be32 daddr = htonl(entry->ipv4_hnapt.dip);
+			__be32 nsaddr = htonl(entry->ipv4_hnapt.new_sip);
+			__be32 ndaddr = htonl(entry->ipv4_hnapt.new_dip);
+
+			*((u32 *)h_source) = swab32(entry->ipv4_hnapt.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv4_hnapt.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv4_hnapt.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv4_hnapt.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|%pI4:%d->%pI4:%d=>%pI4:%d->%pI4:%d|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x|vlan1=%d|vlan2=%d\n",
+				   entry, ppe_id, ei(entry, end),
+				   es(entry), pt(entry), &saddr,
+				   entry->ipv4_hnapt.sport, &daddr,
+				   entry->ipv4_hnapt.dport, &nsaddr,
+				   entry->ipv4_hnapt.new_sport, &ndaddr,
+				   entry->ipv4_hnapt.new_dport, h_source, h_dest,
+				   ntohs(entry->ipv4_hnapt.etype),
+				   entry->ipv4_hnapt.info_blk1,
+				   entry->ipv4_hnapt.info_blk2,
+				   entry->ipv4_hnapt.vlan1,
+				   entry->ipv4_hnapt.vlan2);
+		} else if (IS_IPV4_HNAT(entry)) {
+			__be32 saddr = htonl(entry->ipv4_hnapt.sip);
+			__be32 daddr = htonl(entry->ipv4_hnapt.dip);
+			__be32 nsaddr = htonl(entry->ipv4_hnapt.new_sip);
+			__be32 ndaddr = htonl(entry->ipv4_hnapt.new_dip);
+
+			*((u32 *)h_source) = swab32(entry->ipv4_hnapt.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv4_hnapt.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv4_hnapt.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv4_hnapt.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|%pI4->%pI4=>%pI4->%pI4|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x|vlan1=%d|vlan2=%d\n",
+				   entry, ppe_id, ei(entry, end),
+				   es(entry), pt(entry), &saddr,
+				   &daddr, &nsaddr, &ndaddr, h_source, h_dest,
+				   ntohs(entry->ipv4_hnapt.etype),
+				   entry->ipv4_hnapt.info_blk1,
+				   entry->ipv4_hnapt.info_blk2,
+				   entry->ipv4_hnapt.vlan1,
+				   entry->ipv4_hnapt.vlan2);
+		} else if (IS_IPV6_5T_ROUTE(entry)) {
+			u32 ipv6_sip0 = entry->ipv6_3t_route.ipv6_sip0;
+			u32 ipv6_sip1 = entry->ipv6_3t_route.ipv6_sip1;
+			u32 ipv6_sip2 = entry->ipv6_3t_route.ipv6_sip2;
+			u32 ipv6_sip3 = entry->ipv6_3t_route.ipv6_sip3;
+			u32 ipv6_dip0 = entry->ipv6_3t_route.ipv6_dip0;
+			u32 ipv6_dip1 = entry->ipv6_3t_route.ipv6_dip1;
+			u32 ipv6_dip2 = entry->ipv6_3t_route.ipv6_dip2;
+			u32 ipv6_dip3 = entry->ipv6_3t_route.ipv6_dip3;
+
+			*((u32 *)h_source) =
+				swab32(entry->ipv6_5t_route.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv6_5t_route.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv6_5t_route.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv6_5t_route.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x(sp=%d)->DIP=%08x:%08x:%08x:%08x(dp=%d)|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x\n",
+				   entry, ppe_id, ei(entry, end), es(entry), pt(entry), ipv6_sip0,
+				   ipv6_sip1, ipv6_sip2, ipv6_sip3,
+				   entry->ipv6_5t_route.sport, ipv6_dip0,
+				   ipv6_dip1, ipv6_dip2, ipv6_dip3,
+				   entry->ipv6_5t_route.dport, h_source, h_dest,
+				   ntohs(entry->ipv6_5t_route.etype),
+				   entry->ipv6_5t_route.info_blk1,
+				   entry->ipv6_5t_route.info_blk2);
+		} else if (IS_IPV6_3T_ROUTE(entry)) {
+			u32 ipv6_sip0 = entry->ipv6_3t_route.ipv6_sip0;
+			u32 ipv6_sip1 = entry->ipv6_3t_route.ipv6_sip1;
+			u32 ipv6_sip2 = entry->ipv6_3t_route.ipv6_sip2;
+			u32 ipv6_sip3 = entry->ipv6_3t_route.ipv6_sip3;
+			u32 ipv6_dip0 = entry->ipv6_3t_route.ipv6_dip0;
+			u32 ipv6_dip1 = entry->ipv6_3t_route.ipv6_dip1;
+			u32 ipv6_dip2 = entry->ipv6_3t_route.ipv6_dip2;
+			u32 ipv6_dip3 = entry->ipv6_3t_route.ipv6_dip3;
+
+			*((u32 *)h_source) =
+				swab32(entry->ipv6_5t_route.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv6_5t_route.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv6_5t_route.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv6_5t_route.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x->DIP=%08x:%08x:%08x:%08x|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x\n",
+				   entry, ppe_id, ei(entry, end),
+				   es(entry), pt(entry), ipv6_sip0,
+				   ipv6_sip1, ipv6_sip2, ipv6_sip3, ipv6_dip0,
+				   ipv6_dip1, ipv6_dip2, ipv6_dip3, h_source,
+				   h_dest, ntohs(entry->ipv6_5t_route.etype),
+				   entry->ipv6_5t_route.info_blk1,
+				   entry->ipv6_5t_route.info_blk2);
+		} else if (IS_IPV6_6RD(entry)) {
+			u32 ipv6_sip0 = entry->ipv6_3t_route.ipv6_sip0;
+			u32 ipv6_sip1 = entry->ipv6_3t_route.ipv6_sip1;
+			u32 ipv6_sip2 = entry->ipv6_3t_route.ipv6_sip2;
+			u32 ipv6_sip3 = entry->ipv6_3t_route.ipv6_sip3;
+			u32 ipv6_dip0 = entry->ipv6_3t_route.ipv6_dip0;
+			u32 ipv6_dip1 = entry->ipv6_3t_route.ipv6_dip1;
+			u32 ipv6_dip2 = entry->ipv6_3t_route.ipv6_dip2;
+			u32 ipv6_dip3 = entry->ipv6_3t_route.ipv6_dip3;
+			__be32 tsaddr = htonl(entry->ipv6_6rd.tunnel_sipv4);
+			__be32 tdaddr = htonl(entry->ipv6_6rd.tunnel_dipv4);
+
+			*((u32 *)h_source) =
+				swab32(entry->ipv6_5t_route.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv6_5t_route.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv6_5t_route.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv6_5t_route.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x(sp=%d)->DIP=%08x:%08x:%08x:%08x(dp=%d)|TSIP=%pI4->TDIP=%pI4|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x\n",
+				   entry, ppe_id, ei(entry, end),
+				   es(entry), pt(entry), ipv6_sip0,
+				   ipv6_sip1, ipv6_sip2, ipv6_sip3,
+				   entry->ipv6_5t_route.sport, ipv6_dip0,
+				   ipv6_dip1, ipv6_dip2, ipv6_dip3,
+				   entry->ipv6_5t_route.dport, &tsaddr, &tdaddr,
+				   h_source, h_dest,
+				   ntohs(entry->ipv6_5t_route.etype),
+				   entry->ipv6_5t_route.info_blk1,
+				   entry->ipv6_5t_route.info_blk2);
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+		} else if (IS_IPV6_HNAPT(entry)) {
+			u32 ipv6_sip0 = entry->ipv6_hnapt.ipv6_sip0;
+			u32 ipv6_sip1 = entry->ipv6_hnapt.ipv6_sip1;
+			u32 ipv6_sip2 = entry->ipv6_hnapt.ipv6_sip2;
+			u32 ipv6_sip3 = entry->ipv6_hnapt.ipv6_sip3;
+			u32 ipv6_dip0 = entry->ipv6_hnapt.ipv6_dip0;
+			u32 ipv6_dip1 = entry->ipv6_hnapt.ipv6_dip1;
+			u32 ipv6_dip2 = entry->ipv6_hnapt.ipv6_dip2;
+			u32 ipv6_dip3 = entry->ipv6_hnapt.ipv6_dip3;
+			u32 new_ipv6_ip0 = entry->ipv6_hnapt.new_ipv6_ip0;
+			u32 new_ipv6_ip1 = entry->ipv6_hnapt.new_ipv6_ip1;
+			u32 new_ipv6_ip2 = entry->ipv6_hnapt.new_ipv6_ip2;
+			u32 new_ipv6_ip3 = entry->ipv6_hnapt.new_ipv6_ip3;
+
+			*((u32 *)h_source) = swab32(entry->ipv6_hnapt.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv6_hnapt.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv6_hnapt.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv6_hnapt.dmac_lo);
+			PRINT_COUNT(m, acct);
+
+			if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_SNAT) {
+				seq_printf(m,
+					   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x(sp=%d)->DIP=%08x:%08x:%08x:%08x(dp=%d)|NEW_SIP=%08x:%08x:%08x:%08x(sp=%d)->NEW_DIP=%08x:%08x:%08x:%08x(dp=%d)|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x|vlan1=%d|vlan2=%d\n",
+					   entry, ppe_id, ei(entry, end),
+					   es(entry), pt(entry),
+					   ipv6_sip0, ipv6_sip1,
+					   ipv6_sip2, ipv6_sip3,
+					   entry->ipv6_hnapt.sport,
+					   ipv6_dip0, ipv6_dip1,
+					   ipv6_dip2, ipv6_dip3,
+					   entry->ipv6_hnapt.dport,
+					   new_ipv6_ip0, new_ipv6_ip1,
+					   new_ipv6_ip2, new_ipv6_ip3,
+					   entry->ipv6_hnapt.new_sport,
+					   ipv6_dip0, ipv6_dip1,
+					   ipv6_dip2, ipv6_dip3,
+					   entry->ipv6_hnapt.new_dport,
+					   h_source, h_dest,
+					   ntohs(entry->ipv6_hnapt.etype),
+					   entry->ipv6_hnapt.info_blk1,
+					   entry->ipv6_hnapt.info_blk2,
+					   entry->ipv6_hnapt.vlan1,
+					   entry->ipv6_hnapt.vlan2);
+			} else if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_DNAT) {
+				seq_printf(m,
+					   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x(sp=%d)->DIP=%08x:%08x:%08x:%08x(dp=%d)|NEW_SIP=%08x:%08x:%08x:%08x(sp=%d)->NEW_DIP=%08x:%08x:%08x:%08x(dp=%d)|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x|vlan1=%d|vlan2=%d\n",
+					   entry, ppe_id, ei(entry, end),
+					   es(entry), pt(entry),
+					   ipv6_sip0, ipv6_sip1,
+					   ipv6_sip2, ipv6_sip3,
+					   entry->ipv6_hnapt.sport,
+					   ipv6_dip0, ipv6_dip1,
+					   ipv6_dip2, ipv6_dip3,
+					   entry->ipv6_hnapt.dport,
+					   ipv6_sip0, ipv6_sip1,
+					   ipv6_sip2, ipv6_sip3,
+					   entry->ipv6_hnapt.new_sport,
+					   new_ipv6_ip0, new_ipv6_ip1,
+					   new_ipv6_ip2, new_ipv6_ip3,
+					   entry->ipv6_hnapt.new_dport,
+					   h_source, h_dest,
+					   ntohs(entry->ipv6_hnapt.etype),
+					   entry->ipv6_hnapt.info_blk1,
+					   entry->ipv6_hnapt.info_blk2,
+					   entry->ipv6_hnapt.vlan1,
+					   entry->ipv6_hnapt.vlan2);
+			}
+		} else if (IS_IPV6_HNAT(entry)) {
+			u32 ipv6_sip0 = entry->ipv6_hnapt.ipv6_sip0;
+			u32 ipv6_sip1 = entry->ipv6_hnapt.ipv6_sip1;
+			u32 ipv6_sip2 = entry->ipv6_hnapt.ipv6_sip2;
+			u32 ipv6_sip3 = entry->ipv6_hnapt.ipv6_sip3;
+			u32 ipv6_dip0 = entry->ipv6_hnapt.ipv6_dip0;
+			u32 ipv6_dip1 = entry->ipv6_hnapt.ipv6_dip1;
+			u32 ipv6_dip2 = entry->ipv6_hnapt.ipv6_dip2;
+			u32 ipv6_dip3 = entry->ipv6_hnapt.ipv6_dip3;
+			u32 new_ipv6_ip0 = entry->ipv6_hnapt.new_ipv6_ip0;
+			u32 new_ipv6_ip1 = entry->ipv6_hnapt.new_ipv6_ip1;
+			u32 new_ipv6_ip2 = entry->ipv6_hnapt.new_ipv6_ip2;
+			u32 new_ipv6_ip3 = entry->ipv6_hnapt.new_ipv6_ip3;
+
+			*((u32 *)h_source) = swab32(entry->ipv6_hnapt.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv6_hnapt.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv6_hnapt.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv6_hnapt.dmac_lo);
+			PRINT_COUNT(m, acct);
+
+			if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_SNAT) {
+				seq_printf(m,
+					   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x->DIP=%08x:%08x:%08x:%08x|NEW_SIP=%08x:%08x:%08x:%08x->NEW_DIP=%08x:%08x:%08x:%08x|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x|vlan1=%d|vlan2=%d\n",
+					   entry, ppe_id, ei(entry, end),
+					   es(entry), pt(entry),
+					   ipv6_sip0, ipv6_sip1,
+					   ipv6_sip2, ipv6_sip3,
+					   ipv6_dip0, ipv6_dip1,
+					   ipv6_dip2, ipv6_dip3,
+					   new_ipv6_ip0, new_ipv6_ip1,
+					   new_ipv6_ip2, new_ipv6_ip3,
+					   ipv6_dip0, ipv6_dip1,
+					   ipv6_dip2, ipv6_dip3,
+					   h_source, h_dest,
+					   ntohs(entry->ipv6_hnapt.etype),
+					   entry->ipv6_hnapt.info_blk1,
+					   entry->ipv6_hnapt.info_blk2,
+					   entry->ipv6_hnapt.vlan1,
+					   entry->ipv6_hnapt.vlan2);
+			} else if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_DNAT) {
+				seq_printf(m,
+					   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%08x:%08x:%08x:%08x->DIP=%08x:%08x:%08x:%08x|NEW_SIP=%08x:%08x:%08x:%08x->NEW_DIP=%08x:%08x:%08x:%08x|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x|vlan1=%d|vlan2=%d\n",
+					   entry, ppe_id, ei(entry, end),
+					   es(entry), pt(entry),
+					   ipv6_sip0, ipv6_sip1,
+					   ipv6_sip2, ipv6_sip3,
+					   ipv6_dip0, ipv6_dip1,
+					   ipv6_dip2, ipv6_dip3,
+					   ipv6_sip0, ipv6_sip1,
+					   ipv6_sip2, ipv6_sip3,
+					   new_ipv6_ip0, new_ipv6_ip1,
+					   new_ipv6_ip2, new_ipv6_ip3,
+					   h_source, h_dest,
+					   ntohs(entry->ipv6_hnapt.etype),
+					   entry->ipv6_hnapt.info_blk1,
+					   entry->ipv6_hnapt.info_blk2,
+					   entry->ipv6_hnapt.vlan1,
+					   entry->ipv6_hnapt.vlan2);
+			}
+#endif
+		} else if (IS_IPV4_DSLITE(entry)) {
+			__be32 saddr = htonl(entry->ipv4_hnapt.sip);
+			__be32 daddr = htonl(entry->ipv4_hnapt.dip);
+			u32 ipv6_tsip0 = entry->ipv4_dslite.tunnel_sipv6_0;
+			u32 ipv6_tsip1 = entry->ipv4_dslite.tunnel_sipv6_1;
+			u32 ipv6_tsip2 = entry->ipv4_dslite.tunnel_sipv6_2;
+			u32 ipv6_tsip3 = entry->ipv4_dslite.tunnel_sipv6_3;
+			u32 ipv6_tdip0 = entry->ipv4_dslite.tunnel_dipv6_0;
+			u32 ipv6_tdip1 = entry->ipv4_dslite.tunnel_dipv6_1;
+			u32 ipv6_tdip2 = entry->ipv4_dslite.tunnel_dipv6_2;
+			u32 ipv6_tdip3 = entry->ipv4_dslite.tunnel_dipv6_3;
+
+			*((u32 *)h_source) = swab32(entry->ipv4_dslite.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv4_dslite.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv4_dslite.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv4_dslite.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%pI4->DIP=%pI4|TSIP=%08x:%08x:%08x:%08x->TDIP=%08x:%08x:%08x:%08x|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x\n",
+				   entry, ppe_id, ei(entry, end),
+				   es(entry), pt(entry), &saddr,
+				   &daddr, ipv6_tsip0, ipv6_tsip1, ipv6_tsip2,
+				   ipv6_tsip3, ipv6_tdip0, ipv6_tdip1, ipv6_tdip2,
+				   ipv6_tdip3, h_source, h_dest,
+				   ntohs(entry->ipv6_5t_route.etype),
+				   entry->ipv6_5t_route.info_blk1,
+				   entry->ipv6_5t_route.info_blk2);
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+		} else if (IS_IPV4_MAPE(entry)) {
+			__be32 saddr = htonl(entry->ipv4_dslite.sip);
+			__be32 daddr = htonl(entry->ipv4_dslite.dip);
+			__be32 nsaddr = htonl(entry->ipv4_mape.new_sip);
+			__be32 ndaddr = htonl(entry->ipv4_mape.new_dip);
+			u32 ipv6_tsip0 = entry->ipv4_dslite.tunnel_sipv6_0;
+			u32 ipv6_tsip1 = entry->ipv4_dslite.tunnel_sipv6_1;
+			u32 ipv6_tsip2 = entry->ipv4_dslite.tunnel_sipv6_2;
+			u32 ipv6_tsip3 = entry->ipv4_dslite.tunnel_sipv6_3;
+			u32 ipv6_tdip0 = entry->ipv4_dslite.tunnel_dipv6_0;
+			u32 ipv6_tdip1 = entry->ipv4_dslite.tunnel_dipv6_1;
+			u32 ipv6_tdip2 = entry->ipv4_dslite.tunnel_dipv6_2;
+			u32 ipv6_tdip3 = entry->ipv4_dslite.tunnel_dipv6_3;
+
+			*((u32 *)h_source) = swab32(entry->ipv4_dslite.smac_hi);
+			*((u16 *)&h_source[4]) =
+				swab16(entry->ipv4_dslite.smac_lo);
+			*((u32 *)h_dest) = swab32(entry->ipv4_dslite.dmac_hi);
+			*((u16 *)&h_dest[4]) =
+				swab16(entry->ipv4_dslite.dmac_lo);
+			PRINT_COUNT(m, acct);
+			seq_printf(m,
+				   "addr=0x%p|ppe=%d|index=%d|state=%s|type=%s|SIP=%pI4:%d->DIP=%pI4:%d|NSIP=%pI4:%d->NDIP=%pI4:%d|TSIP=%08x:%08x:%08x:%08x->TDIP=%08x:%08x:%08x:%08x|%pM=>%pM|etype=0x%04x|info1=0x%x|info2=0x%x\n",
+				   entry, ppe_id, ei(entry, end),
+				   es(entry), pt(entry),
+				   &saddr, entry->ipv4_dslite.sport,
+				   &daddr, entry->ipv4_dslite.dport,
+				   &nsaddr, entry->ipv4_mape.new_sport,
+				   &ndaddr, entry->ipv4_mape.new_dport,
+				   ipv6_tsip0, ipv6_tsip1, ipv6_tsip2,
+				   ipv6_tsip3, ipv6_tdip0, ipv6_tdip1,
+				   ipv6_tdip2, ipv6_tdip3, h_source, h_dest,
+				   ntohs(entry->ipv6_5t_route.etype),
+				   entry->ipv6_5t_route.info_blk1,
+				   entry->ipv6_5t_route.info_blk2);
+#endif
+		} else
+			seq_printf(m, "addr=0x%p|ppe=%d|index=%d state=%s\n", entry, ppe_id, ei(entry, end),
+				   es(entry));
+		entry++;
+		entry_index++;
+	}
+
+	return 0;
+}
+
+static int hnat_debug_show(struct seq_file *m, void *private)
+{
+	int i;
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		__hnat_debug_show(m, private, i);
+
+	return 0;
+}
+
+static int hnat_debug_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_debug_show, file->private_data);
+}
+
+static const struct file_operations hnat_debug_fops = {
+	.open = hnat_debug_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int hnat_whnat_show(struct seq_file *m, void *private)
+{
+	int i;
+	struct net_device *dev;
+
+	for (i = 0; i < MAX_IF_NUM; i++) {
+		dev = hnat_priv->wifi_hook_if[i];
+		if (dev)
+			seq_printf(m, "%d:%s\n", i, dev->name);
+		else
+			continue;
+	}
+
+	return 0;
+}
+
+static int hnat_whnat_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_whnat_show, file->private_data);
+}
+
+static ssize_t hnat_whnat_write(struct file *file, const char __user *buf,
+				size_t length, loff_t *offset)
+{
+	char line[64] = {0};
+	struct net_device *dev;
+	int enable;
+	char name[32];
+	size_t size;
+
+	if (length >= sizeof(line))
+		return -EINVAL;
+
+	if (copy_from_user(line, buf, length))
+		return -EFAULT;
+
+	if (sscanf(line, "%15s %1d", name, &enable) != 2)
+		return -EFAULT;
+
+	line[length] = '\0';
+
+	dev = dev_get_by_name(&init_net, name);
+
+	if (dev) {
+		if (enable) {
+			mtk_ppe_dev_register_hook(dev);
+			pr_info("register wifi extern if = %s\n", dev->name);
+		} else {
+			mtk_ppe_dev_unregister_hook(dev);
+			pr_info("unregister wifi extern if = %s\n", dev->name);
+		}
+		dev_put(dev);
+	} else {
+		pr_info("no such device!\n");
+	}
+
+	size = strlen(line);
+	*offset += size;
+
+	return length;
+}
+
+
+static const struct file_operations hnat_whnat_fops = {
+	.open = hnat_whnat_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_whnat_write,
+	.release = single_release,
+};
+
+int cpu_reason_read(struct seq_file *m, void *private)
+{
+	int i;
+
+	pr_info("============ CPU REASON =========\n");
+	pr_info("(2)IPv4(IPv6) TTL(hop limit) = %u\n", dbg_cpu_reason_cnt[0]);
+	pr_info("(3)Ipv4(IPv6) has option(extension) header = %u\n",
+		dbg_cpu_reason_cnt[1]);
+	pr_info("(7)No flow is assigned = %u\n", dbg_cpu_reason_cnt[2]);
+	pr_info("(8)IPv4 HNAT doesn't support IPv4 /w fragment = %u\n",
+		dbg_cpu_reason_cnt[3]);
+	pr_info("(9)IPv4 HNAPT/DS-Lite doesn't support IPv4 /w fragment = %u\n",
+		dbg_cpu_reason_cnt[4]);
+	pr_info("(10)IPv4 HNAPT/DS-Lite can't find TCP/UDP sport/dport = %u\n",
+		dbg_cpu_reason_cnt[5]);
+	pr_info("(11)IPv6 5T-route/6RD can't find TCP/UDP sport/dport = %u\n",
+		dbg_cpu_reason_cnt[6]);
+	pr_info("(12)Ingress packet is TCP fin/syn/rst = %u\n",
+		dbg_cpu_reason_cnt[7]);
+	pr_info("(13)FOE Un-hit = %u\n", dbg_cpu_reason_cnt[8]);
+	pr_info("(14)FOE Hit unbind = %u\n", dbg_cpu_reason_cnt[9]);
+	pr_info("(15)FOE Hit unbind & rate reach = %u\n",
+		dbg_cpu_reason_cnt[10]);
+	pr_info("(16)Hit bind PPE TCP FIN entry = %u\n",
+		dbg_cpu_reason_cnt[11]);
+	pr_info("(17)Hit bind PPE entry and TTL(hop limit) = 1 and TTL(hot limit) - 1 = %u\n",
+		dbg_cpu_reason_cnt[12]);
+	pr_info("(18)Hit bind and VLAN replacement violation = %u\n",
+		dbg_cpu_reason_cnt[13]);
+	pr_info("(19)Hit bind and keep alive with unicast old-header packet = %u\n",
+		dbg_cpu_reason_cnt[14]);
+	pr_info("(20)Hit bind and keep alive with multicast new-header packet = %u\n",
+		dbg_cpu_reason_cnt[15]);
+	pr_info("(21)Hit bind and keep alive with duplicate old-header packet = %u\n",
+		dbg_cpu_reason_cnt[16]);
+	pr_info("(22)FOE Hit bind & force to CPU = %u\n",
+		dbg_cpu_reason_cnt[17]);
+	pr_info("(28)Hit bind and exceed MTU =%u\n", dbg_cpu_reason_cnt[18]);
+	pr_info("(24)Hit bind multicast packet to CPU = %u\n",
+		dbg_cpu_reason_cnt[19]);
+	pr_info("(25)Hit bind multicast packet to GMAC & CPU = %u\n",
+		dbg_cpu_reason_cnt[20]);
+	pr_info("(26)Pre bind = %u\n", dbg_cpu_reason_cnt[21]);
+
+	for (i = 0; i < 22; i++)
+		dbg_cpu_reason_cnt[i] = 0;
+	return 0;
+}
+
+static int cpu_reason_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, cpu_reason_read, file->private_data);
+}
+
+ssize_t cpu_reason_write(struct file *file, const char __user *buffer,
+			 size_t count, loff_t *data)
+{
+	char buf[32];
+	char *p_buf;
+	u32 len = count;
+	long arg0 = 0, arg1 = 0;
+	char *p_token = NULL;
+	char *p_delimiter = " \t";
+	int ret;
+
+	if (len >= sizeof(buf)) {
+		pr_info("input handling fail!\n");
+		return -1;
+	}
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = '\0';
+
+	p_buf = buf;
+	p_token = strsep(&p_buf, p_delimiter);
+	if (!p_token)
+		arg0 = 0;
+	else
+		ret = kstrtol(p_token, 10, &arg0);
+
+	switch (arg0) {
+	case 0:
+	case 1:
+		p_token = strsep(&p_buf, p_delimiter);
+		if (!p_token)
+			arg1 = 0;
+		else
+			ret = kstrtol(p_token, 10, &arg1);
+		break;
+	default:
+		pr_info("no handler defined for command id(0x%08lx)\n\r", arg0);
+		arg0 = 0;
+		arg1 = 0;
+		break;
+	}
+
+	(*hnat_set_func[arg0])(arg1);
+
+	return len;
+}
+
+static const struct file_operations cpu_reason_fops = {
+	.open = cpu_reason_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = cpu_reason_write,
+	.release = single_release,
+};
+
+void dbg_dump_entry(struct seq_file *m, struct foe_entry *entry,
+		    uint32_t index)
+{
+	__be32 saddr, daddr, nsaddr, ndaddr;
+
+	saddr = htonl(entry->ipv4_hnapt.sip);
+	daddr = htonl(entry->ipv4_hnapt.dip);
+	nsaddr = htonl(entry->ipv4_hnapt.new_sip);
+	ndaddr = htonl(entry->ipv4_hnapt.new_dip);
+
+	if (IS_IPV4_HNAPT(entry)) {
+		seq_printf(m,
+			   "NAPT(%d): %pI4:%d->%pI4:%d => %pI4:%d->%pI4:%d\n",
+			   index, &saddr, entry->ipv4_hnapt.sport, &daddr,
+			   entry->ipv4_hnapt.dport, &nsaddr,
+			   entry->ipv4_hnapt.new_sport, &ndaddr,
+			   entry->ipv4_hnapt.new_dport);
+	} else if (IS_IPV4_HNAT(entry)) {
+		seq_printf(m, "NAT(%d): %pI4->%pI4 => %pI4->%pI4\n",
+			   index, &saddr, &daddr, &nsaddr, &ndaddr);
+	}
+
+	if (IS_IPV4_DSLITE(entry)) {
+		seq_printf(m,
+			   "IPv4 Ds-Lite(%d): %pI4:%d->%pI4:%d => %08X:%08X:%08X:%08X->%08X:%08X:%08X:%08X\n",
+			   index, &saddr, entry->ipv4_dslite.sport, &daddr,
+			   entry->ipv4_dslite.dport,
+			   entry->ipv4_dslite.tunnel_sipv6_0,
+			   entry->ipv4_dslite.tunnel_sipv6_1,
+			   entry->ipv4_dslite.tunnel_sipv6_2,
+			   entry->ipv4_dslite.tunnel_sipv6_3,
+			   entry->ipv4_dslite.tunnel_dipv6_0,
+			   entry->ipv4_dslite.tunnel_dipv6_1,
+			   entry->ipv4_dslite.tunnel_dipv6_2,
+			   entry->ipv4_dslite.tunnel_dipv6_3);
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+	} else if (IS_IPV4_MAPE(entry)) {
+		nsaddr = htonl(entry->ipv4_mape.new_sip);
+		ndaddr = htonl(entry->ipv4_mape.new_dip);
+
+		seq_printf(m,
+			   "IPv4 MAP-E(%d): %pI4:%d->%pI4:%d => %pI4:%d->%pI4:%d | Tunnel=%08X:%08X:%08X:%08X->%08X:%08X:%08X:%08X\n",
+			   index, &saddr, entry->ipv4_dslite.sport,
+			   &daddr, entry->ipv4_dslite.dport,
+			   &nsaddr, entry->ipv4_mape.new_sport,
+			   &ndaddr, entry->ipv4_mape.new_dport,
+			   entry->ipv4_dslite.tunnel_sipv6_0,
+			   entry->ipv4_dslite.tunnel_sipv6_1,
+			   entry->ipv4_dslite.tunnel_sipv6_2,
+			   entry->ipv4_dslite.tunnel_sipv6_3,
+			   entry->ipv4_dslite.tunnel_dipv6_0,
+			   entry->ipv4_dslite.tunnel_dipv6_1,
+			   entry->ipv4_dslite.tunnel_dipv6_2,
+			   entry->ipv4_dslite.tunnel_dipv6_3);
+#endif
+	} else if (IS_IPV6_3T_ROUTE(entry)) {
+		seq_printf(m,
+			   "IPv6_3T(%d): %08X:%08X:%08X:%08X => %08X:%08X:%08X:%08X (Prot=%d)\n",
+			   index, entry->ipv6_3t_route.ipv6_sip0,
+			   entry->ipv6_3t_route.ipv6_sip1,
+			   entry->ipv6_3t_route.ipv6_sip2,
+			   entry->ipv6_3t_route.ipv6_sip3,
+			   entry->ipv6_3t_route.ipv6_dip0,
+			   entry->ipv6_3t_route.ipv6_dip1,
+			   entry->ipv6_3t_route.ipv6_dip2,
+			   entry->ipv6_3t_route.ipv6_dip3,
+			   entry->ipv6_3t_route.prot);
+	} else if (IS_IPV6_5T_ROUTE(entry)) {
+		seq_printf(m,
+			   "IPv6_5T(%d): %08X:%08X:%08X:%08X:%d => %08X:%08X:%08X:%08X:%d\n",
+			   index, entry->ipv6_5t_route.ipv6_sip0,
+			   entry->ipv6_5t_route.ipv6_sip1,
+			   entry->ipv6_5t_route.ipv6_sip2,
+			   entry->ipv6_5t_route.ipv6_sip3,
+			   entry->ipv6_5t_route.sport,
+			   entry->ipv6_5t_route.ipv6_dip0,
+			   entry->ipv6_5t_route.ipv6_dip1,
+			   entry->ipv6_5t_route.ipv6_dip2,
+			   entry->ipv6_5t_route.ipv6_dip3,
+			   entry->ipv6_5t_route.dport);
+	} else if (IS_IPV6_6RD(entry)) {
+		seq_printf(m,
+			   "IPv6_6RD(%d): %08X:%08X:%08X:%08X:%d => %08X:%08X:%08X:%08X:%d\n",
+			   index, entry->ipv6_6rd.ipv6_sip0,
+			   entry->ipv6_6rd.ipv6_sip1, entry->ipv6_6rd.ipv6_sip2,
+			   entry->ipv6_6rd.ipv6_sip3, entry->ipv6_6rd.sport,
+			   entry->ipv6_6rd.ipv6_dip0, entry->ipv6_6rd.ipv6_dip1,
+			   entry->ipv6_6rd.ipv6_dip2, entry->ipv6_6rd.ipv6_dip3,
+			   entry->ipv6_6rd.dport);
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	} else if (IS_IPV6_HNAPT(entry)) {
+		if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_SNAT) {
+			seq_printf(m,
+				   "IPv6_HNAPT(%d): %08X:%08X:%08X:%08X:%d->%08X:%08X:%08X:%08X:%d => %08X:%08X:%08X:%08X:%d -> %08X:%08X:%08X:%08X:%d\n",
+				   index, entry->ipv6_hnapt.ipv6_sip0,
+				   entry->ipv6_hnapt.ipv6_sip1,
+				   entry->ipv6_hnapt.ipv6_sip2,
+				   entry->ipv6_hnapt.ipv6_sip3,
+				   entry->ipv6_hnapt.sport,
+				   entry->ipv6_hnapt.ipv6_dip0,
+				   entry->ipv6_hnapt.ipv6_dip1,
+				   entry->ipv6_hnapt.ipv6_dip2,
+				   entry->ipv6_hnapt.ipv6_dip3,
+				   entry->ipv6_hnapt.dport,
+				   entry->ipv6_hnapt.new_ipv6_ip0,
+				   entry->ipv6_hnapt.new_ipv6_ip1,
+				   entry->ipv6_hnapt.new_ipv6_ip2,
+				   entry->ipv6_hnapt.new_ipv6_ip3,
+				   entry->ipv6_hnapt.new_sport,
+				   entry->ipv6_hnapt.ipv6_dip0,
+				   entry->ipv6_hnapt.ipv6_dip1,
+				   entry->ipv6_hnapt.ipv6_dip2,
+				   entry->ipv6_hnapt.ipv6_dip3,
+				   entry->ipv6_hnapt.new_dport);
+		} else if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_DNAT) {
+			seq_printf(m,
+				   "IPv6_HNAPT(%d): %08X:%08X:%08X:%08X:%d->%08X:%08X:%08X:%08X:%d => %08X:%08X:%08X:%08X:%d -> %08X:%08X:%08X:%08X:%d\n",
+				   index, entry->ipv6_hnapt.ipv6_sip0,
+				   entry->ipv6_hnapt.ipv6_sip1,
+				   entry->ipv6_hnapt.ipv6_sip2,
+				   entry->ipv6_hnapt.ipv6_sip3,
+				   entry->ipv6_hnapt.sport,
+				   entry->ipv6_hnapt.ipv6_dip0,
+				   entry->ipv6_hnapt.ipv6_dip1,
+				   entry->ipv6_hnapt.ipv6_dip2,
+				   entry->ipv6_hnapt.ipv6_dip3,
+				   entry->ipv6_hnapt.dport,
+				   entry->ipv6_hnapt.ipv6_sip0,
+				   entry->ipv6_hnapt.ipv6_sip1,
+				   entry->ipv6_hnapt.ipv6_sip2,
+				   entry->ipv6_hnapt.ipv6_sip3,
+				   entry->ipv6_hnapt.new_sport,
+				   entry->ipv6_hnapt.new_ipv6_ip0,
+				   entry->ipv6_hnapt.new_ipv6_ip1,
+				   entry->ipv6_hnapt.new_ipv6_ip2,
+				   entry->ipv6_hnapt.new_ipv6_ip3,
+				   entry->ipv6_hnapt.new_dport);
+		}
+	} else if (IS_IPV6_HNAT(entry)) {
+		if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_SNAT) {
+			seq_printf(m,
+				   "IPv6_HNAT(%d): %08X:%08X:%08X:%08X->%08X:%08X:%08X:%08X => %08X:%08X:%08X:%08X -> %08X:%08X:%08X:%08X\n",
+				   index, entry->ipv6_hnapt.ipv6_sip0,
+				   entry->ipv6_hnapt.ipv6_sip1,
+				   entry->ipv6_hnapt.ipv6_sip2,
+				   entry->ipv6_hnapt.ipv6_sip3,
+				   entry->ipv6_hnapt.ipv6_dip0,
+				   entry->ipv6_hnapt.ipv6_dip1,
+				   entry->ipv6_hnapt.ipv6_dip2,
+				   entry->ipv6_hnapt.ipv6_dip3,
+				   entry->ipv6_hnapt.new_ipv6_ip0,
+				   entry->ipv6_hnapt.new_ipv6_ip1,
+				   entry->ipv6_hnapt.new_ipv6_ip2,
+				   entry->ipv6_hnapt.new_ipv6_ip3,
+				   entry->ipv6_hnapt.ipv6_dip0,
+				   entry->ipv6_hnapt.ipv6_dip1,
+				   entry->ipv6_hnapt.ipv6_dip2,
+				   entry->ipv6_hnapt.ipv6_dip3);
+		} else if (entry->ipv6_hnapt.eg_ipv6_dir == IPV6_DNAT) {
+			seq_printf(m,
+				   "IPv6_HNAT(%d): %08X:%08X:%08X:%08X->%08X:%08X:%08X:%08X => %08X:%08X:%08X:%08X -> %08X:%08X:%08X:%08X\n",
+				   index, entry->ipv6_hnapt.ipv6_sip0,
+				   entry->ipv6_hnapt.ipv6_sip1,
+				   entry->ipv6_hnapt.ipv6_sip2,
+				   entry->ipv6_hnapt.ipv6_sip3,
+				   entry->ipv6_hnapt.ipv6_dip0,
+				   entry->ipv6_hnapt.ipv6_dip1,
+				   entry->ipv6_hnapt.ipv6_dip2,
+				   entry->ipv6_hnapt.ipv6_dip3,
+				   entry->ipv6_hnapt.ipv6_sip0,
+				   entry->ipv6_hnapt.ipv6_sip1,
+				   entry->ipv6_hnapt.ipv6_sip2,
+				   entry->ipv6_hnapt.ipv6_sip3,
+				   entry->ipv6_hnapt.new_ipv6_ip0,
+				   entry->ipv6_hnapt.new_ipv6_ip1,
+				   entry->ipv6_hnapt.new_ipv6_ip2,
+				   entry->ipv6_hnapt.new_ipv6_ip3);
+		}
+#endif
+	}
+}
+
+int __hnat_entry_read(struct seq_file *m, void *private, u32 ppe_id)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct foe_entry *entry, *end;
+	int hash_index;
+	int cnt;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	hash_index = 0;
+	cnt = 0;
+	entry = h->foe_table_cpu[ppe_id];
+	end = h->foe_table_cpu[ppe_id] + hnat_priv->foe_etry_num;
+
+	seq_printf(m, "============================\n");
+	seq_printf(m, "PPE_ID = %d\n", ppe_id);
+
+	while (entry < end) {
+		if (entry->bfib1.state == dbg_entry_state) {
+			cnt++;
+			dbg_dump_entry(m, entry, hash_index);
+		}
+		hash_index++;
+		entry++;
+	}
+
+	seq_printf(m, "Total State = %s cnt = %d\n",
+		   dbg_entry_state == 0 ?
+		   "Invalid" : dbg_entry_state == 1 ?
+		   "Unbind" : dbg_entry_state == 2 ?
+		   "BIND" : dbg_entry_state == 3 ?
+		   "FIN" : "Unknown", cnt);
+
+	return 0;
+}
+
+int hnat_entry_read(struct seq_file *m, void *private)
+{
+	int i;
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		__hnat_entry_read(m, private, i);
+
+	return 0;
+}
+
+ssize_t hnat_entry_write(struct file *file, const char __user *buffer,
+			 size_t count, loff_t *data)
+{
+	char buf[32];
+	char *p_buf;
+	u32 len = count;
+	long arg0 = 0, arg1 = 0;
+	char *p_token = NULL;
+	char *p_delimiter = " \t";
+	int ret;
+
+	if (len >= sizeof(buf)) {
+		pr_info("input handling fail!\n");
+		return -1;
+	}
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = '\0';
+
+	p_buf = buf;
+	p_token = strsep(&p_buf, p_delimiter);
+	if (!p_token)
+		arg0 = 0;
+	else
+		ret = kstrtol(p_token, 10, &arg0);
+
+	switch (arg0) {
+	case 0:
+	case 1:
+	case 2:
+	case 3:
+	case 4:
+	case 5:
+		p_token = strsep(&p_buf, p_delimiter);
+		if (!p_token)
+			arg1 = 0;
+		else
+			ret = kstrtol(p_token, 10, &arg1);
+		break;
+	default:
+		pr_info("no handler defined for command id(0x%08lx)\n\r", arg0);
+		arg0 = 0;
+		arg1 = 0;
+		break;
+	}
+
+	(*entry_set_func[arg0])(arg1);
+
+	return len;
+}
+
+static int hnat_entry_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_entry_read, file->private_data);
+}
+
+static const struct file_operations hnat_entry_fops = {
+	.open = hnat_entry_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_entry_write,
+	.release = single_release,
+};
+
+int __hnat_setting_read(struct seq_file *m, void *private, u32 ppe_id)
+{
+	struct mtk_hnat *h = hnat_priv;
+	int i;
+	int cr_max;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	cr_max = 319 * 4;
+	for (i = 0; i < cr_max; i = i + 0x10) {
+		pr_info("0x%p : 0x%08x 0x%08x 0x%08x 0x%08x\n",
+			(void *)h->foe_table_dev[ppe_id] + i,
+			readl(h->ppe_base[ppe_id] + i),
+			readl(h->ppe_base[ppe_id] + i + 4),
+			readl(h->ppe_base[ppe_id] + i + 8),
+			readl(h->ppe_base[ppe_id] + i + 0xc));
+	}
+
+	return 0;
+}
+
+int hnat_setting_read(struct seq_file *m, void *private)
+{
+	int i;
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		__hnat_setting_read(m, private, i);
+
+	return 0;
+}
+
+static int hnat_setting_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_setting_read, file->private_data);
+}
+
+ssize_t hnat_setting_write(struct file *file, const char __user *buffer,
+			   size_t count, loff_t *data)
+{
+	char buf[32];
+	char *p_buf;
+	u32 len = count;
+	long arg0 = 0, arg1 = 0;
+	char *p_token = NULL;
+	char *p_delimiter = " \t";
+	int ret;
+
+	if (len >= sizeof(buf)) {
+		pr_info("input handling fail!\n");
+		return -1;
+	}
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = '\0';
+
+	p_buf = buf;
+	p_token = strsep(&p_buf, p_delimiter);
+	if (!p_token)
+		arg0 = 0;
+	else
+		ret = kstrtol(p_token, 10, &arg0);
+
+	switch (arg0) {
+	case 0:
+	case 1:
+	case 2:
+	case 3:
+	case 4:
+	case 5:
+	case 6:
+	case 7:
+		p_token = strsep(&p_buf, p_delimiter);
+		if (!p_token)
+			arg1 = 0;
+		else
+			ret = kstrtol(p_token, 10, &arg1);
+		break;
+	default:
+		pr_info("no handler defined for command id(0x%08lx)\n\r", arg0);
+		arg0 = 0;
+		arg1 = 0;
+		break;
+	}
+
+	(*cr_set_func[arg0])(arg1);
+
+	return len;
+}
+
+static const struct file_operations hnat_setting_fops = {
+	.open = hnat_setting_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_setting_write,
+	.release = single_release,
+};
+
+int __mcast_table_dump(struct seq_file *m, void *private, u32 ppe_id)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct ppe_mcast_h mcast_h;
+	struct ppe_mcast_l mcast_l;
+	u8 i, max;
+	void __iomem *reg;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	if (!h->pmcast)
+		return 0;
+
+	max = h->pmcast->max_entry;
+	pr_info("============================\n");
+	pr_info("PPE_ID = %d\n", ppe_id);
+	pr_info("MAC | VID | PortMask | QosPortMask\n");
+	for (i = 0; i < max; i++) {
+		if (i < 0x10) {
+			reg = h->ppe_base[ppe_id] + PPE_MCAST_H_0 + i * 8;
+			mcast_h.u.value = readl(reg);
+			reg = h->ppe_base[ppe_id] + PPE_MCAST_L_0 + i * 8;
+			mcast_l.addr = readl(reg);
+		} else {
+			reg = h->fe_base + PPE_MCAST_H_10 + (i - 0x10) * 8;
+			mcast_h.u.value = readl(reg);
+			reg = h->fe_base + PPE_MCAST_L_10 + (i - 0x10) * 8;
+			mcast_l.addr = readl(reg);
+		}
+		pr_info("%08x %d %c%c%c%c %c%c%c%c (QID=%d, mc_mpre_sel=%d)\n",
+			mcast_l.addr,
+			mcast_h.u.info.mc_vid,
+			(mcast_h.u.info.mc_px_en & 0x08) ? '1' : '-',
+			(mcast_h.u.info.mc_px_en & 0x04) ? '1' : '-',
+			(mcast_h.u.info.mc_px_en & 0x02) ? '1' : '-',
+			(mcast_h.u.info.mc_px_en & 0x01) ? '1' : '-',
+			(mcast_h.u.info.mc_px_qos_en & 0x08) ? '1' : '-',
+			(mcast_h.u.info.mc_px_qos_en & 0x04) ? '1' : '-',
+			(mcast_h.u.info.mc_px_qos_en & 0x02) ? '1' : '-',
+			(mcast_h.u.info.mc_px_qos_en & 0x01) ? '1' : '-',
+			mcast_h.u.info.mc_qos_qid +
+			((mcast_h.u.info.mc_qos_qid54) << 4),
+			mcast_h.u.info.mc_mpre_sel);
+	}
+
+	return 0;
+}
+
+int mcast_table_dump(struct seq_file *m, void *private)
+{
+	int i;
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		__mcast_table_dump(m, private, i);
+
+	return 0;
+}
+
+static int mcast_table_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, mcast_table_dump, file->private_data);
+}
+
+static const struct file_operations hnat_mcast_fops = {
+	.open = mcast_table_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static int hnat_ext_show(struct seq_file *m, void *private)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (ext_entry->dev)
+			seq_printf(m, "ext devices [%d] = %s  (dev=%p, ifindex=%d)\n",
+				   i, ext_entry->name, ext_entry->dev,
+				   ext_entry->dev->ifindex);
+	}
+
+	return 0;
+}
+
+static int hnat_ext_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_ext_show, file->private_data);
+}
+
+static const struct file_operations hnat_ext_fops = {
+	.open = hnat_ext_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+static ssize_t hnat_sched_show(struct file *file, char __user *user_buf,
+			       size_t count, loff_t *ppos)
+{
+	long id = (long)file->private_data;
+	struct mtk_hnat *h = hnat_priv;
+	u32 qdma_tx_sch;
+	int enable;
+	int scheduling;
+	int max_rate;
+	char *buf;
+	unsigned int len = 0, buf_len = 1500;
+	ssize_t ret_cnt;
+	int scheduler, i;
+	u32 sch_reg;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	if (hnat_priv->data->num_of_sch == 4)
+		qdma_tx_sch = readl(h->fe_base + QDMA_TX_4SCH_BASE(id));
+	else
+		qdma_tx_sch = readl(h->fe_base + QDMA_TX_2SCH_BASE);
+
+	if (id & 0x1)
+		qdma_tx_sch >>= 16;
+	qdma_tx_sch &= 0xffff;
+	enable = !!(qdma_tx_sch & BIT(11));
+	scheduling = !!(qdma_tx_sch & BIT(15));
+	max_rate = ((qdma_tx_sch >> 4) & 0x7f);
+	qdma_tx_sch &= 0xf;
+	while (qdma_tx_sch--)
+		max_rate *= 10;
+
+	len += scnprintf(buf + len, buf_len - len,
+			 "EN\tScheduling\tMAX\tQueue#\n%d\t%s%16d\t", enable,
+			 (scheduling == 1) ? "WRR" : "SP", max_rate);
+
+	for (i = 0; i < MTK_QDMA_TX_NUM; i++) {
+		cr_set_field(h->fe_base + QDMA_PAGE, QTX_CFG_PAGE,
+			     (i / NUM_OF_Q_PER_PAGE));
+		sch_reg = readl(h->fe_base + QTX_SCH(i % NUM_OF_Q_PER_PAGE));
+		if (hnat_priv->data->num_of_sch == 4)
+			scheduler = (sch_reg >> 30) & 0x3;
+		else
+			scheduler = !!(sch_reg & BIT(31));
+		if (id == scheduler)
+			len += scnprintf(buf + len, buf_len - len, "%d  ", i);
+	}
+
+	len += scnprintf(buf + len, buf_len - len, "\n");
+	if (len > buf_len)
+		len = buf_len;
+
+	ret_cnt = simple_read_from_buffer(user_buf, count, ppos, buf, len);
+
+	kfree(buf);
+	return ret_cnt;
+}
+
+static ssize_t hnat_sched_write(struct file *file, const char __user *buf,
+				size_t length, loff_t *offset)
+{
+	long id = (long)file->private_data;
+	struct mtk_hnat *h = hnat_priv;
+	char line[64] = {0};
+	int enable, rate, exp = 0, shift = 0;
+	char scheduling[32];
+	size_t size;
+	u32 qdma_tx_sch;
+	u32 val = 0;
+
+	if (length >= sizeof(line))
+		return -EINVAL;
+
+	if (copy_from_user(line, buf, length))
+		return -EFAULT;
+
+	if (sscanf(line, "%1d %3s %9d", &enable, scheduling, &rate) != 3)
+		return -EFAULT;
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	if (rate > 100000000 || rate < 0 ||
+	    rate > 100000000 || rate < 0)
+#else
+	if (rate > 10000000 || rate < 0 ||
+	    rate > 10000000 || rate < 0)
+#endif
+		return -EINVAL;
+
+	while (rate > 127) {
+		rate /= 10;
+		exp++;
+	}
+
+	line[length] = '\0';
+
+	if (enable)
+		val |= BIT(11);
+	if (strcmp(scheduling, "sp") != 0)
+		val |= BIT(15);
+	val |= (rate & 0x7f) << 4;
+	val |= exp & 0xf;
+	if (id & 0x1)
+		shift = 16;
+
+	if (hnat_priv->data->num_of_sch == 4)
+		qdma_tx_sch = readl(h->fe_base + QDMA_TX_4SCH_BASE(id));
+	else
+		qdma_tx_sch = readl(h->fe_base + QDMA_TX_2SCH_BASE);
+
+	qdma_tx_sch &= ~(0xffff << shift);
+	qdma_tx_sch |= val << shift;
+	if (hnat_priv->data->num_of_sch == 4)
+		writel(qdma_tx_sch, h->fe_base + QDMA_TX_4SCH_BASE(id));
+	else
+		writel(qdma_tx_sch, h->fe_base + QDMA_TX_2SCH_BASE);
+
+	size = strlen(line);
+	*offset += size;
+
+	return length;
+}
+
+static const struct file_operations hnat_sched_fops = {
+	.open = simple_open,
+	.read = hnat_sched_show,
+	.write = hnat_sched_write,
+	.llseek = default_llseek,
+};
+
+static ssize_t hnat_queue_show(struct file *file, char __user *user_buf,
+			       size_t count, loff_t *ppos)
+{
+	struct mtk_hnat *h = hnat_priv;
+	long id = (long)file->private_data;
+	u32 qtx_sch;
+	u32 qtx_cfg;
+	int scheduler;
+	int min_rate_en;
+	int min_rate;
+	int min_rate_exp;
+	int max_rate_en;
+	int max_weight;
+	int max_rate;
+	int max_rate_exp;
+	char *buf;
+	unsigned int len = 0, buf_len = 1500;
+	ssize_t ret_cnt;
+
+	buf = kzalloc(buf_len, GFP_KERNEL);
+	if (!buf)
+		return -ENOMEM;
+
+	cr_set_field(h->fe_base + QDMA_PAGE, QTX_CFG_PAGE, (id / NUM_OF_Q_PER_PAGE));
+	qtx_cfg = readl(h->fe_base + QTX_CFG(id % NUM_OF_Q_PER_PAGE));
+	qtx_sch = readl(h->fe_base + QTX_SCH(id % NUM_OF_Q_PER_PAGE));
+	if (hnat_priv->data->num_of_sch == 4)
+		scheduler = (qtx_sch >> 30) & 0x3;
+	else
+		scheduler = !!(qtx_sch & BIT(31));
+	min_rate_en = !!(qtx_sch & BIT(27));
+	min_rate = (qtx_sch >> 20) & 0x7f;
+	min_rate_exp = (qtx_sch >> 16) & 0xf;
+	max_rate_en = !!(qtx_sch & BIT(11));
+	max_weight = (qtx_sch >> 12) & 0xf;
+	max_rate = (qtx_sch >> 4) & 0x7f;
+	max_rate_exp = qtx_sch & 0xf;
+	while (min_rate_exp--)
+		min_rate *= 10;
+
+	while (max_rate_exp--)
+		max_rate *= 10;
+
+	len += scnprintf(buf + len, buf_len - len,
+			 "scheduler: %d\nhw resv: %d\nsw resv: %d\n", scheduler,
+			 (qtx_cfg >> 8) & 0xff, qtx_cfg & 0xff);
+
+	if (hnat_priv->data->version != MTK_HNAT_V1_1) {
+		/* Switch to debug mode */
+		cr_set_field(h->fe_base + QTX_MIB_IF, MIB_ON_QTX_CFG, 1);
+		cr_set_field(h->fe_base + QTX_MIB_IF, VQTX_MIB_EN, 1);
+		qtx_cfg = readl(h->fe_base + QTX_CFG(id % NUM_OF_Q_PER_PAGE));
+		qtx_sch = readl(h->fe_base + QTX_SCH(id % NUM_OF_Q_PER_PAGE));
+		len += scnprintf(buf + len, buf_len - len,
+				 "packet count: %u\n", qtx_cfg);
+		len += scnprintf(buf + len, buf_len - len,
+				 "packet drop: %u\n\n", qtx_sch);
+
+		/* Recover to normal mode */
+		cr_set_field(hnat_priv->fe_base + QTX_MIB_IF,
+			     MIB_ON_QTX_CFG, 0);
+		cr_set_field(hnat_priv->fe_base + QTX_MIB_IF, VQTX_MIB_EN, 0);
+	}
+
+	len += scnprintf(buf + len, buf_len - len,
+			 "      EN     RATE     WEIGHT\n");
+	len += scnprintf(buf + len, buf_len - len,
+			 "----------------------------\n");
+	len += scnprintf(buf + len, buf_len - len,
+			 "max%5d%9d%9d\n", max_rate_en, max_rate, max_weight);
+	len += scnprintf(buf + len, buf_len - len,
+			 "min%5d%9d        -\n", min_rate_en, min_rate);
+
+	if (len > buf_len)
+		len = buf_len;
+
+	ret_cnt = simple_read_from_buffer(user_buf, count, ppos, buf, len);
+
+	kfree(buf);
+	return ret_cnt;
+}
+
+static ssize_t hnat_queue_write(struct file *file, const char __user *buf,
+				size_t length, loff_t *offset)
+{
+	long id = (long)file->private_data;
+	struct mtk_hnat *h = hnat_priv;
+	char line[64] = {0};
+	int max_enable, max_rate, max_exp = 0;
+	int min_enable, min_rate, min_exp = 0;
+	int weight;
+	int resv;
+	int scheduler;
+	size_t size;
+	u32 qtx_sch = 0;
+
+	cr_set_field(h->fe_base + QDMA_PAGE, QTX_CFG_PAGE, (id / NUM_OF_Q_PER_PAGE));
+	if (length >= sizeof(line))
+		return -EINVAL;
+
+	if (copy_from_user(line, buf, length))
+		return -EFAULT;
+
+	if (sscanf(line, "%d %d %d %d %d %d %d", &scheduler, &min_enable, &min_rate,
+		   &max_enable, &max_rate, &weight, &resv) != 7)
+		return -EFAULT;
+
+	line[length] = '\0';
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	if (max_rate > 100000000 || max_rate < 0 ||
+	    min_rate > 100000000 || min_rate < 0)
+#else
+	if (max_rate > 10000000 || max_rate < 0 ||
+	    min_rate > 10000000 || min_rate < 0)
+#endif
+		return -EINVAL;
+
+	while (max_rate > 127) {
+		max_rate /= 10;
+		max_exp++;
+	}
+
+	while (min_rate > 127) {
+		min_rate /= 10;
+		min_exp++;
+	}
+
+	if (hnat_priv->data->num_of_sch == 4)
+		qtx_sch |= (scheduler & 0x3) << 30;
+	else
+		qtx_sch |= (scheduler & 0x1) << 31;
+	if (min_enable)
+		qtx_sch |= BIT(27);
+	qtx_sch |= (min_rate & 0x7f) << 20;
+	qtx_sch |= (min_exp & 0xf) << 16;
+	if (max_enable)
+		qtx_sch |= BIT(11);
+	qtx_sch |= (weight & 0xf) << 12;
+	qtx_sch |= (max_rate & 0x7f) << 4;
+	qtx_sch |= max_exp & 0xf;
+	writel(qtx_sch, h->fe_base + QTX_SCH(id % NUM_OF_Q_PER_PAGE));
+
+	resv &= 0xff;
+	qtx_sch = readl(h->fe_base + QTX_CFG(id % NUM_OF_Q_PER_PAGE));
+	qtx_sch &= 0xffff0000;
+	qtx_sch |= (resv << 8) | resv;
+	writel(qtx_sch, h->fe_base + QTX_CFG(id % NUM_OF_Q_PER_PAGE));
+
+	size = strlen(line);
+	*offset += size;
+
+	return length;
+}
+
+static const struct file_operations hnat_queue_fops = {
+	.open = simple_open,
+	.read = hnat_queue_show,
+	.write = hnat_queue_write,
+	.llseek = default_llseek,
+};
+
+static ssize_t hnat_ppd_if_write(struct file *file, const char __user *buffer,
+				 size_t count, loff_t *data)
+{
+	char buf[IFNAMSIZ];
+	struct net_device *dev;
+	char *p, *tmp;
+
+	if (count >= IFNAMSIZ)
+		return -EFAULT;
+
+	memset(buf, 0, IFNAMSIZ);
+	if (copy_from_user(buf, buffer, count))
+		return -EFAULT;
+
+	tmp = buf;
+	p = strsep(&tmp, "\n\r ");
+	dev = dev_get_by_name(&init_net, p);
+
+	if (dev) {
+		if (hnat_priv->g_ppdev)
+			dev_put(hnat_priv->g_ppdev);
+		hnat_priv->g_ppdev = dev;
+
+		strncpy(hnat_priv->ppd, p, IFNAMSIZ - 1);
+		pr_info("hnat_priv ppd = %s\n", hnat_priv->ppd);
+	} else {
+		pr_info("no such device!\n");
+	}
+
+	return count;
+}
+
+static int hnat_ppd_if_read(struct seq_file *m, void *private)
+{
+	pr_info("hnat_priv ppd = %s\n", hnat_priv->ppd);
+
+	if (hnat_priv->g_ppdev) {
+		pr_info("hnat_priv g_ppdev name = %s\n",
+			hnat_priv->g_ppdev->name);
+	} else {
+		pr_info("hnat_priv g_ppdev is null!\n");
+	}
+
+	return 0;
+}
+
+static int hnat_ppd_if_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_ppd_if_read, file->private_data);
+}
+
+static const struct file_operations hnat_ppd_if_fops = {
+	.open = hnat_ppd_if_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_ppd_if_write,
+	.release = single_release,
+};
+
+static int hnat_mape_toggle_read(struct seq_file *m, void *private)
+{
+	pr_info("value=%d, %s is enabled now!\n", mape_toggle, (mape_toggle) ? "mape" : "ds-lite");
+
+	return 0;
+}
+
+static int hnat_mape_toggle_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_mape_toggle_read, file->private_data);
+}
+
+static ssize_t hnat_mape_toggle_write(struct file *file, const char __user *buffer,
+				      size_t count, loff_t *data)
+{
+	char buf = 0;
+	int i;
+	u32 ppe_cfg;
+	
+	if ((count < 1) || copy_from_user(&buf, buffer, sizeof(buf)))
+		return -EFAULT;
+
+	if (buf == '1') {
+		pr_info("mape is going to be enabled, ds-lite is going to be disabled !\n");
+		mape_toggle = 1;
+	} else if (buf == '0') {
+		pr_info("ds-lite is going to be enabled, mape is going to be disabled !\n");
+		mape_toggle = 0;
+	} else {
+		pr_info("Invalid parameter.\n");
+		return -EFAULT;
+	}
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		ppe_cfg = readl(hnat_priv->ppe_base[i] + PPE_FLOW_CFG);
+
+		if (mape_toggle)
+			ppe_cfg &= ~BIT_IPV4_DSL_EN;
+		else
+			ppe_cfg |= BIT_IPV4_DSL_EN;
+
+		writel(ppe_cfg, hnat_priv->ppe_base[i] + PPE_FLOW_CFG);
+	}
+
+	return count;
+}
+
+static const struct file_operations hnat_mape_toggle_fops = {
+	.open = hnat_mape_toggle_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_mape_toggle_write,
+	.release = single_release,
+};
+
+static int hnat_hook_toggle_read(struct seq_file *m, void *private)
+{
+	pr_info("value=%d, hook is %s now!\n", hook_toggle, (hook_toggle) ? "enabled" : "disabled");
+
+	return 0;
+}
+
+static int hnat_hook_toggle_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_hook_toggle_read, file->private_data);
+}
+
+static ssize_t hnat_hook_toggle_write(struct file *file, const char __user *buffer,
+				      size_t count, loff_t *data)
+{
+	char buf[8] = {0};
+	int len = count;
+	u32 id;
+
+	if ((len > 8) || copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	if (buf[0] == '1' && !hook_toggle) {
+		pr_info("hook is going to be enabled !\n");
+		hnat_enable_hook();
+
+		if (IS_PPPQ_MODE) {
+			for (id = 0; id < MAX_PPPQ_PORT_NUM; id++)
+				hnat_qos_shaper_ebl(id, 1);
+		}
+	} else if (buf[0] == '0' && hook_toggle) {
+		pr_info("hook is going to be disabled !\n");
+		hnat_disable_hook();
+
+		if (IS_PPPQ_MODE) {
+			for (id = 0; id < MAX_PPPQ_PORT_NUM; id++)
+				hnat_qos_shaper_ebl(id, 0);
+		}
+	}
+
+	return len;
+}
+
+static const struct file_operations hnat_hook_toggle_fops = {
+	.open = hnat_hook_toggle_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_hook_toggle_write,
+	.release = single_release,
+};
+
+static int hnat_xlat_toggle_read(struct seq_file *m, void *private)
+{
+	pr_info("value=%d, xlat is %s now!\n",
+		xlat_toggle, (xlat_toggle) ? "enabled" : "disabled");
+
+	return 0;
+}
+
+static int hnat_xlat_toggle_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_xlat_toggle_read, file->private_data);
+}
+
+static ssize_t hnat_xlat_toggle_write(struct file *file,
+				      const char __user *buffer,
+				      size_t count, loff_t *data)
+{
+	char buf[8] = {0};
+	int len = count;
+	int i;
+	u32 ppe_cfg;
+
+	if ((len > 8) || copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	if (buf[0] == '1' && !xlat_toggle) {
+		pr_info("xlat is going to be enabled !\n");
+		xlat_toggle = 1;
+	} else if (buf[0] == '0' && xlat_toggle) {
+		pr_info("xlat is going to be disabled !\n");
+		xlat_toggle = 0;
+	}
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		ppe_cfg = readl(hnat_priv->ppe_base[i] + PPE_FLOW_CFG);
+
+		if (xlat_toggle)
+			ppe_cfg |= BIT_IPV6_464XLAT_EN;
+		else
+			ppe_cfg &= ~BIT_IPV6_464XLAT_EN;
+
+		writel(ppe_cfg, hnat_priv->ppe_base[i] + PPE_FLOW_CFG);
+	}
+
+	return len;
+}
+
+static const struct file_operations hnat_xlat_toggle_fops = {
+	.open = hnat_xlat_toggle_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_xlat_toggle_write,
+	.release = single_release,
+};
+
+int mtk_ppe_get_xlat_v6_by_v4(u32 *ipv4, struct in6_addr *ipv6,
+			      struct in6_addr *prefix)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct map46 *m = NULL;
+
+	list_for_each_entry(m, &h->xlat.map_list, list) {
+		if (m->ipv4 == *ipv4) {
+			memcpy(ipv6, &m->ipv6, sizeof(*ipv6));
+			memcpy(prefix, &h->xlat.prefix, sizeof(*ipv6));
+			return 0;
+		}
+	}
+
+	return -1;
+}
+
+int mtk_ppe_get_xlat_v4_by_v6(struct in6_addr *ipv6, u32 *ipv4)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct map46 *m = NULL;
+
+	list_for_each_entry(m, &h->xlat.map_list, list) {
+		if (ipv6_addr_equal(ipv6, &m->ipv6)) {
+			*ipv4 = m->ipv4;
+			return 0;
+		}
+	}
+
+	return -1;
+}
+
+static int hnat_xlat_cfg_read(struct seq_file *m, void *private)
+{
+	pr_info("\n464XLAT Config Command Usage:\n");
+	pr_info("Show HQoS usage:\n");
+	pr_info("    cat /sys/kernel/debug/hnat/xlat_cfg\n");
+	pr_info("Set ipv6 prefix :\n");
+	pr_info("    echo prefix <prefix> > /sys/kernel/debug/hnat/xlat_cfg\n");
+	pr_info("Set ipv6 prefix len :\n");
+	pr_info("    echo pfx_len <len> > /sys/kernel/debug/hnat/xlat_cfg\n");
+	pr_info("Add map :\n");
+	pr_info("echo map add <ipv4> <ipv6> > /sys/kernel/debug/hnat/xlat_cfg\n");
+	pr_info("Delete map :\n");
+	pr_info("echo map del <ipv4> <ipv6> > /sys/kernel/debug/hnat/xlat_cfg\n");
+	pr_info("Show config:\n");
+	pr_info("echo show > /sys/kernel/debug/hnat/xlat_cfg\n");
+
+	return 0;
+}
+
+static int hnat_xlat_cfg_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_xlat_cfg_read, file->private_data);
+}
+
+static ssize_t hnat_xlat_cfg_write(struct file *file, const char __user *buffer,
+				      size_t count, loff_t *data)
+{
+	struct mtk_hnat *h = hnat_priv;
+	int len = count;
+	char buf[256] = {0}, v4_str[64] = {0}, v6_str[64] = {0};
+	struct map46 *map = NULL, *m = NULL, *next = NULL;
+	struct in6_addr ipv6;
+	u32 ipv4;
+
+	if ((len > 256) || copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	if (!strncmp(buf, "prefix", 6)) {
+		if (sscanf(buf, "prefix %64s\n", v6_str) != 1) {
+			pr_info("input error\n");
+			return -1;
+		}
+
+		in6_pton(v6_str, -1, (u8 *)&h->xlat.prefix, -1, NULL);
+		pr_info("set prefix = %pI6\n", &h->xlat.prefix);
+	} else if (!strncmp(buf, "pfx_len", 7)) {
+		if (sscanf(buf, "pfx_len %3d", &h->xlat.prefix_len) != 1) {
+			pr_info("input error\n");
+			return -1;
+		}
+
+		pr_info("set pfx_len = %d\n", h->xlat.prefix_len);
+	} else if (!strncmp(buf, "map add", 7)) {
+		if (sscanf(buf, "map add %64s %64s\n", v4_str, v6_str) != 2) {
+			pr_info("input error\n");
+			return -1;
+		}
+
+		map = kmalloc(sizeof(struct map46), GFP_KERNEL);
+		if (!map)
+			return -1;
+
+		in4_pton(v4_str, -1, (u8 *)&map->ipv4, -1, NULL);
+		in6_pton(v6_str, -1, (u8 *)&map->ipv6, -1, NULL);
+		list_for_each_entry(m, &h->xlat.map_list, list) {
+			if (ipv6_addr_equal(&map->ipv6, &m->ipv6) &&
+			    map->ipv4 == m->ipv4) {
+				pr_info("this map already added.\n");
+				kfree(map);
+				return -1;
+			}
+		}
+
+		list_add(&map->list, &h->xlat.map_list);
+		pr_info("add map: %pI4<=>%pI6\n", &map->ipv4, &map->ipv6);
+	} else if (!strncmp(buf, "map del", 7)) {
+		if (sscanf(buf, "map del %64s %64s\n", v4_str, v6_str) != 2) {
+			pr_info("input error\n");
+			return -1;
+		}
+
+		in4_pton(v4_str, -1, (u8 *)&ipv4, -1, NULL);
+		in6_pton(v6_str, -1, (u8 *)&ipv6, -1, NULL);
+
+		list_for_each_entry_safe(m, next, &h->xlat.map_list, list) {
+			if (ipv6_addr_equal(&ipv6, &m->ipv6) &&
+			    ipv4 == m->ipv4) {
+				list_del(&m->list);
+				kfree(m);
+				pr_info("del map: %s<=>%s\n", v4_str, v6_str);
+				return len;
+			}
+		}
+
+		pr_info("not found map: %s<=>%s\n", v4_str, v6_str);
+	} else if (!strncmp(buf, "show", 4)) {
+		pr_info("prefix=%pI6\n", &h->xlat.prefix);
+		pr_info("prefix_len=%d\n", h->xlat.prefix_len);
+
+		list_for_each_entry(m, &h->xlat.map_list, list) {
+			pr_info("map: %pI4<=>%pI6\n", &m->ipv4, &m->ipv6);
+		}
+	} else {
+		pr_info("input error\n");
+		return -1;
+	}
+
+	return len;
+}
+
+static const struct file_operations hnat_xlat_cfg_fops = {
+	.open = hnat_xlat_cfg_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_xlat_cfg_write,
+	.release = single_release,
+};
+
+static void hnat_qos_toggle_usage(void)
+{
+	pr_info("\nHQoS toggle Command Usage:\n");
+	pr_info("Show HQoS mode:\n");
+	pr_info("    cat /sys/kernel/debug/hnat/qos_toggle\n");
+	pr_info("Disable HQoS mode:\n");
+	pr_info("    echo 0 > /sys/kernel/debug/hnat/qos_toggle\n");
+	pr_info("Enable HQoS on bidirection:\n");
+	pr_info("    echo 1 > /sys/kernel/debug/hnat/qos_toggle\n");
+	pr_info("Enable HQoS on uplink only:\n");
+	pr_info("    echo 1 uplink > /sys/kernel/debug/hnat/qos_toggle\n");
+	pr_info("Enable HQoS on downlink only:\n");
+	pr_info("    echo 1 downlink > /sys/kernel/debug/hnat/qos_toggle\n");
+	pr_info("Enable Per-port-per-queue mode:\n");
+	pr_info("    echo 2 > /sys/kernel/debug/hnat/qos_toggle\n");
+	pr_info("Show HQoS toggle usage:\n");
+	pr_info("    echo 3 > /sys/kernel/debug/hnat/qos_toggle\n\n");
+}
+
+static int hnat_qos_toggle_read(struct seq_file *m, void *private)
+{
+	if (qos_toggle == 0) {
+		pr_info("HQoS is disabled now!\n");
+	} else if (qos_toggle == 1) {
+		pr_info("HQoS is enabled now!\n");
+		pr_info("HQoS uplink is %s now!\n",
+				qos_ul_toggle ? "enabled" : "disabled");
+		pr_info("HQoS downlink is %s now!\n",
+				qos_dl_toggle ? "enabled" : "disabled");
+	} else if (qos_toggle == 2) {
+		pr_info("Per-port-per-queue mode is enabled!\n");
+	}
+
+	return 0;
+}
+
+static int hnat_qos_toggle_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_qos_toggle_read, file->private_data);
+}
+
+void hnat_qos_shaper_ebl(u32 id, u32 enable)
+{
+	struct mtk_hnat *h = hnat_priv;
+	u32 cfg;
+
+	if (enable) {
+		cfg = QTX_SCH_MIN_RATE_EN | QTX_SCH_MAX_RATE_EN;
+		cfg |= (1 << QTX_SCH_MIN_RATE_MAN_OFFSET) |
+		       (4 << QTX_SCH_MIN_RATE_EXP_OFFSET) |
+		       (25 << QTX_SCH_MAX_RATE_MAN_OFFSET) |
+		       (5 << QTX_SCH_MAX_RATE_EXP_OFFSET) |
+		       (4 << QTX_SCH_MAX_RATE_WGHT_OFFSET);
+
+		writel(cfg, h->fe_base + QTX_SCH(id % NUM_OF_Q_PER_PAGE));
+	} else {
+		writel(0, h->fe_base + QTX_SCH(id % NUM_OF_Q_PER_PAGE));
+	}
+}
+
+static void hnat_qos_disable(void)
+{
+	struct mtk_hnat *h = hnat_priv;
+	u32 id, cfg;
+
+	for (id = 0; id < MAX_PPPQ_PORT_NUM; id++) {
+		hnat_qos_shaper_ebl(id, 0);
+		writel((4 << QTX_CFG_HW_RESV_CNT_OFFSET) |
+		       (4 << QTX_CFG_SW_RESV_CNT_OFFSET),
+		       h->fe_base + QTX_CFG(id % NUM_OF_Q_PER_PAGE));
+	}
+
+	cfg = (QDMA_TX_SCH_WFQ_EN) | (QDMA_TX_SCH_WFQ_EN << 16);
+	for (id = 0; id < h->data->num_of_sch; id += 2) {
+		if (h->data->num_of_sch == 4)
+			writel(cfg, h->fe_base + QDMA_TX_4SCH_BASE(id));
+		else
+			writel(cfg, h->fe_base + QDMA_TX_2SCH_BASE);
+	}
+}
+
+static void hnat_qos_pppq_enable(void)
+{
+	struct mtk_hnat *h = hnat_priv;
+	u32 id, cfg;
+
+	for (id = 0; id < MAX_PPPQ_PORT_NUM; id++) {
+		if (hook_toggle)
+			hnat_qos_shaper_ebl(id, 1);
+		else
+			hnat_qos_shaper_ebl(id, 0);
+
+		writel((4 << QTX_CFG_HW_RESV_CNT_OFFSET) |
+		       (4 << QTX_CFG_SW_RESV_CNT_OFFSET),
+		       h->fe_base + QTX_CFG(id % NUM_OF_Q_PER_PAGE));
+	}
+
+	cfg = (QDMA_TX_SCH_WFQ_EN) | (QDMA_TX_SCH_WFQ_EN << 16);
+	for (id = 0; id < h->data->num_of_sch; id+= 2) {
+		if (h->data->num_of_sch == 4)
+                        writel(cfg, h->fe_base + QDMA_TX_4SCH_BASE(id));
+                else
+                        writel(cfg, h->fe_base + QDMA_TX_2SCH_BASE);
+	}
+}
+
+static ssize_t hnat_qos_toggle_write(struct file *file, const char __user *buffer,
+				     size_t count, loff_t *data)
+{
+	char buf[32] = {0}, tmp[32];
+	int len = count;
+	char *p_buf = NULL, *p_token = NULL;
+
+	if (len  >= sizeof(buf))
+		return -EFAULT;
+
+	if (copy_from_user(buf, buffer, len))
+		return -EFAULT;
+
+	buf[len] = '\0';
+
+	if (buf[0] == '0') {
+		pr_info("HQoS is going to be disabled!\n");
+		qos_toggle = 0;
+		qos_dl_toggle = 0;
+		qos_ul_toggle = 0;
+		hnat_qos_disable();
+	} else if (buf[0] == '1') {
+		p_buf = buf;
+		p_token = strsep(&p_buf, " \t");
+		if (p_buf) {
+			memcpy(tmp, p_buf, strlen(p_buf));
+			tmp[len] = '\0';
+			if (!strncmp(tmp, "uplink", 6)) {
+				qos_dl_toggle = 0;
+				qos_ul_toggle = 1;
+			} else if (!strncmp(tmp, "downlink", 8)) {
+				qos_ul_toggle = 0;
+				qos_dl_toggle = 1;
+			} else {
+				pr_info("Direction should be uplink or downlink.\n");
+				hnat_qos_toggle_usage();
+				return len;
+			}
+		} else {
+			qos_ul_toggle = 1;
+			qos_dl_toggle = 1;
+		}
+		pr_info("HQoS mode is going to be enabled!\n");
+		pr_info("HQoS uplink is going to be %s!\n",
+				qos_ul_toggle ? "enabled" : "disabled");
+		pr_info("HQoS downlink is going to be %s!\n",
+				qos_dl_toggle ? "enabled" : "disabled");
+		qos_toggle = 1;
+	} else if (buf[0] == '2') {
+		pr_info("Per-port-per-queue mode is going to be enabled!\n");
+		pr_info("PPPQ use qid 0~5 (scheduler 0).\n");
+		qos_toggle = 2;
+		qos_dl_toggle = 1;
+		qos_ul_toggle = 1;
+		hnat_qos_pppq_enable();
+	} else if (buf[0] == '3') {
+		hnat_qos_toggle_usage();
+	} else {
+		pr_info("Input error!\n");
+		hnat_qos_toggle_usage();
+	}
+
+	return len;
+}
+
+static const struct file_operations hnat_qos_toggle_fops = {
+	.open = hnat_qos_toggle_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_qos_toggle_write,
+	.release = single_release,
+};
+
+static int hnat_version_read(struct seq_file *m, void *private)
+{
+	pr_info("HNAT SW version : %s\nHNAT HW version : %d\n", HNAT_SW_VER, hnat_priv->data->version);
+
+	return 0;
+}
+
+static int hnat_version_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_version_read, file->private_data);
+}
+
+static const struct file_operations hnat_version_fops = {
+	.open = hnat_version_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.release = single_release,
+};
+
+u32 hnat_get_ppe_hash(struct foe_entry *entry)
+{
+	u32 hv1 = 0, hv2 = 0, hv3 = 0, hash = 0;
+
+	switch (entry->bfib1.pkt_type) {
+	case IPV4_HNAPT:
+	case IPV4_HNAT:
+	case IPV4_DSLITE:
+		hv1 = entry->ipv4_hnapt.sport << 16 | entry->ipv4_hnapt.dport;
+		hv2 = entry->ipv4_hnapt.dip;
+		hv3 = entry->ipv4_hnapt.sip;
+		break;
+	case IPV6_3T_ROUTE:
+	case IPV6_5T_ROUTE:
+	case IPV6_6RD:
+		hv1 = entry->ipv6_5t_route.ipv6_sip3 ^
+			  entry->ipv6_5t_route.ipv6_dip3;
+		hv1 ^= entry->ipv6_5t_route.sport << 16 |
+			   entry->ipv6_5t_route.dport;
+		hv2 = entry->ipv6_5t_route.ipv6_sip2 ^
+			  entry->ipv6_5t_route.ipv6_dip2;
+		hv2 ^= entry->ipv6_5t_route.ipv6_dip0;
+		hv3 = entry->ipv6_5t_route.ipv6_sip1 ^
+			  entry->ipv6_5t_route.ipv6_dip1;
+		hv3 ^= entry->ipv6_5t_route.ipv6_sip0;
+		break;
+	}
+
+	hash = (hv1 & hv2) | ((~hv1) & hv3);
+	hash = (hash >> 24) | ((hash & 0xffffff) << 8);
+	hash ^= hv1 ^ hv2 ^ hv3;
+	hash ^= hash >> 16;
+	hash <<= 2;
+	hash &= hnat_priv->foe_etry_num - 1;
+
+	return hash;
+}
+
+static u32 hnat_char2hex(const char c)
+{
+	switch (c) {
+	case '0'...'9':
+		return 0x0 + (c - '0');
+	case 'a'...'f':
+		return 0xa + (c - 'a');
+	case 'A'...'F':
+		return 0xa + (c - 'A');
+	default:
+		pr_info("MAC format error\n");
+		return 0;
+	}
+}
+
+static void hnat_parse_mac(char *str, char *mac)
+{
+	int i;
+
+	for (i = 0; i < ETH_ALEN; i++) {
+		mac[i] = (hnat_char2hex(str[i * 3]) << 4) +
+			 (hnat_char2hex(str[i * 3 + 1]));
+	}
+}
+
+static void hnat_static_entry_help(void)
+{
+	pr_info("-------------------- Usage --------------------\n");
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	pr_info("echo $0 $1 $2 ... $15 > /sys/kernel/debug/hnat/static_entry\n\n");
+#else
+	pr_info("echo $0 $1 $2 ... $12 > /sys/kernel/debug/hnat/static_entry\n\n");
+#endif
+
+	pr_info("-------------------- Parameters --------------------\n");
+	pr_info("$0:	HASH		OCT\n");
+	pr_info("$1:	INFO1		HEX\n");
+	pr_info("$2:	ING SIPv4	HEX\n");
+	pr_info("$3:	ING DIPv4	HEX\n");
+	pr_info("$4:	ING SP		HEX\n");
+	pr_info("$5:	ING DP		HEX\n");
+	pr_info("$6:	INFO2		HEX\n");
+	pr_info("$7:	EG SIPv4	HEX\n");
+	pr_info("$8:	EG DIPv4	HEX\n");
+	pr_info("$9:	EG SP		HEX\n");
+	pr_info("$10:	EG DP		HEX\n");
+	pr_info("$11:	DMAC		STR (00:11:22:33:44:55)\n");
+	pr_info("$12:	SMAC		STR (00:11:22:33:44:55)\n");
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	pr_info("$13:	TPORT IDX	HEX\n");
+	pr_info("$14:	TOPS ENTRY	HEX\n");
+	pr_info("$15:	CDRT IDX	HEX\n");
+#endif
+}
+
+static int hnat_static_entry_read(struct seq_file *m, void *private)
+{
+	hnat_static_entry_help();
+
+	return 0;
+}
+
+static int hnat_static_entry_open(struct inode *inode, struct file *file)
+{
+	return single_open(file, hnat_static_entry_read, file->private_data);
+}
+
+static ssize_t hnat_static_entry_write(struct file *file,
+				       const char __user *buffer,
+				       size_t count, loff_t *data)
+{
+	struct foe_entry *foe, entry = { 0 };
+	char buf[256], dmac_str[18], smac_str[18], dmac[6], smac[6];
+	int len = count, hash, coll = 0;
+	u32 ppe_id = 0;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	u32 tport_id, tops_entry, cdrt_id;
+#endif
+
+	if (len >= sizeof(buf) || copy_from_user(buf, buffer, len)) {
+		pr_info("Input handling fail!\n");
+		len = sizeof(buf) - 1;
+		return -EFAULT;
+	}
+
+	buf[len] = '\0';
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	if (sscanf(buf,
+		   "%5d %8x %8x %8x %hx %hx %8x %8x %8x %hx %hx %18s %18s %4x %4x %4x",
+		   &hash,
+		   &entry.ipv4_hnapt.info_blk1,
+		   &entry.ipv4_hnapt.sip,
+		   &entry.ipv4_hnapt.dip,
+		   &entry.ipv4_hnapt.sport,
+		   &entry.ipv4_hnapt.dport,
+		   &entry.ipv4_hnapt.info_blk2,
+		   &entry.ipv4_hnapt.new_sip,
+		   &entry.ipv4_hnapt.new_dip,
+		   &entry.ipv4_hnapt.new_sport,
+		   &entry.ipv4_hnapt.new_dport,
+		   dmac_str, smac_str, &tport_id, &tops_entry, &cdrt_id) != 16)
+		return -EFAULT;
+
+	if ((hash >= (int)hnat_priv->foe_etry_num) || (hash < -1) ||
+	    (TPORT_ID(tport_id) != tport_id) ||
+	    (TOPS_ENTRY(tops_entry) != tops_entry) ||
+	    (CDRT_ID(cdrt_id) != cdrt_id)) {
+		hnat_static_entry_help();
+		return -EFAULT;
+	}
+
+	entry.ipv4_hnapt.tport_id = tport_id;
+	entry.ipv4_hnapt.tops_entry = tops_entry;
+	entry.ipv4_hnapt.cdrt_id = cdrt_id;
+#else
+	if (sscanf(buf,
+		   "%5d %8x %8x %8x %hx %hx %8x %8x %8x %hx %hx %18s %18s",
+		   &hash,
+		   &entry.ipv4_hnapt.info_blk1,
+		   &entry.ipv4_hnapt.sip,
+		   &entry.ipv4_hnapt.dip,
+		   &entry.ipv4_hnapt.sport,
+		   &entry.ipv4_hnapt.dport,
+		   &entry.ipv4_hnapt.info_blk2,
+		   &entry.ipv4_hnapt.new_sip,
+		   &entry.ipv4_hnapt.new_dip,
+		   &entry.ipv4_hnapt.new_sport,
+		   &entry.ipv4_hnapt.new_dport,
+		   dmac_str, smac_str) != 13)
+		return -EFAULT;
+
+	if ((hash >= (int)hnat_priv->foe_etry_num) || (hash < -1)) {
+		hnat_static_entry_help();
+		return -EFAULT;
+	}
+#endif
+
+	hnat_parse_mac(smac_str, smac);
+	hnat_parse_mac(dmac_str, dmac);
+	entry.ipv4_hnapt.dmac_hi = swab32(*((u32 *)dmac));
+	entry.ipv4_hnapt.dmac_lo = swab16(*((u16 *)&dmac[4]));
+	entry.ipv4_hnapt.smac_hi = swab32(*((u32 *)smac));
+	entry.ipv4_hnapt.smac_lo = swab16(*((u16 *)&smac[4]));
+
+	if (hash == -1)
+		hash = hnat_get_ppe_hash(&entry);
+
+	foe = &hnat_priv->foe_table_cpu[ppe_id][hash];
+	while ((foe->ipv4_hnapt.bfib1.state == BIND) && (coll < 4)) {
+		hash++;
+		coll++;
+		foe = &hnat_priv->foe_table_cpu[ppe_id][hash];
+	};
+	memcpy(foe, &entry, sizeof(entry));
+
+	debug_level = 7;
+	entry_detail(ppe_id, hash);
+
+	return len;
+}
+
+static const struct file_operations hnat_static_fops = {
+	.open = hnat_static_entry_open,
+	.read = seq_read,
+	.llseek = seq_lseek,
+	.write = hnat_static_entry_write,
+	.release = single_release,
+};
+
+int get_ppe_mib(u32 ppe_id, int index, u64 *pkt_cnt, u64 *byte_cnt)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct hnat_accounting *acct;
+	struct foe_entry *entry;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	if (index < 0 || index >= h->foe_etry_num) {
+		pr_info("Invalid entry index\n");
+		return -EINVAL;
+	}
+
+	acct = hnat_get_count(h, ppe_id, index, NULL);
+	entry = hnat_priv->foe_table_cpu[ppe_id] + index;
+
+	if (!acct)
+		return -1;
+
+	if (entry->bfib1.state != BIND)
+		return -1;
+
+	*pkt_cnt = acct->packets;
+	*byte_cnt = acct->bytes;
+
+	return 0;
+}
+EXPORT_SYMBOL(get_ppe_mib);
+
+int is_entry_binding(u32 ppe_id, int index)
+{
+	struct mtk_hnat *h = hnat_priv;
+	struct foe_entry *entry;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	if (index < 0 || index >= h->foe_etry_num) {
+		pr_info("Invalid entry index\n");
+		return -EINVAL;
+	}
+
+	entry = hnat_priv->foe_table_cpu[ppe_id] + index;
+
+	return entry->bfib1.state == BIND;
+}
+EXPORT_SYMBOL(is_entry_binding);
+
+#define dump_register(nm)                                                      \
+	{                                                                      \
+		.name = __stringify(nm), .offset = PPE_##nm,                   \
+	}
+
+static const struct debugfs_reg32 hnat_regs[] = {
+	dump_register(GLO_CFG),     dump_register(FLOW_CFG),
+	dump_register(IP_PROT_CHK), dump_register(IP_PROT_0),
+	dump_register(IP_PROT_1),   dump_register(IP_PROT_2),
+	dump_register(IP_PROT_3),   dump_register(TB_CFG),
+	dump_register(TB_BASE),     dump_register(TB_USED),
+	dump_register(BNDR),	dump_register(BIND_LMT_0),
+	dump_register(BIND_LMT_1),  dump_register(KA),
+	dump_register(UNB_AGE),     dump_register(BND_AGE_0),
+	dump_register(BND_AGE_1),   dump_register(HASH_SEED),
+	dump_register(DFT_CPORT),   dump_register(MCAST_PPSE),
+	dump_register(MCAST_L_0),   dump_register(MCAST_H_0),
+	dump_register(MCAST_L_1),   dump_register(MCAST_H_1),
+	dump_register(MCAST_L_2),   dump_register(MCAST_H_2),
+	dump_register(MCAST_L_3),   dump_register(MCAST_H_3),
+	dump_register(MCAST_L_4),   dump_register(MCAST_H_4),
+	dump_register(MCAST_L_5),   dump_register(MCAST_H_5),
+	dump_register(MCAST_L_6),   dump_register(MCAST_H_6),
+	dump_register(MCAST_L_7),   dump_register(MCAST_H_7),
+	dump_register(MCAST_L_8),   dump_register(MCAST_H_8),
+	dump_register(MCAST_L_9),   dump_register(MCAST_H_9),
+	dump_register(MCAST_L_A),   dump_register(MCAST_H_A),
+	dump_register(MCAST_L_B),   dump_register(MCAST_H_B),
+	dump_register(MCAST_L_C),   dump_register(MCAST_H_C),
+	dump_register(MCAST_L_D),   dump_register(MCAST_H_D),
+	dump_register(MCAST_L_E),   dump_register(MCAST_H_E),
+	dump_register(MCAST_L_F),   dump_register(MCAST_H_F),
+	dump_register(MTU_DRP),     dump_register(MTU_VLYR_0),
+	dump_register(MTU_VLYR_1),  dump_register(MTU_VLYR_2),
+	dump_register(VPM_TPID),    dump_register(VPM_TPID),
+	dump_register(CAH_CTRL),    dump_register(CAH_TAG_SRH),
+	dump_register(CAH_LINE_RW), dump_register(CAH_WDATA),
+	dump_register(CAH_RDATA),
+};
+
+int hnat_init_debugfs(struct mtk_hnat *h)
+{
+	int ret = 0;
+	struct dentry *root;
+	struct dentry *file;
+	long i;
+	char name[16];
+
+	root = debugfs_create_dir("hnat", NULL);
+	if (!root) {
+		dev_notice(h->dev, "%s:err at %d\n", __func__, __LINE__);
+		ret = -ENOMEM;
+		goto err0;
+	}
+	h->root = root;
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		h->regset[i] = kzalloc(sizeof(*h->regset[i]), GFP_KERNEL);
+		if (!h->regset[i]) {
+			dev_notice(h->dev, "%s:err at %d\n", __func__, __LINE__);
+			ret = -ENOMEM;
+			goto err1;
+		}
+		h->regset[i]->regs = hnat_regs;
+		h->regset[i]->nregs = ARRAY_SIZE(hnat_regs);
+		h->regset[i]->base = h->ppe_base[i];
+
+		ret = snprintf(name, sizeof(name), "regdump%ld", i);
+		if (ret != strlen(name)) {
+			ret = -ENOMEM;
+			goto err1;
+		}
+		file = debugfs_create_regset32(name, 0444,
+					       root, h->regset[i]);
+		if (!file) {
+			dev_notice(h->dev, "%s:err at %d\n", __func__, __LINE__);
+			ret = -ENOMEM;
+			goto err1;
+		}
+	}
+
+	debugfs_create_file("all_entry", 0444, root, h, &hnat_debug_fops);
+	debugfs_create_file("external_interface", 0444, root, h,
+			    &hnat_ext_fops);
+	debugfs_create_file("whnat_interface", 0444, root, h,
+			    &hnat_whnat_fops);
+	debugfs_create_file("cpu_reason", 0444, root, h,
+			    &cpu_reason_fops);
+	debugfs_create_file("hnat_entry", 0444, root, h,
+			    &hnat_entry_fops);
+	debugfs_create_file("hnat_setting", 0444, root, h,
+			    &hnat_setting_fops);
+	debugfs_create_file("mcast_table", 0444, root, h,
+			    &hnat_mcast_fops);
+	debugfs_create_file("hook_toggle", 0444, root, h,
+			    &hnat_hook_toggle_fops);
+	debugfs_create_file("mape_toggle", 0444, root, h,
+			    &hnat_mape_toggle_fops);
+	debugfs_create_file("qos_toggle", 0444, root, h,
+			    &hnat_qos_toggle_fops);
+	debugfs_create_file("hnat_version", 0444, root, h,
+			    &hnat_version_fops);
+	debugfs_create_file("hnat_ppd_if", 0444, root, h,
+			    &hnat_ppd_if_fops);
+	debugfs_create_file("static_entry", 0444, root, h,
+			    &hnat_static_fops);
+	debugfs_create_file("xlat_toggle", 0444, root, h,
+			    &hnat_xlat_toggle_fops);
+	debugfs_create_file("xlat_cfg", 0444, root, h,
+			    &hnat_xlat_cfg_fops);
+
+	for (i = 0; i < hnat_priv->data->num_of_sch; i++) {
+		ret = snprintf(name, sizeof(name), "qdma_sch%ld", i);
+		if (ret != strlen(name)) {
+			ret = -ENOMEM;
+			goto err1;
+		}
+		debugfs_create_file(name, 0444, root, (void *)i,
+				    &hnat_sched_fops);
+	}
+
+	for (i = 0; i < MTK_QDMA_TX_NUM; i++) {
+		ret = snprintf(name, sizeof(name), "qdma_txq%ld", i);
+		if (ret != strlen(name)) {
+			ret = -ENOMEM;
+			goto err1;
+		}
+		debugfs_create_file(name, 0444, root, (void *)i,
+				    &hnat_queue_fops);
+	}
+
+	return 0;
+
+err1:
+	debugfs_remove_recursive(root);
+err0:
+	return ret;
+}
+
+void hnat_deinit_debugfs(struct mtk_hnat *h)
+{
+	int i;
+
+	debugfs_remove_recursive(h->root);
+	h->root = NULL;
+
+	for (i = 0; i < CFG_PPE_NUM; i++)
+		kfree(h->regset[i]);
+}
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.c b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.c
new file mode 100644
index 00000000..edf17cb4
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.c
@@ -0,0 +1,354 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Zhiqiang Yang <zhiqiang.yang@mediatek.com>
+ */
+#include <net/sock.h>
+#include <linux/netlink.h>
+#include <linux/rtnetlink.h>
+#include <linux/if_bridge.h>
+#include "hnat.h"
+
+/* *
+ * mcast_entry_get - Returns the index of an unused entry
+ * or an already existed entry in mtbl
+ */
+static int mcast_entry_get(u16 vlan_id, u32 dst_mac)
+{
+	int index = -1;
+	u8 i;
+	struct ppe_mcast_group *p = hnat_priv->pmcast->mtbl;
+	u8 max = hnat_priv->pmcast->max_entry;
+
+	for (i = 0; i < max; i++) {
+		if ((index == -1) && (!p->valid)) {
+			index = i; /*get the first unused entry index*/
+			continue;
+		}
+		if ((p->vid == vlan_id) && (p->mac_hi == dst_mac)) {
+			index = i;
+			break;
+		}
+		p++;
+	}
+	if (index == -1)
+		pr_info("%s:group table is full\n", __func__);
+
+	return index;
+}
+
+static void get_mac_from_mdb_entry(struct br_mdb_entry *entry,
+				   u32 *mac_hi, u16 *mac_lo)
+{
+	switch (ntohs(entry->addr.proto)) {
+	case ETH_P_IP:
+		*mac_lo = 0x0100;
+		*mac_hi = swab32((entry->addr.u.ip4 & 0xfffffe00) + 0x5e);
+		break;
+	case ETH_P_IPV6:
+		*mac_lo = 0x3333;
+		*mac_hi = swab32(entry->addr.u.ip6.s6_addr32[3]);
+		break;
+	}
+	trace_printk("%s:group mac_h=0x%08x, mac_l=0x%04x\n",
+		     __func__, *mac_hi, *mac_lo);
+}
+
+/*set_hnat_mtbl - set ppe multicast register*/
+static int set_hnat_mtbl(struct ppe_mcast_group *group, u32 ppe_id, int index)
+{
+	struct ppe_mcast_h mcast_h;
+	struct ppe_mcast_l mcast_l;
+	u16 mac_lo = group->mac_lo;
+	u32 mac_hi = group->mac_hi;
+	u8 mc_port = group->mc_port;
+	void __iomem *reg;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	mcast_h.u.value = 0;
+	mcast_l.addr = 0;
+	if (mac_lo == 0x0100)
+		mcast_h.u.info.mc_mpre_sel = 0;
+	else if (mac_lo == 0x3333)
+		mcast_h.u.info.mc_mpre_sel = 1;
+
+	mcast_h.u.info.mc_px_en = mc_port;
+	mcast_l.addr = mac_hi;
+	mcast_h.u.info.valid = group->valid;
+	trace_printk("%s:index=%d,group info=0x%x,addr=0x%x\n",
+		     __func__, index, mcast_h.u.value, mcast_l.addr);
+	if (index < 0x10) {
+		reg = hnat_priv->ppe_base[ppe_id] + PPE_MCAST_H_0 + ((index) * 8);
+		writel(mcast_h.u.value, reg);
+		reg = hnat_priv->ppe_base[ppe_id] + PPE_MCAST_L_0 + ((index) * 8);
+		writel(mcast_l.addr, reg);
+	} else {
+		index = index - 0x10;
+		reg = hnat_priv->fe_base + PPE_MCAST_H_10 + ((index) * 8);
+		writel(mcast_h.u.value, reg);
+		reg = hnat_priv->fe_base + PPE_MCAST_L_10 + ((index) * 8);
+		writel(mcast_h.u.value, reg);
+	}
+
+	return 0;
+}
+
+/**
+ * hnat_mcast_table_update -
+ *	1.get a valid group entry
+ *	2.update group info
+ *		a.update eif&oif count
+ *		b.eif ==0 & oif == 0,delete it from group table
+ *		c.oif != 0,set mc forward port to cpu,else do not forward to cpu
+ *	3.set the group info to ppe register
+ */
+static int hnat_mcast_table_update(int type, struct br_mdb_entry *entry)
+{
+	struct net_device *dev;
+	u32 mac_hi = 0;
+	u16 mac_lo = 0;
+	int i, index;
+	struct ppe_mcast_group *group;
+
+	rcu_read_lock();
+	dev = dev_get_by_index_rcu(&init_net, entry->ifindex);
+	if (!dev) {
+		rcu_read_unlock();
+		return -ENODEV;
+	}
+	rcu_read_unlock();
+
+	get_mac_from_mdb_entry(entry, &mac_hi, &mac_lo);
+	index = mcast_entry_get(entry->vid, mac_hi);
+	if (index == -1)
+		return -1;
+
+	group = &hnat_priv->pmcast->mtbl[index];
+	group->mac_hi = mac_hi;
+	group->mac_lo = mac_lo;
+	switch (type) {
+	case RTM_NEWMDB:
+		if (IS_LAN(dev) || IS_WAN(dev))
+			group->eif++;
+		else
+			group->oif++;
+		group->vid = entry->vid;
+		group->valid = true;
+		break;
+	case RTM_DELMDB:
+		if (group->valid) {
+			if (IS_LAN(dev) || IS_WAN(dev))
+				group->eif--;
+			else
+				group->oif--;
+			}
+		break;
+	}
+	trace_printk("%s:devname=%s,eif=%d,oif=%d\n", __func__,
+		     dev->name, group->eif, group->oif);
+	if (group->valid) {
+		if (group->oif && group->eif)
+			/*eth&wifi both in group,forward to cpu&GDMA1*/
+			group->mc_port = (MCAST_TO_PDMA || MCAST_TO_GDMA1);
+		else if (group->oif)
+			/*only wifi in group,forward to cpu only*/
+			group->mc_port = MCAST_TO_PDMA;
+		else
+			/*only eth in group,forward to GDMA1 only*/
+			group->mc_port = MCAST_TO_GDMA1;
+		if (!group->oif && !group->eif)
+			/*nobody in this group,clear the entry*/
+			memset(group, 0, sizeof(struct ppe_mcast_group));
+
+		for (i = 0; i < CFG_PPE_NUM; i++)
+			set_hnat_mtbl(group, i, index);
+	}
+
+	return 0;
+}
+
+static void hnat_mcast_nlmsg_handler(struct work_struct *work)
+{
+	struct sk_buff *skb = NULL;
+	struct nlmsghdr *nlh;
+	struct nlattr *nest, *nest2, *info;
+	struct br_port_msg *bpm;
+	struct br_mdb_entry *entry;
+	struct ppe_mcast_table *pmcast;
+	struct sock *sk;
+
+	pmcast = container_of(work, struct ppe_mcast_table, work);
+	sk = pmcast->msock->sk;
+
+	while ((skb = skb_dequeue(&sk->sk_receive_queue))) {
+		nlh = nlmsg_hdr(skb);
+		if (!nlmsg_ok(nlh, skb->len)) {
+			kfree_skb(skb);
+			continue;
+		}
+		bpm = nlmsg_data(nlh);
+		nest = nlmsg_find_attr(nlh, sizeof(bpm), MDBA_MDB);
+		if (!nest) {
+			kfree_skb(skb);
+			continue;
+		}
+		nest2 = nla_find_nested(nest, MDBA_MDB_ENTRY);
+		if (nest2) {
+			info = nla_find_nested(nest2, MDBA_MDB_ENTRY_INFO);
+			if (!info) {
+				kfree_skb(skb);
+				continue;
+			}
+
+			entry = (struct br_mdb_entry *)nla_data(info);
+			trace_printk("%s:cmd=0x%2x,ifindex=0x%x,state=0x%x",
+				     __func__, nlh->nlmsg_type,
+				     entry->ifindex, entry->state);
+			trace_printk("vid=0x%x,ip=0x%x,proto=0x%x\n",
+				     entry->vid, entry->addr.u.ip4,
+				     entry->addr.proto);
+			hnat_mcast_table_update(nlh->nlmsg_type, entry);
+		}
+		kfree_skb(skb);
+	}
+}
+
+static void hnat_mcast_nlmsg_rcv(struct sock *sk)
+{
+	struct ppe_mcast_table *pmcast = hnat_priv->pmcast;
+	struct workqueue_struct *queue = pmcast->queue;
+	struct work_struct *work = &pmcast->work;
+
+	queue_work(queue, work);
+}
+
+static struct socket *hnat_mcast_netlink_open(struct net *net)
+{
+	struct socket *sock = NULL;
+	int ret;
+	struct sockaddr_nl addr;
+
+	ret = sock_create_kern(net, PF_NETLINK, SOCK_RAW, NETLINK_ROUTE, &sock);
+	if (ret < 0)
+		goto out;
+
+	sock->sk->sk_data_ready = hnat_mcast_nlmsg_rcv;
+	addr.nl_family = PF_NETLINK;
+	addr.nl_pid = 65536; /*fix me:how to get an unique id?*/
+	addr.nl_groups = RTMGRP_MDB;
+	ret = sock->ops->bind(sock, (struct sockaddr *)&addr, sizeof(addr));
+	if (ret < 0)
+		goto out;
+
+	return sock;
+out:
+	if (sock)
+		sock_release(sock);
+
+	return NULL;
+}
+
+static void hnat_mcast_check_timestamp(struct timer_list *t)
+{
+	struct foe_entry *entry;
+	int i, hash_index;
+	u16 e_ts, foe_ts;
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		for (hash_index = 0; hash_index < hnat_priv->foe_etry_num; hash_index++) {
+			entry = hnat_priv->foe_table_cpu[i] + hash_index;
+			if (entry->bfib1.sta == 1) {
+				e_ts = (entry->ipv4_hnapt.m_timestamp) & 0xffff;
+				foe_ts = foe_timestamp(hnat_priv);
+				if ((foe_ts - e_ts) > 0x3000)
+					foe_ts = (~(foe_ts)) & 0xffff;
+				if (abs(foe_ts - e_ts) > 20)
+					entry_delete(i, hash_index);
+			}
+		}
+	}
+	mod_timer(&hnat_priv->hnat_mcast_check_timer, jiffies + 10 * HZ);
+}
+
+int hnat_mcast_enable(u32 ppe_id)
+{
+	struct ppe_mcast_table *pmcast;
+
+	if (ppe_id >= CFG_PPE_NUM)
+		return -EINVAL;
+
+	pmcast = kzalloc(sizeof(*pmcast), GFP_KERNEL);
+	if (!pmcast)
+		return -1;
+
+	if (hnat_priv->data->version == MTK_HNAT_V1_1)
+		pmcast->max_entry = 0x10;
+	else
+		pmcast->max_entry = MAX_MCAST_ENTRY;
+
+	INIT_WORK(&pmcast->work, hnat_mcast_nlmsg_handler);
+	pmcast->queue = create_singlethread_workqueue("ppe_mcast");
+	if (!pmcast->queue)
+		goto err1;
+
+	pmcast->msock = hnat_mcast_netlink_open(&init_net);
+	if (!pmcast->msock)
+		goto err2;
+
+	hnat_priv->pmcast = pmcast;
+
+	/* mt7629 should checkout mcast entry life time manualy */
+	if (hnat_priv->data->version == MTK_HNAT_V1_3) {
+		timer_setup(&hnat_priv->hnat_mcast_check_timer,
+			    hnat_mcast_check_timestamp, 0);
+		hnat_priv->hnat_mcast_check_timer.expires = jiffies;
+		add_timer(&hnat_priv->hnat_mcast_check_timer);
+	}
+
+	/* Enable multicast table lookup */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_GLO_CFG, MCAST_TB_EN, 1);
+	/* multicast port0 map to PDMA */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MCAST_PPSE, MC_P0_PPSE, 0);
+	/* multicast port1 map to GMAC1 */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MCAST_PPSE, MC_P1_PPSE, 1);
+	/* multicast port2 map to GMAC2 */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MCAST_PPSE, MC_P2_PPSE, 2);
+	/* multicast port3 map to QDMA */
+	cr_set_field(hnat_priv->ppe_base[ppe_id] + PPE_MCAST_PPSE, MC_P3_PPSE, 5);
+
+	return 0;
+err2:
+	if (pmcast->queue)
+		destroy_workqueue(pmcast->queue);
+err1:
+	kfree(pmcast);
+
+	return -1;
+}
+
+int hnat_mcast_disable(void)
+{
+	struct ppe_mcast_table *pmcast = hnat_priv->pmcast;
+
+	if (!pmcast)
+		return -EINVAL;
+
+	if (hnat_priv->data->version == MTK_HNAT_V1_3)
+		del_timer_sync(&hnat_priv->hnat_mcast_check_timer);
+
+	flush_work(&pmcast->work);
+	destroy_workqueue(pmcast->queue);
+	sock_release(pmcast->msock);
+	kfree(pmcast);
+
+	return 0;
+}
+
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.h b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.h
new file mode 100644
index 00000000..ad5b5d1e
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_mcast.h
@@ -0,0 +1,69 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Zhiqiang Yang <zhiqiang.yang@mediatek.com>
+ */
+
+#ifndef NF_HNAT_MCAST_H
+#define NF_HNAT_MCAST_H
+
+#define RTMGRP_IPV4_MROUTE 0x20
+#define RTMGRP_MDB 0x2000000
+
+#define MAX_MCAST_ENTRY 64
+
+#define MCAST_TO_PDMA (0x1 << 0)
+#define MCAST_TO_GDMA1 (0x1 << 1)
+#define MCAST_TO_GDMA2 (0x1 << 2)
+
+struct ppe_mcast_group {
+	u32 mac_hi; /*multicast mac addr*/
+	u16 mac_lo; /*multicast mac addr*/
+	u16 vid;
+	u8 mc_port; /*1:forward to cpu,2:forward to GDMA1,4:forward to GDMA2*/
+	u8 eif; /*num of eth if added to multi group. */
+	u8 oif; /* num of other if added to multi group ,ex wifi.*/
+	bool valid;
+};
+
+struct ppe_mcast_table {
+	struct workqueue_struct *queue;
+	struct work_struct work;
+	struct socket *msock;
+	struct ppe_mcast_group mtbl[MAX_MCAST_ENTRY];
+	u8 max_entry;
+};
+
+struct ppe_mcast_h {
+	union {
+		u32 value;
+		struct {
+			u32 mc_vid:12;
+			u32 mc_qos_qid54:2; /* mt7622 only */
+			u32 valid:1;
+			u32 rev1:1;
+			/*0:forward to cpu,1:forward to GDMA1*/
+			u32 mc_px_en:4;
+			u32 mc_mpre_sel:2; /* 0=01:00, 2=33:33 */
+			u32 mc_vid_cmp:1;
+			u32 rev2:1;
+			u32 mc_px_qos_en:4;
+			u32 mc_qos_qid:4;
+		} info;
+	} u;
+};
+
+struct ppe_mcast_l {
+	u32 addr;
+};
+
+int hnat_mcast_enable(u32 ppe_id);
+int hnat_mcast_disable(void);
+
+#endif
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_nf_hook.c b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_nf_hook.c
new file mode 100644
index 00000000..d34ac9de
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_nf_hook.c
@@ -0,0 +1,2945 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Sean Wang <sean.wang@mediatek.com>
+ *   Copyright (C) 2016-2017 John Crispin <blogic@openwrt.org>
+ */
+
+#include <linux/netfilter_bridge.h>
+#include <linux/netfilter_ipv6.h>
+
+#include <net/arp.h>
+#include <net/neighbour.h>
+#include <net/netfilter/nf_conntrack_helper.h>
+#include <net/netfilter/nf_flow_table.h>
+#include <net/ipv6.h>
+#include <net/ip6_route.h>
+#include <net/ip.h>
+#include <net/tcp.h>
+#include <net/udp.h>
+#include <net/netfilter/nf_conntrack.h>
+#include <net/netfilter/nf_conntrack_acct.h>
+
+#include "nf_hnat_mtk.h"
+#include "hnat.h"
+
+#include "../mtk_eth_soc.h"
+#include "../mtk_eth_reset.h"
+
+#define do_ge2ext_fast(dev, skb)                                               \
+	((IS_LAN_GRP(dev) || IS_WAN(dev) || IS_PPD(dev)) && \
+	 skb_hnat_is_hashed(skb) && \
+	 skb_hnat_reason(skb) == HIT_BIND_FORCE_TO_CPU)
+#define do_ext2ge_fast_learn(dev, skb)                                         \
+	(IS_PPD(dev) &&                                                        \
+	 (skb_hnat_sport(skb) == NR_PDMA_PORT ||                           \
+	  skb_hnat_sport(skb) == NR_QDMA_PORT) &&                       \
+	  ((get_dev_from_index(skb->vlan_tci & VLAN_VID_MASK)) ||   \
+		 get_wandev_from_index(skb->vlan_tci & VLAN_VID_MASK)))
+#define do_mape_w2l_fast(dev, skb)                                          \
+		(mape_toggle && IS_WAN(dev) && (!is_from_mape(skb)))
+
+static struct ipv6hdr mape_l2w_v6h;
+static struct ipv6hdr mape_w2l_v6h;
+static inline uint8_t get_wifi_hook_if_index_from_dev(const struct net_device *dev)
+{
+	int i;
+
+	for (i = 1; i < MAX_IF_NUM; i++) {
+		if (hnat_priv->wifi_hook_if[i] == dev)
+			return i;
+	}
+
+	return 0;
+}
+
+static inline int get_ext_device_number(void)
+{
+	int i, number = 0;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++)
+		number += 1;
+	return number;
+}
+
+static inline int find_extif_from_devname(const char *name)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (!strcmp(name, ext_entry->name))
+			return 1;
+	}
+	return 0;
+}
+
+static inline int get_index_from_dev(const struct net_device *dev)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (dev == ext_entry->dev)
+			return ext_entry->dev->ifindex;
+	}
+	return 0;
+}
+
+static inline struct net_device *get_dev_from_index(int index)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+	struct net_device *dev = 0;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (ext_entry->dev && index == ext_entry->dev->ifindex) {
+			dev = ext_entry->dev;
+			break;
+		}
+	}
+	return dev;
+}
+
+static inline struct net_device *get_wandev_from_index(int index)
+{
+	if (!hnat_priv->g_wandev)
+		hnat_priv->g_wandev = dev_get_by_name(&init_net, hnat_priv->wan);
+
+	if (hnat_priv->g_wandev && hnat_priv->g_wandev->ifindex == index)
+		return hnat_priv->g_wandev;
+	return NULL;
+}
+
+static inline int extif_set_dev(struct net_device *dev)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (!strcmp(dev->name, ext_entry->name) && !ext_entry->dev) {
+			dev_hold(dev);
+			ext_entry->dev = dev;
+			pr_info("%s(%s)\n", __func__, dev->name);
+
+			return ext_entry->dev->ifindex;
+		}
+	}
+
+	return -1;
+}
+
+static inline int extif_put_dev(struct net_device *dev)
+{
+	int i;
+	struct extdev_entry *ext_entry;
+
+	for (i = 0; i < MAX_EXT_DEVS && hnat_priv->ext_if[i]; i++) {
+		ext_entry = hnat_priv->ext_if[i];
+		if (ext_entry->dev == dev) {
+			ext_entry->dev = NULL;
+			dev_put(dev);
+			pr_info("%s(%s)\n", __func__, dev->name);
+
+			return 0;
+		}
+	}
+
+	return -1;
+}
+
+int ext_if_add(struct extdev_entry *ext_entry)
+{
+	int len = get_ext_device_number();
+
+	if (len < MAX_EXT_DEVS)
+		hnat_priv->ext_if[len++] = ext_entry;
+
+	return len;
+}
+
+int ext_if_del(struct extdev_entry *ext_entry)
+{
+	int i, j;
+
+	for (i = 0; i < MAX_EXT_DEVS; i++) {
+		if (hnat_priv->ext_if[i] == ext_entry) {
+			for (j = i; hnat_priv->ext_if[j] && j < MAX_EXT_DEVS - 1; j++)
+				hnat_priv->ext_if[j] = hnat_priv->ext_if[j + 1];
+			hnat_priv->ext_if[j] = NULL;
+			break;
+		}
+	}
+
+	return i;
+}
+
+void foe_clear_all_bind_entries(void)
+{
+	int i, hash_index;
+	struct foe_entry *entry;
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		cr_set_field(hnat_priv->ppe_base[i] + PPE_TB_CFG,
+			     SMA, SMA_ONLY_FWD_CPU);
+
+		for (hash_index = 0; hash_index < hnat_priv->foe_etry_num; hash_index++) {
+			entry = hnat_priv->foe_table_cpu[i] + hash_index;
+			if (entry->bfib1.state == BIND) {
+				entry->ipv4_hnapt.udib1.state = INVALID;
+				entry->ipv4_hnapt.udib1.time_stamp =
+					readl((hnat_priv->fe_base + 0x0010)) & 0xFF;
+			}
+		}
+	}
+
+	/* clear HWNAT cache */
+	hnat_cache_ebl(1);
+
+	mod_timer(&hnat_priv->hnat_sma_build_entry_timer, jiffies + 3 * HZ);
+}
+
+static void gmac_ppe_fwd_enable(struct net_device *dev)
+{
+	if (IS_LAN(dev) || IS_GMAC1_MODE)
+		set_gmac_ppe_fwd(NR_GMAC1_PORT, 1);
+	else if (IS_WAN(dev))
+		set_gmac_ppe_fwd(NR_GMAC2_PORT, 1);
+	else if (IS_LAN2(dev))
+		set_gmac_ppe_fwd(NR_GMAC3_PORT, 1);
+}
+
+int nf_hnat_netdevice_event(struct notifier_block *unused, unsigned long event,
+			    void *ptr)
+{
+	struct net_device *dev;
+
+	dev = netdev_notifier_info_to_dev(ptr);
+
+	switch (event) {
+	case NETDEV_UP:
+		gmac_ppe_fwd_enable(dev);
+
+		extif_set_dev(dev);
+
+		break;
+	case NETDEV_GOING_DOWN:
+		if (!get_wifi_hook_if_index_from_dev(dev))
+			extif_put_dev(dev);
+
+		if (!IS_LAN_GRP(dev) && !IS_WAN(dev) &&
+		    !find_extif_from_devname(dev->name) &&
+		    !dev->netdev_ops->ndo_flow_offload_check)
+			break;
+
+		foe_clear_all_bind_entries();
+
+		break;
+	case NETDEV_UNREGISTER:
+		if (hnat_priv->g_ppdev == dev) {
+			hnat_priv->g_ppdev = NULL;
+			dev_put(dev);
+		}
+		if (hnat_priv->g_wandev == dev) {
+			hnat_priv->g_wandev = NULL;
+			dev_put(dev);
+		}
+
+		break;
+	case NETDEV_REGISTER:
+		if (IS_PPD(dev) && !hnat_priv->g_ppdev)
+			hnat_priv->g_ppdev = dev_get_by_name(&init_net, hnat_priv->ppd);
+		if (IS_WAN(dev) && !hnat_priv->g_wandev)
+			hnat_priv->g_wandev = dev_get_by_name(&init_net, hnat_priv->wan);
+
+		break;
+	case MTK_FE_RESET_NAT_DONE:
+		pr_info("[%s] HNAT driver starts to do warm init !\n", __func__);
+		hnat_warm_init();
+		break;
+	default:
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+void foe_clear_entry(struct neighbour *neigh)
+{
+	u32 *daddr = (u32 *)neigh->primary_key;
+	unsigned char h_dest[ETH_ALEN];
+	struct foe_entry *entry;
+	int i, hash_index;
+	u32 dip;
+
+	dip = (u32)(*daddr);
+
+	for (i = 0; i < CFG_PPE_NUM; i++) {
+		if (!hnat_priv->foe_table_cpu[i])
+			continue;
+
+		for (hash_index = 0; hash_index < hnat_priv->foe_etry_num; hash_index++) {
+			entry = hnat_priv->foe_table_cpu[i] + hash_index;
+			if (entry->bfib1.state == BIND &&
+			    entry->ipv4_hnapt.new_dip == ntohl(dip) &&
+			    IS_IPV4_HNAPT(entry)) {
+				*((u32 *)h_dest) = swab32(entry->ipv4_hnapt.dmac_hi);
+				*((u16 *)&h_dest[4]) =
+					swab16(entry->ipv4_hnapt.dmac_lo);
+				if (strncmp(h_dest, neigh->ha, ETH_ALEN) != 0) {
+					cr_set_field(hnat_priv->ppe_base[i] + PPE_TB_CFG,
+						     SMA, SMA_ONLY_FWD_CPU);
+
+					entry->ipv4_hnapt.udib1.state = INVALID;
+					entry->ipv4_hnapt.udib1.time_stamp =
+						readl((hnat_priv->fe_base + 0x0010)) & 0xFF;
+
+					/* clear HWNAT cache */
+					hnat_cache_ebl(1);
+
+					mod_timer(&hnat_priv->hnat_sma_build_entry_timer,
+						  jiffies + 3 * HZ);
+
+					if (debug_level >= 7) {
+						pr_info("%s: state=%d\n", __func__,
+							neigh->nud_state);
+						pr_info("Delete old entry: dip =%pI4\n", &dip);
+						pr_info("Old mac= %pM\n", h_dest);
+						pr_info("New mac= %pM\n", neigh->ha);
+					}
+				}
+			}
+		}
+	}
+}
+
+int nf_hnat_netevent_handler(struct notifier_block *unused, unsigned long event,
+			     void *ptr)
+{
+	struct net_device *dev = NULL;
+	struct neighbour *neigh = NULL;
+
+	switch (event) {
+	case NETEVENT_NEIGH_UPDATE:
+		neigh = ptr;
+		dev = neigh->dev;
+		if (dev)
+			foe_clear_entry(neigh);
+		break;
+	}
+
+	return NOTIFY_DONE;
+}
+
+unsigned int mape_add_ipv6_hdr(struct sk_buff *skb, struct ipv6hdr mape_ip6h)
+{
+	struct ethhdr *eth = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	struct iphdr *iph = NULL;
+
+	if (skb_headroom(skb) < IPV6_HDR_LEN || skb_shared(skb) ||
+	    (skb_cloned(skb) && !skb_clone_writable(skb, 0))) {
+		return -1;
+	}
+
+	/* point to L3 */
+	memcpy(skb->data - IPV6_HDR_LEN - ETH_HLEN, skb_push(skb, ETH_HLEN), ETH_HLEN);
+	memcpy(skb_push(skb, IPV6_HDR_LEN - ETH_HLEN), &mape_ip6h, IPV6_HDR_LEN);
+
+	eth = (struct ethhdr *)(skb->data - ETH_HLEN);
+	eth->h_proto = htons(ETH_P_IPV6);
+	skb->protocol = htons(ETH_P_IPV6);
+
+	iph = (struct iphdr *)(skb->data + IPV6_HDR_LEN);
+	ip6h = (struct ipv6hdr *)(skb->data);
+	ip6h->payload_len = iph->tot_len; /* maybe different with ipv4 */
+
+	skb_set_network_header(skb, 0);
+	skb_set_transport_header(skb, iph->ihl * 4 + IPV6_HDR_LEN);
+	return 0;
+}
+
+static void fix_skb_packet_type(struct sk_buff *skb, struct net_device *dev,
+				struct ethhdr *eth)
+{
+	skb->pkt_type = PACKET_HOST;
+	if (unlikely(is_multicast_ether_addr(eth->h_dest))) {
+		if (ether_addr_equal_64bits(eth->h_dest, dev->broadcast))
+			skb->pkt_type = PACKET_BROADCAST;
+		else
+			skb->pkt_type = PACKET_MULTICAST;
+	}
+}
+
+unsigned int do_hnat_ext_to_ge(struct sk_buff *skb, const struct net_device *in,
+			       const char *func)
+{
+	if (hnat_priv->g_ppdev && hnat_priv->g_ppdev->flags & IFF_UP) {
+		u16 vlan_id = 0;
+		skb_set_network_header(skb, 0);
+		skb_push(skb, ETH_HLEN);
+		set_to_ppe(skb);
+
+		vlan_id = skb_vlan_tag_get_id(skb);
+		if (vlan_id) {
+			skb = vlan_insert_tag(skb, skb->vlan_proto, skb->vlan_tci);
+			if (!skb)
+				return -1;
+		}
+
+		/*set where we come from*/
+		skb->vlan_proto = htons(ETH_P_8021Q);
+		skb->vlan_tci =
+			(VLAN_CFI_MASK | (in->ifindex & VLAN_VID_MASK));
+		trace_printk(
+			"%s: vlan_prot=0x%x, vlan_tci=%x, in->name=%s, skb->dev->name=%s\n",
+			__func__, ntohs(skb->vlan_proto), skb->vlan_tci,
+			in->name, hnat_priv->g_ppdev->name);
+		skb->dev = hnat_priv->g_ppdev;
+		dev_queue_xmit(skb);
+		trace_printk("%s: called from %s successfully\n", __func__, func);
+		return 0;
+	}
+
+	trace_printk("%s: called from %s fail\n", __func__, func);
+	return -1;
+}
+
+unsigned int do_hnat_ext_to_ge2(struct sk_buff *skb, const char *func)
+{
+	struct ethhdr *eth = eth_hdr(skb);
+	struct net_device *dev;
+	struct foe_entry *entry;
+
+	trace_printk("%s: vlan_prot=0x%x, vlan_tci=%x\n", __func__,
+		     ntohs(skb->vlan_proto), skb->vlan_tci);
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return -1;
+
+	dev = get_dev_from_index(skb->vlan_tci & VLAN_VID_MASK);
+
+	if (dev) {
+		/*set where we to go*/
+		skb->dev = dev;
+		skb->vlan_proto = 0;
+		skb->vlan_tci = 0;
+
+		if (ntohs(eth->h_proto) == ETH_P_8021Q) {
+			skb = skb_vlan_untag(skb);
+			if (unlikely(!skb))
+				return -1;
+		}
+
+		if (IS_BOND_MODE &&
+		    (((hnat_priv->data->version == MTK_HNAT_V2 ||
+		       hnat_priv->data->version == MTK_HNAT_V3) &&
+				(skb_hnat_entry(skb) != 0x7fff)) ||
+		     ((hnat_priv->data->version != MTK_HNAT_V2 &&
+		       hnat_priv->data->version != MTK_HNAT_V3) &&
+				(skb_hnat_entry(skb) != 0x3fff))))
+			skb_set_hash(skb, skb_hnat_entry(skb) >> 1, PKT_HASH_TYPE_L4);
+
+		set_from_extge(skb);
+		fix_skb_packet_type(skb, skb->dev, eth);
+		netif_rx(skb);
+		trace_printk("%s: called from %s successfully\n", __func__,
+			     func);
+		return 0;
+	} else {
+		/* MapE WAN --> LAN/WLAN PingPong. */
+		dev = get_wandev_from_index(skb->vlan_tci & VLAN_VID_MASK);
+		if (mape_toggle && dev) {
+			if (!mape_add_ipv6_hdr(skb, mape_w2l_v6h)) {
+				skb_set_mac_header(skb, -ETH_HLEN);
+				skb->dev = dev;
+				set_from_mape(skb);
+				skb->vlan_proto = 0;
+				skb->vlan_tci = 0;
+				fix_skb_packet_type(skb, skb->dev, eth_hdr(skb));
+				entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+				entry->bfib1.pkt_type = IPV4_HNAPT;
+				netif_rx(skb);
+				return 0;
+			}
+		}
+		trace_printk("%s: called from %s fail\n", __func__, func);
+		return -1;
+	}
+}
+
+unsigned int do_hnat_ge_to_ext(struct sk_buff *skb, const char *func)
+{
+	/*set where we to go*/
+	u8 index;
+	struct foe_entry *entry;
+	struct net_device *dev;
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return -1;
+
+	entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+
+	if (IS_IPV4_GRP(entry))
+		index = entry->ipv4_hnapt.act_dp & UDF_PINGPONG_IFIDX;
+	else
+		index = entry->ipv6_5t_route.act_dp & UDF_PINGPONG_IFIDX;
+
+	dev = get_dev_from_index(index);
+	if (!dev) {
+		trace_printk("%s: called from %s. Get wifi interface fail\n",
+			     __func__, func);
+		return 0;
+	}
+
+	skb->dev = dev;
+
+	if (IS_HQOS_MODE && eth_hdr(skb)->h_proto == HQOS_MAGIC_TAG) {
+		skb = skb_unshare(skb, GFP_ATOMIC);
+		if (!skb)
+			return NF_ACCEPT;
+
+		if (unlikely(!pskb_may_pull(skb, VLAN_HLEN)))
+			return NF_ACCEPT;
+
+		skb_pull_rcsum(skb, VLAN_HLEN);
+
+		memmove(skb->data - ETH_HLEN, skb->data - ETH_HLEN - VLAN_HLEN,
+			2 * ETH_ALEN);
+	}
+
+	if (skb->dev) {
+		skb_set_network_header(skb, 0);
+		skb_push(skb, ETH_HLEN);
+		dev_queue_xmit(skb);
+		trace_printk("%s: called from %s successfully\n", __func__,
+			     func);
+		return 0;
+	} else {
+		if (mape_toggle) {
+			/* Add ipv6 header mape for lan/wlan -->wan */
+			dev = get_wandev_from_index(index);
+			if (dev) {
+				if (!mape_add_ipv6_hdr(skb, mape_l2w_v6h)) {
+					skb_set_network_header(skb, 0);
+					skb_push(skb, ETH_HLEN);
+					skb_set_mac_header(skb, 0);
+					skb->dev = dev;
+					dev_queue_xmit(skb);
+					return 0;
+				}
+				trace_printk("%s: called from %s fail[MapE]\n", __func__,
+					     func);
+				return -1;
+			}
+		}
+	}
+	/*if external devices is down, invalidate related ppe entry*/
+	if (entry_hnat_is_bound(entry)) {
+		entry->bfib1.state = INVALID;
+		if (IS_IPV4_GRP(entry))
+			entry->ipv4_hnapt.act_dp &= ~UDF_PINGPONG_IFIDX;
+		else
+			entry->ipv6_5t_route.act_dp &= ~UDF_PINGPONG_IFIDX;
+
+		/* clear HWNAT cache */
+		hnat_cache_ebl(1);
+	}
+	trace_printk("%s: called from %s fail, index=%x\n", __func__,
+		     func, index);
+	return -1;
+}
+
+static void pre_routing_print(struct sk_buff *skb, const struct net_device *in,
+			      const struct net_device *out, const char *func)
+{
+	trace_printk(
+		"[%s]: %s(iif=0x%x CB2=0x%x)-->%s (ppe_hash=0x%x) sport=0x%x reason=0x%x alg=0x%x from %s\n",
+		__func__, in->name, skb_hnat_iface(skb),
+		HNAT_SKB_CB2(skb)->magic, out->name, skb_hnat_entry(skb),
+		skb_hnat_sport(skb), skb_hnat_reason(skb), skb_hnat_alg(skb),
+		func);
+}
+
+static void post_routing_print(struct sk_buff *skb, const struct net_device *in,
+			       const struct net_device *out, const char *func)
+{
+	trace_printk(
+		"[%s]: %s(iif=0x%x, CB2=0x%x)-->%s (ppe_hash=0x%x) sport=0x%x reason=0x%x alg=0x%x from %s\n",
+		__func__, in->name, skb_hnat_iface(skb),
+		HNAT_SKB_CB2(skb)->magic, out->name, skb_hnat_entry(skb),
+		skb_hnat_sport(skb), skb_hnat_reason(skb), skb_hnat_alg(skb),
+		func);
+}
+
+static inline void hnat_set_iif(const struct nf_hook_state *state,
+				struct sk_buff *skb, int val)
+{
+	if (IS_WHNAT(state->in) && FROM_WED(skb)) {
+		return;
+	} else if (IS_LAN(state->in)) {
+		skb_hnat_iface(skb) = FOE_MAGIC_GE_LAN;
+	} else if (IS_LAN2(state->in)) {
+		skb_hnat_iface(skb) = FOE_MAGIC_GE_LAN2;
+	} else if (IS_PPD(state->in)) {
+		skb_hnat_iface(skb) = FOE_MAGIC_GE_PPD;
+	} else if (IS_EXT(state->in)) {
+		skb_hnat_iface(skb) = FOE_MAGIC_EXT;
+	} else if (IS_WAN(state->in)) {
+		skb_hnat_iface(skb) = FOE_MAGIC_GE_WAN;
+	} else if (!IS_BR(state->in)) {
+		if (state->in->netdev_ops->ndo_flow_offload_check) {
+			skb_hnat_iface(skb) = FOE_MAGIC_GE_VIRTUAL;
+		} else {
+			skb_hnat_iface(skb) = FOE_INVALID;
+
+			if (is_magic_tag_valid(skb) &&
+			    IS_SPACE_AVAILABLE_HEAD(skb))
+				memset(skb_hnat_info(skb), 0, FOE_INFO_LEN);
+		}
+	}
+}
+
+static inline void hnat_set_alg(const struct nf_hook_state *state,
+				struct sk_buff *skb, int val)
+{
+	skb_hnat_alg(skb) = val;
+}
+
+static inline void hnat_set_head_frags(const struct nf_hook_state *state,
+				       struct sk_buff *head_skb, int val,
+				       void (*fn)(const struct nf_hook_state *state,
+						  struct sk_buff *skb, int val))
+{
+	struct sk_buff *segs = skb_shinfo(head_skb)->frag_list;
+
+	fn(state, head_skb, val);
+	while (segs) {
+		fn(state, segs, val);
+		segs = segs->next;
+	}
+}
+
+static void ppe_fill_flow_lbl(struct foe_entry *entry, struct ipv6hdr *ip6h)
+{
+	entry->ipv4_dslite.flow_lbl[0] = ip6h->flow_lbl[2];
+	entry->ipv4_dslite.flow_lbl[1] = ip6h->flow_lbl[1];
+	entry->ipv4_dslite.flow_lbl[2] = ip6h->flow_lbl[0];
+}
+
+unsigned int do_hnat_mape_w2l_fast(struct sk_buff *skb, const struct net_device *in,
+				   const char *func)
+{
+	struct ipv6hdr *ip6h = ipv6_hdr(skb);
+	struct iphdr _iphdr;
+	struct iphdr *iph;
+	struct ethhdr *eth;
+
+	/* WAN -> LAN/WLAN MapE. */
+	if (mape_toggle && (ip6h->nexthdr == NEXTHDR_IPIP)) {
+		iph = skb_header_pointer(skb, IPV6_HDR_LEN, sizeof(_iphdr), &_iphdr);
+		if (unlikely(!iph))
+			return -1;
+
+		switch (iph->protocol) {
+		case IPPROTO_UDP:
+		case IPPROTO_TCP:
+			break;
+		default:
+			return -1;
+		}
+		mape_w2l_v6h = *ip6h;
+
+		/* Remove ipv6 header. */
+		memcpy(skb->data + IPV6_HDR_LEN - ETH_HLEN,
+		       skb->data - ETH_HLEN, ETH_HLEN);
+		skb_pull(skb, IPV6_HDR_LEN - ETH_HLEN);
+		skb_set_mac_header(skb, 0);
+		skb_set_network_header(skb, ETH_HLEN);
+		skb_set_transport_header(skb, ETH_HLEN + sizeof(_iphdr));
+
+		eth = eth_hdr(skb);
+		eth->h_proto = htons(ETH_P_IP);
+		set_to_ppe(skb);
+
+		skb->vlan_proto = htons(ETH_P_8021Q);
+		skb->vlan_tci =
+		(VLAN_CFI_MASK | (in->ifindex & VLAN_VID_MASK));
+
+		if (!hnat_priv->g_ppdev)
+			hnat_priv->g_ppdev = dev_get_by_name(&init_net, hnat_priv->ppd);
+
+		skb->dev = hnat_priv->g_ppdev;
+		skb->protocol = htons(ETH_P_IP);
+
+		dev_queue_xmit(skb);
+
+		return 0;
+	}
+	return -1;
+}
+
+void mtk_464xlat_pre_process(struct sk_buff *skb)
+{
+	struct foe_entry *foe;
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return;
+
+	foe = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+	if (foe->bfib1.state != BIND &&
+	    skb_hnat_reason(skb) == HIT_UNBIND_RATE_REACH)
+		memcpy(&headroom[skb_hnat_entry(skb)], skb->head,
+		       sizeof(struct hnat_desc));
+
+	if (foe->bfib1.state == BIND)
+		memset(&headroom[skb_hnat_entry(skb)], 0,
+		       sizeof(struct hnat_desc));
+}
+
+static unsigned int is_ppe_support_type(struct sk_buff *skb)
+{
+	struct ethhdr *eth = NULL;
+	struct iphdr *iph = NULL;
+	struct ipv6hdr *ip6h = NULL;
+	struct iphdr _iphdr;
+
+	eth = eth_hdr(skb);
+	if (!is_magic_tag_valid(skb) || !IS_SPACE_AVAILABLE_HEAD(skb) ||
+	    is_broadcast_ether_addr(eth->h_dest))
+		return 0;
+
+	switch (ntohs(skb->protocol)) {
+	case ETH_P_IP:
+		iph = ip_hdr(skb);
+
+		/* do not accelerate non tcp/udp traffic */
+		if ((iph->protocol == IPPROTO_TCP) ||
+		    (iph->protocol == IPPROTO_UDP) ||
+		    (iph->protocol == IPPROTO_IPV6)) {
+			return 1;
+		}
+
+		break;
+	case ETH_P_IPV6:
+		ip6h = ipv6_hdr(skb);
+
+		if ((ip6h->nexthdr == NEXTHDR_TCP) ||
+		    (ip6h->nexthdr == NEXTHDR_UDP)) {
+			return 1;
+		} else if (ip6h->nexthdr == NEXTHDR_IPIP) {
+			iph = skb_header_pointer(skb, IPV6_HDR_LEN,
+						 sizeof(_iphdr), &_iphdr);
+			if (unlikely(!iph))
+				return 0;
+
+			if ((iph->protocol == IPPROTO_TCP) ||
+			    (iph->protocol == IPPROTO_UDP)) {
+				return 1;
+			}
+
+		}
+
+		break;
+	case ETH_P_8021Q:
+		return 1;
+	}
+
+	return 0;
+}
+
+static unsigned int
+mtk_hnat_ipv6_nf_pre_routing(void *priv, struct sk_buff *skb,
+			     const struct nf_hook_state *state)
+{
+	if (!skb)
+		goto drop;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	if (!is_ppe_support_type(skb)) {
+		hnat_set_head_frags(state, skb, 1, hnat_set_alg);
+		return NF_ACCEPT;
+	}
+
+	hnat_set_head_frags(state, skb, -1, hnat_set_iif);
+
+	pre_routing_print(skb, state->in, state->out, __func__);
+
+	/* packets from external devices -> xxx ,step 1 , learning stage & bound stage*/
+	if (do_ext2ge_fast_try(state->in, skb)) {
+		if (!do_hnat_ext_to_ge(skb, state->in, __func__))
+			return NF_STOLEN;
+		return NF_ACCEPT;
+	}
+
+	/* packets form ge -> external device
+	 * For standalone wan interface
+	 */
+	if (do_ge2ext_fast(state->in, skb)) {
+		if (!do_hnat_ge_to_ext(skb, __func__))
+			return NF_STOLEN;
+		goto drop;
+	}
+
+
+#if !(defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3))
+	/* MapE need remove ipv6 header and pingpong. */
+	if (do_mape_w2l_fast(state->in, skb)) {
+		if (!do_hnat_mape_w2l_fast(skb, state->in, __func__))
+			return NF_STOLEN;
+		else
+			return NF_ACCEPT;
+	}
+
+	if (is_from_mape(skb))
+		clr_from_extge(skb);
+#endif
+	if (xlat_toggle)
+		mtk_464xlat_pre_process(skb);
+
+	return NF_ACCEPT;
+drop:
+	if (skb)
+		printk_ratelimited(KERN_WARNING
+			"%s:drop (in_dev=%s, iif=0x%x, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, state->in->name, skb_hnat_iface(skb),
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int
+mtk_hnat_ipv4_nf_pre_routing(void *priv, struct sk_buff *skb,
+			     const struct nf_hook_state *state)
+{
+	struct flow_offload_hw_path hw_path = { .dev = skb->dev,
+						.virt_dev = skb->dev };
+
+	if (!skb)
+		goto drop;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	if (!is_ppe_support_type(skb)) {
+		hnat_set_head_frags(state, skb, 1, hnat_set_alg);
+		return NF_ACCEPT;
+	}
+
+	hnat_set_head_frags(state, skb, -1, hnat_set_iif);
+
+	/*
+	 * Avoid mistakenly binding of outer IP, ports in SW L2TP decap flow.
+	 * In pre-routing, if dev is virtual iface, TOPS module is not loaded,
+	 * and it's L2TP flow, then do not bind.
+	 */
+	if (skb_hnat_iface(skb) == FOE_MAGIC_GE_VIRTUAL
+	    && skb->dev->netdev_ops->ndo_flow_offload_check) {
+		skb->dev->netdev_ops->ndo_flow_offload_check(&hw_path);
+
+		if (hw_path.flags & FLOW_OFFLOAD_PATH_TNL)
+			skb_hnat_alg(skb) = 1;
+	}
+
+	pre_routing_print(skb, state->in, state->out, __func__);
+
+	/* packets from external devices -> xxx ,step 1 , learning stage & bound stage*/
+	if (do_ext2ge_fast_try(state->in, skb)) {
+		if (!do_hnat_ext_to_ge(skb, state->in, __func__))
+			return NF_STOLEN;
+		return NF_ACCEPT;
+	}
+
+	/* packets form ge -> external device
+	 * For standalone wan interface
+	 */
+	if (do_ge2ext_fast(state->in, skb)) {
+		if (!do_hnat_ge_to_ext(skb, __func__))
+			return NF_STOLEN;
+		goto drop;
+	}
+	if (xlat_toggle)
+		mtk_464xlat_pre_process(skb);
+
+	return NF_ACCEPT;
+drop:
+	if (skb)
+		printk_ratelimited(KERN_WARNING
+			"%s:drop (in_dev=%s, iif=0x%x, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, state->in->name, skb_hnat_iface(skb),
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int
+mtk_hnat_br_nf_local_in(void *priv, struct sk_buff *skb,
+			const struct nf_hook_state *state)
+{
+	struct vlan_ethhdr *veth;
+
+	if (!skb)
+		goto drop;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	if (IS_HQOS_MODE && hnat_priv->data->whnat) {
+		veth = (struct vlan_ethhdr *)skb_mac_header(skb);
+
+		if (eth_hdr(skb)->h_proto == HQOS_MAGIC_TAG) {
+			skb_hnat_entry(skb) = ntohs(veth->h_vlan_TCI) & 0x3fff;
+			skb_hnat_reason(skb) = HIT_BIND_FORCE_TO_CPU;
+		}
+	}
+
+	if (!HAS_HQOS_MAGIC_TAG(skb) && !is_ppe_support_type(skb)) {
+		hnat_set_head_frags(state, skb, 1, hnat_set_alg);
+		return NF_ACCEPT;
+	}
+
+	hnat_set_head_frags(state, skb, -1, hnat_set_iif);
+
+	pre_routing_print(skb, state->in, state->out, __func__);
+
+	if (unlikely(debug_level >= 7)) {
+		hnat_cpu_reason_cnt(skb);
+		if (skb_hnat_reason(skb) == dbg_cpu_reason)
+			foe_dump_pkt(skb);
+	}
+
+	/* packets from external devices -> xxx ,step 1 , learning stage & bound stage*/
+	if ((skb_hnat_iface(skb) == FOE_MAGIC_EXT) && !is_from_extge(skb) &&
+	    !is_multicast_ether_addr(eth_hdr(skb)->h_dest)) {
+		if (!hnat_priv->g_ppdev)
+			hnat_priv->g_ppdev = dev_get_by_name(&init_net, hnat_priv->ppd);
+
+		if (!do_hnat_ext_to_ge(skb, state->in, __func__))
+			return NF_STOLEN;
+		return NF_ACCEPT;
+	}
+
+	if (hnat_priv->data->whnat) {
+		if (skb_hnat_iface(skb) == FOE_MAGIC_EXT)
+			clr_from_extge(skb);
+
+		/* packets from external devices -> xxx ,step 2, learning stage */
+		if (do_ext2ge_fast_learn(state->in, skb) && (!qos_toggle ||
+		    (qos_toggle && eth_hdr(skb)->h_proto != HQOS_MAGIC_TAG))) {
+			if (!do_hnat_ext_to_ge2(skb, __func__))
+				return NF_STOLEN;
+			goto drop;
+		}
+
+		/* packets form ge -> external device */
+		if (do_ge2ext_fast(state->in, skb)) {
+			if (!do_hnat_ge_to_ext(skb, __func__))
+				return NF_STOLEN;
+			goto drop;
+		}
+	}
+
+#if !(defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3))
+	/* MapE need remove ipv6 header and pingpong. (bridge mode) */
+	if (do_mape_w2l_fast(state->in, skb)) {
+		if (!do_hnat_mape_w2l_fast(skb, state->in, __func__))
+			return NF_STOLEN;
+		else
+			return NF_ACCEPT;
+	}
+#endif
+	return NF_ACCEPT;
+drop:
+	if (skb)
+		printk_ratelimited(KERN_WARNING
+			"%s:drop (in_dev=%s, iif=0x%x, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, state->in->name, skb_hnat_iface(skb),
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int hnat_ipv6_get_nexthop(struct sk_buff *skb,
+					  const struct net_device *out,
+					  struct flow_offload_hw_path *hw_path)
+{
+	const struct in6_addr *ipv6_nexthop;
+	struct neighbour *neigh = NULL;
+	struct dst_entry *dst = skb_dst(skb);
+	struct ethhdr *eth;
+	u16 eth_pppoe_hlen = ETH_HLEN + PPPOE_SES_HLEN;
+
+	if (hw_path->flags & FLOW_OFFLOAD_PATH_PPPOE) {
+		if (ipv6_hdr(skb)->nexthdr == NEXTHDR_IPIP) {
+			eth = (struct ethhdr *)(skb->data - eth_pppoe_hlen);
+			eth->h_proto = skb->protocol;
+			ether_addr_copy(eth->h_dest, hw_path->eth_dest);
+			ether_addr_copy(eth->h_source,  hw_path->eth_src);
+		} else {
+			eth = eth_hdr(skb);
+			memcpy(eth->h_source, hw_path->eth_src, ETH_ALEN);
+			memcpy(eth->h_dest, hw_path->eth_dest, ETH_ALEN);
+		}
+
+		return 0;
+	}
+
+	rcu_read_lock_bh();
+	ipv6_nexthop =
+		rt6_nexthop((struct rt6_info *)dst, &ipv6_hdr(skb)->daddr);
+	neigh = __ipv6_neigh_lookup_noref(dst->dev, ipv6_nexthop);
+	if (unlikely(!neigh)) {
+		dev_notice(hnat_priv->dev, "%s:No neigh (daddr=%pI6)\n", __func__,
+			   &ipv6_hdr(skb)->daddr);
+		rcu_read_unlock_bh();
+		return -1;
+	}
+
+	/* why do we get all zero ethernet address ? */
+	if (!is_valid_ether_addr(neigh->ha)) {
+		rcu_read_unlock_bh();
+		return -1;
+	}
+
+	if (ipv6_hdr(skb)->nexthdr == NEXTHDR_IPIP) {
+		/*copy ether type for DS-Lite and MapE */
+		eth = (struct ethhdr *)(skb->data - ETH_HLEN);
+		eth->h_proto = skb->protocol;
+	} else {
+		eth = eth_hdr(skb);
+	}
+
+	ether_addr_copy(eth->h_dest, neigh->ha);
+	ether_addr_copy(eth->h_source, out->dev_addr);
+
+	rcu_read_unlock_bh();
+
+	return 0;
+}
+
+static unsigned int hnat_ipv4_get_nexthop(struct sk_buff *skb,
+					  const struct net_device *out,
+					  struct flow_offload_hw_path *hw_path)
+{
+	u32 nexthop;
+	struct neighbour *neigh;
+	struct dst_entry *dst = skb_dst(skb);
+	struct rtable *rt = (struct rtable *)dst;
+	struct net_device *dev = (__force struct net_device *)out;
+
+	if (hw_path->flags & FLOW_OFFLOAD_PATH_PPPOE) {
+		memcpy(eth_hdr(skb)->h_source, hw_path->eth_src, ETH_ALEN);
+		memcpy(eth_hdr(skb)->h_dest, hw_path->eth_dest, ETH_ALEN);
+		return 0;
+	}
+
+	rcu_read_lock_bh();
+	nexthop = (__force u32)rt_nexthop(rt, ip_hdr(skb)->daddr);
+	neigh = __ipv4_neigh_lookup_noref(dev, nexthop);
+	if (unlikely(!neigh)) {
+		dev_notice(hnat_priv->dev, "%s:No neigh (daddr=%pI4)\n", __func__,
+			   &ip_hdr(skb)->daddr);
+		rcu_read_unlock_bh();
+		return -1;
+	}
+
+	/* why do we get all zero ethernet address ? */
+	if (!is_valid_ether_addr(neigh->ha)) {
+		rcu_read_unlock_bh();
+		return -1;
+	}
+
+	memcpy(eth_hdr(skb)->h_dest, neigh->ha, ETH_ALEN);
+	memcpy(eth_hdr(skb)->h_source, out->dev_addr, ETH_ALEN);
+
+	rcu_read_unlock_bh();
+
+	return 0;
+}
+
+static u16 ppe_get_chkbase(struct iphdr *iph)
+{
+	u16 org_chksum = ntohs(iph->check);
+	u16 org_tot_len = ntohs(iph->tot_len);
+	u16 org_id = ntohs(iph->id);
+	u16 chksum_tmp, tot_len_tmp, id_tmp;
+	u32 tmp = 0;
+	u16 chksum_base = 0;
+
+	chksum_tmp = ~(org_chksum);
+	tot_len_tmp = ~(org_tot_len);
+	id_tmp = ~(org_id);
+	tmp = chksum_tmp + tot_len_tmp + id_tmp;
+	tmp = ((tmp >> 16) & 0x7) + (tmp & 0xFFFF);
+	tmp = ((tmp >> 16) & 0x7) + (tmp & 0xFFFF);
+	chksum_base = tmp & 0xFFFF;
+
+	return chksum_base;
+}
+
+struct foe_entry ppe_fill_L2_info(struct ethhdr *eth, struct foe_entry entry,
+				  struct flow_offload_hw_path *hw_path)
+{
+	switch ((int)entry.bfib1.pkt_type) {
+	case IPV4_HNAPT:
+	case IPV4_HNAT:
+		entry.ipv4_hnapt.dmac_hi = swab32(*((u32 *)eth->h_dest));
+		entry.ipv4_hnapt.dmac_lo = swab16(*((u16 *)&eth->h_dest[4]));
+		entry.ipv4_hnapt.smac_hi = swab32(*((u32 *)eth->h_source));
+		entry.ipv4_hnapt.smac_lo = swab16(*((u16 *)&eth->h_source[4]));
+		entry.ipv4_hnapt.pppoe_id = hw_path->pppoe_sid;
+		break;
+	case IPV4_DSLITE:
+	case IPV4_MAP_E:
+	case IPV6_6RD:
+	case IPV6_5T_ROUTE:
+	case IPV6_3T_ROUTE:
+	case IPV6_HNAPT:
+	case IPV6_HNAT:
+		entry.ipv6_5t_route.dmac_hi = swab32(*((u32 *)eth->h_dest));
+		entry.ipv6_5t_route.dmac_lo = swab16(*((u16 *)&eth->h_dest[4]));
+		entry.ipv6_5t_route.smac_hi = swab32(*((u32 *)eth->h_source));
+		entry.ipv6_5t_route.smac_lo =
+			swab16(*((u16 *)&eth->h_source[4]));
+		entry.ipv6_5t_route.pppoe_id = hw_path->pppoe_sid;
+		break;
+	}
+	return entry;
+}
+
+struct foe_entry ppe_fill_info_blk(struct ethhdr *eth, struct foe_entry entry,
+				   struct flow_offload_hw_path *hw_path)
+{
+	entry.bfib1.psn = (hw_path->flags & FLOW_OFFLOAD_PATH_PPPOE) ? 1 : 0;
+	entry.bfib1.vlan_layer += (hw_path->flags & FLOW_OFFLOAD_PATH_VLAN) ? 1 : 0;
+	entry.bfib1.vpm = (entry.bfib1.vlan_layer) ? 1 : 0;
+	entry.bfib1.cah = 1;
+	entry.bfib1.time_stamp = (hnat_priv->data->version == MTK_HNAT_V2 ||
+				  hnat_priv->data->version == MTK_HNAT_V3) ?
+		readl(hnat_priv->fe_base + 0x0010) & (0xFF) :
+		readl(hnat_priv->fe_base + 0x0010) & (0x7FFF);
+
+	switch ((int)entry.bfib1.pkt_type) {
+	case IPV4_HNAPT:
+	case IPV4_HNAT:
+		if (hnat_priv->data->mcast &&
+		    is_multicast_ether_addr(&eth->h_dest[0])) {
+			entry.ipv4_hnapt.iblk2.mcast = 1;
+			if (hnat_priv->data->version == MTK_HNAT_V1_3) {
+				entry.bfib1.sta = 1;
+				entry.ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv);
+			}
+		} else {
+			entry.ipv4_hnapt.iblk2.mcast = 0;
+		}
+
+		entry.ipv4_hnapt.iblk2.port_ag =
+			(hnat_priv->data->version == MTK_HNAT_V2 ||
+			 hnat_priv->data->version == MTK_HNAT_V3) ? 0xf : 0x3f;
+		break;
+	case IPV4_DSLITE:
+	case IPV4_MAP_E:
+	case IPV6_6RD:
+	case IPV6_5T_ROUTE:
+	case IPV6_3T_ROUTE:
+	case IPV6_HNAPT:
+	case IPV6_HNAT:
+		if (hnat_priv->data->mcast &&
+		    is_multicast_ether_addr(&eth->h_dest[0])) {
+			entry.ipv6_5t_route.iblk2.mcast = 1;
+			if (hnat_priv->data->version == MTK_HNAT_V1_3) {
+				entry.bfib1.sta = 1;
+				entry.ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv);
+			}
+		} else {
+			entry.ipv6_5t_route.iblk2.mcast = 0;
+		}
+
+		entry.ipv6_5t_route.iblk2.port_ag =
+			(hnat_priv->data->version == MTK_HNAT_V2 ||
+			 hnat_priv->data->version == MTK_HNAT_V3) ? 0xf : 0x3f;
+		break;
+	}
+	return entry;
+}
+
+static struct ethhdr *get_ipv6_ipip_ethhdr(struct sk_buff *skb,
+					   struct flow_offload_hw_path *hw_path)
+{
+	struct ethhdr *eth;
+	u16 eth_pppoe_hlen = ETH_HLEN + PPPOE_SES_HLEN;
+
+	if (hw_path->flags & FLOW_OFFLOAD_PATH_PPPOE)
+		eth = (struct ethhdr *)(skb->data - eth_pppoe_hlen);
+	else
+		eth = (struct ethhdr *)(skb->data - ETH_HLEN);
+
+	return eth;
+}
+
+static unsigned int skb_to_hnat_info(struct sk_buff *skb,
+				     const struct net_device *dev,
+				     struct foe_entry *foe,
+				     struct flow_offload_hw_path *hw_path)
+{
+	struct foe_entry entry = { 0 };
+	int whnat = IS_WHNAT(dev);
+	struct ethhdr *eth;
+	struct iphdr *iph;
+	struct ipv6hdr *ip6h;
+	struct tcpudphdr _ports;
+	const struct tcpudphdr *pptr;
+	struct nf_conn *ct;
+	enum ip_conntrack_info ctinfo;
+	u32 gmac = NR_DISCARD;
+	int udp = 0;
+	u32 qid = 0;
+	u32 port_id = 0;
+	int mape = 0;
+
+	ct = nf_ct_get(skb, &ctinfo);
+
+	if (ipv6_hdr(skb)->nexthdr == NEXTHDR_IPIP)
+		/* point to ethernet header for DS-Lite and MapE */
+		eth = get_ipv6_ipip_ethhdr(skb, hw_path);
+	else
+		eth = eth_hdr(skb);
+
+	/*do not bind multicast if PPE mcast not enable*/
+	if (!hnat_priv->data->mcast && is_multicast_ether_addr(eth->h_dest))
+		return 0;
+
+	if (whnat && is_hnat_pre_filled(foe))
+		return 0;
+
+	entry.bfib1.pkt_type = foe->udib1.pkt_type; /* Get packte type state*/
+	entry.bfib1.state = foe->udib1.state;
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+	entry.bfib1.sp = foe->udib1.sp;
+#endif
+
+	switch (ntohs(eth->h_proto)) {
+	case ETH_P_IP:
+		iph = ip_hdr(skb);
+		/* Do not bind if pkt is fragmented */
+		if (ip_is_fragment(iph))
+			return 0;
+
+		switch (iph->protocol) {
+		case IPPROTO_UDP:
+			udp = 1;
+			/* fallthrough */
+		case IPPROTO_TCP:
+			entry.ipv4_hnapt.etype = htons(ETH_P_IP);
+
+			/* DS-Lite WAN->LAN */
+			if (entry.ipv4_hnapt.bfib1.pkt_type == IPV4_DSLITE ||
+			    entry.ipv4_hnapt.bfib1.pkt_type == IPV4_MAP_E) {
+				entry.ipv4_dslite.sip = foe->ipv4_dslite.sip;
+				entry.ipv4_dslite.dip = foe->ipv4_dslite.dip;
+				entry.ipv4_dslite.sport =
+					foe->ipv4_dslite.sport;
+				entry.ipv4_dslite.dport =
+					foe->ipv4_dslite.dport;
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+				if (entry.bfib1.pkt_type == IPV4_MAP_E) {
+					pptr = skb_header_pointer(skb,
+								  iph->ihl * 4,
+								  sizeof(_ports),
+								  &_ports);
+					if (unlikely(!pptr))
+						return -1;
+
+					entry.ipv4_mape.new_sip =
+							ntohl(iph->saddr);
+					entry.ipv4_mape.new_dip =
+							ntohl(iph->daddr);
+					entry.ipv4_mape.new_sport =
+							ntohs(pptr->src);
+					entry.ipv4_mape.new_dport =
+							ntohs(pptr->dst);
+				}
+#endif
+
+				entry.ipv4_dslite.tunnel_sipv6_0 =
+					foe->ipv4_dslite.tunnel_sipv6_0;
+				entry.ipv4_dslite.tunnel_sipv6_1 =
+					foe->ipv4_dslite.tunnel_sipv6_1;
+				entry.ipv4_dslite.tunnel_sipv6_2 =
+					foe->ipv4_dslite.tunnel_sipv6_2;
+				entry.ipv4_dslite.tunnel_sipv6_3 =
+					foe->ipv4_dslite.tunnel_sipv6_3;
+
+				entry.ipv4_dslite.tunnel_dipv6_0 =
+					foe->ipv4_dslite.tunnel_dipv6_0;
+				entry.ipv4_dslite.tunnel_dipv6_1 =
+					foe->ipv4_dslite.tunnel_dipv6_1;
+				entry.ipv4_dslite.tunnel_dipv6_2 =
+					foe->ipv4_dslite.tunnel_dipv6_2;
+				entry.ipv4_dslite.tunnel_dipv6_3 =
+					foe->ipv4_dslite.tunnel_dipv6_3;
+
+				entry.ipv4_dslite.bfib1.rmt = 1;
+				entry.ipv4_dslite.iblk2.dscp = iph->tos;
+				entry.ipv4_dslite.vlan1 = hw_path->vlan_id;
+				if (hnat_priv->data->per_flow_accounting)
+					entry.ipv4_dslite.iblk2.mibf = 1;
+
+			} else {
+				entry.ipv4_hnapt.iblk2.dscp = iph->tos;
+				if (hnat_priv->data->per_flow_accounting)
+					entry.ipv4_hnapt.iblk2.mibf = 1;
+
+				entry.ipv4_hnapt.vlan1 = hw_path->vlan_id;
+
+				if (skb_vlan_tagged(skb)) {
+					entry.bfib1.vlan_layer += 1;
+
+					if (entry.ipv4_hnapt.vlan1)
+						entry.ipv4_hnapt.vlan2 =
+							skb->vlan_tci;
+					else
+						entry.ipv4_hnapt.vlan1 =
+							skb->vlan_tci;
+			}
+
+				entry.ipv4_hnapt.sip = foe->ipv4_hnapt.sip;
+				entry.ipv4_hnapt.dip = foe->ipv4_hnapt.dip;
+				entry.ipv4_hnapt.sport = foe->ipv4_hnapt.sport;
+				entry.ipv4_hnapt.dport = foe->ipv4_hnapt.dport;
+
+				entry.ipv4_hnapt.new_sip = ntohl(iph->saddr);
+				entry.ipv4_hnapt.new_dip = ntohl(iph->daddr);
+			}
+
+			entry.ipv4_hnapt.bfib1.udp = udp;
+			if (IS_IPV4_HNAPT(foe)) {
+				pptr = skb_header_pointer(skb, iph->ihl * 4,
+							  sizeof(_ports),
+							  &_ports);
+				if (unlikely(!pptr))
+					return -1;
+
+				entry.ipv4_hnapt.new_sport = ntohs(pptr->src);
+				entry.ipv4_hnapt.new_dport = ntohs(pptr->dst);
+			}
+
+			break;
+
+		default:
+			return -1;
+		}
+		trace_printk(
+			"[%s]skb->head=%p, skb->data=%p,ip_hdr=%p, skb->len=%d, skb->data_len=%d\n",
+			__func__, skb->head, skb->data, iph, skb->len,
+			skb->data_len);
+		break;
+
+	case ETH_P_IPV6:
+		ip6h = ipv6_hdr(skb);
+		switch (ip6h->nexthdr) {
+		case NEXTHDR_UDP:
+			udp = 1;
+			/* fallthrough */
+		case NEXTHDR_TCP: /* IPv6-5T or IPv6-3T */
+			entry.ipv6_5t_route.etype = htons(ETH_P_IPV6);
+
+			entry.ipv6_5t_route.vlan1 = hw_path->vlan_id;
+
+			if (skb_vlan_tagged(skb)) {
+				entry.bfib1.vlan_layer += 1;
+
+				if (entry.ipv6_5t_route.vlan1)
+					entry.ipv6_5t_route.vlan2 =
+						skb->vlan_tci;
+				else
+					entry.ipv6_5t_route.vlan1 =
+						skb->vlan_tci;
+			}
+
+			if (hnat_priv->data->per_flow_accounting)
+				entry.ipv6_5t_route.iblk2.mibf = 1;
+			entry.ipv6_5t_route.bfib1.udp = udp;
+
+			if (IS_IPV6_6RD(foe)) {
+				entry.ipv6_5t_route.bfib1.rmt = 1;
+				entry.ipv6_6rd.tunnel_sipv4 =
+					foe->ipv6_6rd.tunnel_sipv4;
+				entry.ipv6_6rd.tunnel_dipv4 =
+					foe->ipv6_6rd.tunnel_dipv4;
+			}
+
+			entry.ipv6_3t_route.ipv6_sip0 =
+				foe->ipv6_3t_route.ipv6_sip0;
+			entry.ipv6_3t_route.ipv6_sip1 =
+				foe->ipv6_3t_route.ipv6_sip1;
+			entry.ipv6_3t_route.ipv6_sip2 =
+				foe->ipv6_3t_route.ipv6_sip2;
+			entry.ipv6_3t_route.ipv6_sip3 =
+				foe->ipv6_3t_route.ipv6_sip3;
+
+			entry.ipv6_3t_route.ipv6_dip0 =
+				foe->ipv6_3t_route.ipv6_dip0;
+			entry.ipv6_3t_route.ipv6_dip1 =
+				foe->ipv6_3t_route.ipv6_dip1;
+			entry.ipv6_3t_route.ipv6_dip2 =
+				foe->ipv6_3t_route.ipv6_dip2;
+			entry.ipv6_3t_route.ipv6_dip3 =
+				foe->ipv6_3t_route.ipv6_dip3;
+
+			if (IS_IPV6_3T_ROUTE(foe)) {
+				entry.ipv6_3t_route.prot =
+					foe->ipv6_3t_route.prot;
+				entry.ipv6_3t_route.hph =
+					foe->ipv6_3t_route.hph;
+			}
+
+			if (IS_IPV6_5T_ROUTE(foe) || IS_IPV6_6RD(foe)) {
+				entry.ipv6_5t_route.sport =
+					foe->ipv6_5t_route.sport;
+				entry.ipv6_5t_route.dport =
+					foe->ipv6_5t_route.dport;
+			}
+
+			if (ct && (ct->status & IPS_SRC_NAT)) {
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+				entry.bfib1.pkt_type = IPV6_HNAPT;
+
+				if (IS_WAN(dev) || IS_DSA_WAN(dev)) {
+					entry.ipv6_hnapt.eg_ipv6_dir =
+						IPV6_SNAT;
+					entry.ipv6_hnapt.new_ipv6_ip0 =
+						ntohl(ip6h->saddr.s6_addr32[0]);
+					entry.ipv6_hnapt.new_ipv6_ip1 =
+						ntohl(ip6h->saddr.s6_addr32[1]);
+					entry.ipv6_hnapt.new_ipv6_ip2 =
+						ntohl(ip6h->saddr.s6_addr32[2]);
+					entry.ipv6_hnapt.new_ipv6_ip3 =
+						ntohl(ip6h->saddr.s6_addr32[3]);
+				} else {
+					entry.ipv6_hnapt.eg_ipv6_dir =
+						IPV6_DNAT;
+					entry.ipv6_hnapt.new_ipv6_ip0 =
+						ntohl(ip6h->daddr.s6_addr32[0]);
+					entry.ipv6_hnapt.new_ipv6_ip1 =
+						ntohl(ip6h->daddr.s6_addr32[1]);
+					entry.ipv6_hnapt.new_ipv6_ip2 =
+						ntohl(ip6h->daddr.s6_addr32[2]);
+					entry.ipv6_hnapt.new_ipv6_ip3 =
+						ntohl(ip6h->daddr.s6_addr32[3]);
+				}
+
+				pptr = skb_header_pointer(skb, IPV6_HDR_LEN,
+							  sizeof(_ports),
+							  &_ports);
+				if (unlikely(!pptr))
+					return -1;
+
+				entry.ipv6_hnapt.new_sport = ntohs(pptr->src);
+				entry.ipv6_hnapt.new_dport = ntohs(pptr->dst);
+#else
+				return -1;
+#endif
+			}
+
+			entry.ipv6_5t_route.iblk2.dscp =
+				(ip6h->priority << 4 |
+				 (ip6h->flow_lbl[0] >> 4));
+			break;
+
+		case NEXTHDR_IPIP:
+			iph = (struct iphdr *)skb_inner_network_header(skb);
+			/* don't process inner fragment packets */
+			if (ip_is_fragment(iph))
+				return 0;
+
+			if ((!mape_toggle &&
+			     entry.bfib1.pkt_type == IPV4_DSLITE) ||
+			    (mape_toggle &&
+			     entry.bfib1.pkt_type == IPV4_MAP_E)) {
+				/* DS-Lite LAN->WAN */
+				entry.ipv4_dslite.bfib1.udp =
+					foe->ipv4_dslite.bfib1.udp;
+				entry.ipv4_dslite.sip = foe->ipv4_dslite.sip;
+				entry.ipv4_dslite.dip = foe->ipv4_dslite.dip;
+				entry.ipv4_dslite.sport =
+					foe->ipv4_dslite.sport;
+				entry.ipv4_dslite.dport =
+					foe->ipv4_dslite.dport;
+
+				entry.ipv4_dslite.tunnel_sipv6_0 =
+					ntohl(ip6h->saddr.s6_addr32[0]);
+				entry.ipv4_dslite.tunnel_sipv6_1 =
+					ntohl(ip6h->saddr.s6_addr32[1]);
+				entry.ipv4_dslite.tunnel_sipv6_2 =
+					ntohl(ip6h->saddr.s6_addr32[2]);
+				entry.ipv4_dslite.tunnel_sipv6_3 =
+					ntohl(ip6h->saddr.s6_addr32[3]);
+
+				entry.ipv4_dslite.tunnel_dipv6_0 =
+					ntohl(ip6h->daddr.s6_addr32[0]);
+				entry.ipv4_dslite.tunnel_dipv6_1 =
+					ntohl(ip6h->daddr.s6_addr32[1]);
+				entry.ipv4_dslite.tunnel_dipv6_2 =
+					ntohl(ip6h->daddr.s6_addr32[2]);
+				entry.ipv4_dslite.tunnel_dipv6_3 =
+					ntohl(ip6h->daddr.s6_addr32[3]);
+
+				ppe_fill_flow_lbl(&entry, ip6h);
+
+				entry.ipv4_dslite.priority = ip6h->priority;
+				entry.ipv4_dslite.hop_limit = ip6h->hop_limit;
+				entry.ipv4_dslite.vlan1 = hw_path->vlan_id;
+				if (hnat_priv->data->per_flow_accounting)
+					entry.ipv4_dslite.iblk2.mibf = 1;
+				/* Map-E LAN->WAN record inner IPv4 header info. */
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+				if (mape_toggle) {
+					entry.ipv4_dslite.iblk2.dscp = foe->ipv4_dslite.iblk2.dscp;
+					entry.ipv4_mape.new_sip = foe->ipv4_mape.new_sip;
+					entry.ipv4_mape.new_dip = foe->ipv4_mape.new_dip;
+					entry.ipv4_mape.new_sport = foe->ipv4_mape.new_sport;
+					entry.ipv4_mape.new_dport = foe->ipv4_mape.new_dport;
+				}
+#endif
+			} else if (mape_toggle &&
+				   entry.bfib1.pkt_type == IPV4_HNAPT) {
+				/* MapE LAN -> WAN */
+				mape = 1;
+				entry.ipv4_hnapt.iblk2.dscp =
+					foe->ipv4_hnapt.iblk2.dscp;
+				if (hnat_priv->data->per_flow_accounting)
+					entry.ipv4_hnapt.iblk2.mibf = 1;
+
+				if (IS_GMAC1_MODE)
+					entry.ipv4_hnapt.vlan1 = 1;
+				else
+					entry.ipv4_hnapt.vlan1 = hw_path->vlan_id;
+
+				entry.ipv4_hnapt.sip = foe->ipv4_hnapt.sip;
+				entry.ipv4_hnapt.dip = foe->ipv4_hnapt.dip;
+				entry.ipv4_hnapt.sport = foe->ipv4_hnapt.sport;
+				entry.ipv4_hnapt.dport = foe->ipv4_hnapt.dport;
+
+				entry.ipv4_hnapt.new_sip =
+					foe->ipv4_hnapt.new_sip;
+				entry.ipv4_hnapt.new_dip =
+					foe->ipv4_hnapt.new_dip;
+				entry.ipv4_hnapt.etype = htons(ETH_P_IP);
+
+				if (IS_HQOS_MODE) {
+					entry.ipv4_hnapt.iblk2.qid =
+						(hnat_priv->data->version ==
+						 MTK_HNAT_V2 ||
+						 hnat_priv->data->version ==
+						 MTK_HNAT_V3) ?
+						 skb->mark & 0x7f : skb->mark & 0xf;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+					if ((IS_HQOS_UL_MODE && IS_WAN(dev)) ||
+					    (IS_HQOS_DL_MODE &&
+					     IS_LAN_GRP(dev)) ||
+					    (IS_PPPQ_MODE &&
+					     IS_PPPQ_PATH(dev, skb)))
+						entry.ipv4_hnapt.tport_id = 1;
+					else
+						entry.ipv4_hnapt.tport_id = 0;
+#else
+					entry.ipv4_hnapt.iblk2.fqos = 1;
+#endif
+				}
+
+				entry.ipv4_hnapt.bfib1.udp =
+					foe->ipv4_hnapt.bfib1.udp;
+
+				entry.ipv4_hnapt.new_sport =
+					foe->ipv4_hnapt.new_sport;
+				entry.ipv4_hnapt.new_dport =
+					foe->ipv4_hnapt.new_dport;
+				mape_l2w_v6h = *ip6h;
+			}
+			break;
+
+		default:
+			return -1;
+		}
+
+		trace_printk(
+			"[%s]skb->head=%p, skb->data=%p,ipv6_hdr=%p, skb->len=%d, skb->data_len=%d\n",
+			__func__, skb->head, skb->data, ip6h, skb->len,
+			skb->data_len);
+		break;
+
+	default:
+		iph = ip_hdr(skb);
+		switch (entry.bfib1.pkt_type) {
+		case IPV6_6RD: /* 6RD LAN->WAN */
+			entry.ipv6_6rd.ipv6_sip0 = foe->ipv6_6rd.ipv6_sip0;
+			entry.ipv6_6rd.ipv6_sip1 = foe->ipv6_6rd.ipv6_sip1;
+			entry.ipv6_6rd.ipv6_sip2 = foe->ipv6_6rd.ipv6_sip2;
+			entry.ipv6_6rd.ipv6_sip3 = foe->ipv6_6rd.ipv6_sip3;
+
+			entry.ipv6_6rd.ipv6_dip0 = foe->ipv6_6rd.ipv6_dip0;
+			entry.ipv6_6rd.ipv6_dip1 = foe->ipv6_6rd.ipv6_dip1;
+			entry.ipv6_6rd.ipv6_dip2 = foe->ipv6_6rd.ipv6_dip2;
+			entry.ipv6_6rd.ipv6_dip3 = foe->ipv6_6rd.ipv6_dip3;
+
+			entry.ipv6_6rd.sport = foe->ipv6_6rd.sport;
+			entry.ipv6_6rd.dport = foe->ipv6_6rd.dport;
+			entry.ipv6_6rd.tunnel_sipv4 = ntohl(iph->saddr);
+			entry.ipv6_6rd.tunnel_dipv4 = ntohl(iph->daddr);
+			entry.ipv6_6rd.hdr_chksum = ppe_get_chkbase(iph);
+			entry.ipv6_6rd.flag = (ntohs(iph->frag_off) >> 13);
+			entry.ipv6_6rd.ttl = iph->ttl;
+			entry.ipv6_6rd.dscp = iph->tos;
+			entry.ipv6_6rd.per_flow_6rd_id = 1;
+			entry.ipv6_6rd.vlan1 = hw_path->vlan_id;
+			if (hnat_priv->data->per_flow_accounting)
+				entry.ipv6_6rd.iblk2.mibf = 1;
+			break;
+
+		default:
+			return -1;
+		}
+	}
+
+	/* Fill Layer2 Info.*/
+	entry = ppe_fill_L2_info(eth, entry, hw_path);
+
+	/* Fill Info Blk*/
+	entry = ppe_fill_info_blk(eth, entry, hw_path);
+
+	if (IS_LAN(dev)) {
+		if (IS_DSA_LAN(dev))
+			port_id = hnat_dsa_fill_stag(dev, &entry, hw_path,
+						     ntohs(eth->h_proto),
+						     mape);
+
+		if (IS_BOND_MODE)
+			gmac = ((skb_hnat_entry(skb) >> 1) % hnat_priv->gmac_num) ?
+				 NR_GMAC2_PORT : NR_GMAC1_PORT;
+		else
+			gmac = NR_GMAC1_PORT;
+	} else if (IS_LAN2(dev)) {
+		gmac = NR_GMAC3_PORT;
+	} else if (IS_WAN(dev)) {
+		if (IS_DSA_WAN(dev))
+			port_id = hnat_dsa_fill_stag(dev,&entry, hw_path,
+						     ntohs(eth->h_proto),
+						     mape);
+		if (mape_toggle && mape == 1) {
+			gmac = NR_PDMA_PORT;
+			/* Set act_dp = wan_dev */
+			entry.ipv4_hnapt.act_dp &= ~UDF_PINGPONG_IFIDX;
+			entry.ipv4_hnapt.act_dp |= dev->ifindex & UDF_PINGPONG_IFIDX;
+		} else {
+			gmac = (IS_GMAC1_MODE) ? NR_GMAC1_PORT : NR_GMAC2_PORT;
+		}
+	} else if (IS_EXT(dev) && (FROM_GE_PPD(skb) || FROM_GE_LAN_GRP(skb) ||
+		   FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb) || FROM_WED(skb))) {
+		if (!hnat_priv->data->whnat && IS_GMAC1_MODE) {
+			entry.bfib1.vpm = 1;
+			entry.bfib1.vlan_layer = 1;
+
+			if (FROM_GE_LAN(skb))
+				entry.ipv4_hnapt.vlan1 = 1;
+			else if (FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb))
+				entry.ipv4_hnapt.vlan1 = 2;
+		}
+
+		trace_printk("learn of lan or wan(iif=%x) --> %s(ext)\n",
+			     skb_hnat_iface(skb), dev->name);
+		/* To CPU then stolen by pre-routing hant hook of LAN/WAN
+		 * Current setting is PDMA RX.
+		 */
+		gmac = NR_PDMA_PORT;
+		if (IS_IPV4_GRP(foe)) {
+			entry.ipv4_hnapt.act_dp &= ~UDF_PINGPONG_IFIDX;
+			entry.ipv4_hnapt.act_dp |= dev->ifindex & UDF_PINGPONG_IFIDX;
+		} else {
+			entry.ipv6_5t_route.act_dp &= ~UDF_PINGPONG_IFIDX;
+			entry.ipv6_5t_route.act_dp |= dev->ifindex & UDF_PINGPONG_IFIDX;
+		}
+	} else {
+		printk_ratelimited(KERN_WARNING
+					"Unknown case of dp, iif=%x --> %s\n",
+					skb_hnat_iface(skb), dev->name);
+
+		return 0;
+	}
+
+	if (IS_HQOS_MODE || skb->mark >= MAX_PPPQ_PORT_NUM)
+		qid = skb->mark & (MTK_QDMA_TX_MASK);
+	else if (IS_PPPQ_MODE && IS_PPPQ_PATH(dev, skb))
+		qid = port_id & MTK_QDMA_TX_MASK;
+	else
+		qid = 0;
+
+	if (IS_IPV4_GRP(foe)) {
+		entry.ipv4_hnapt.iblk2.dp = gmac;
+		entry.ipv4_hnapt.iblk2.port_mg =
+			(hnat_priv->data->version == MTK_HNAT_V1_1) ? 0x3f : 0;
+
+		if (qos_toggle) {
+			if (hnat_priv->data->version == MTK_HNAT_V2 ||
+			    hnat_priv->data->version == MTK_HNAT_V3) {
+				entry.ipv4_hnapt.iblk2.qid = qid & 0x7f;
+			} else {
+				/* qid[5:0]= port_mg[1:0]+ qid[3:0] */
+				entry.ipv4_hnapt.iblk2.qid = qid & 0xf;
+				if (hnat_priv->data->version != MTK_HNAT_V1_1)
+					entry.ipv4_hnapt.iblk2.port_mg |=
+						((qid >> 4) & 0x3);
+
+				if (((IS_EXT(dev) && (FROM_GE_LAN_GRP(skb) ||
+				      FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb))) ||
+				      ((mape_toggle && mape == 1) && !FROM_EXT(skb))) &&
+				      (!whnat)) {
+					entry.ipv4_hnapt.etype = htons(HQOS_MAGIC_TAG);
+					entry.ipv4_hnapt.vlan1 = skb_hnat_entry(skb);
+					entry.bfib1.vlan_layer = 1;
+				}
+			}
+
+			if (FROM_EXT(skb) || skb_hnat_sport(skb) == NR_QDMA_PORT ||
+			    (IS_PPPQ_MODE && !IS_DSA_LAN(dev) && !IS_DSA_WAN(dev)))
+				entry.ipv4_hnapt.iblk2.fqos = 0;
+			else
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+				if ((IS_HQOS_UL_MODE && IS_WAN(dev)) ||
+				    (IS_HQOS_DL_MODE && IS_LAN_GRP(dev)) ||
+				    (IS_PPPQ_MODE &&
+				     IS_PPPQ_PATH(dev, skb)))
+					entry.ipv4_hnapt.tport_id = 1;
+				else
+					entry.ipv4_hnapt.tport_id = 0;
+#else
+				entry.ipv4_hnapt.iblk2.fqos =
+					(!IS_PPPQ_MODE ||
+					(IS_PPPQ_MODE &&
+					 IS_PPPQ_PATH(dev, skb)));
+#endif
+		} else {
+			entry.ipv4_hnapt.iblk2.fqos = 0;
+		}
+	} else {
+		entry.ipv6_5t_route.iblk2.dp = gmac;
+		entry.ipv6_5t_route.iblk2.port_mg =
+			(hnat_priv->data->version == MTK_HNAT_V1_1) ? 0x3f : 0;
+
+		if (qos_toggle) {
+			if (hnat_priv->data->version == MTK_HNAT_V2 ||
+			    hnat_priv->data->version == MTK_HNAT_V3) {
+				entry.ipv6_5t_route.iblk2.qid = qid & 0x7f;
+			} else {
+				/* qid[5:0]= port_mg[1:0]+ qid[3:0] */
+				entry.ipv6_5t_route.iblk2.qid = qid & 0xf;
+				if (hnat_priv->data->version != MTK_HNAT_V1_1)
+					entry.ipv6_5t_route.iblk2.port_mg |=
+								((qid >> 4) & 0x3);
+
+				if (IS_EXT(dev) && (FROM_GE_LAN_GRP(skb) ||
+				    FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb)) &&
+				    (!whnat)) {
+					entry.ipv6_5t_route.etype = htons(HQOS_MAGIC_TAG);
+					entry.ipv6_5t_route.vlan1 = skb_hnat_entry(skb);
+					entry.bfib1.vlan_layer = 1;
+				}
+			}
+
+			if (FROM_EXT(skb) ||
+			    (IS_PPPQ_MODE && !IS_DSA_LAN(dev) && !IS_DSA_WAN(dev)))
+				entry.ipv6_5t_route.iblk2.fqos = 0;
+			else
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+				if ((IS_HQOS_UL_MODE && IS_WAN(dev)) ||
+					(IS_HQOS_DL_MODE && IS_LAN_GRP(dev)) ||
+					(IS_PPPQ_MODE &&
+					 IS_PPPQ_PATH(dev, skb)))
+					entry.ipv6_5t_route.tport_id = 1;
+				else
+					entry.ipv6_5t_route.tport_id = 0;
+#else
+				entry.ipv6_5t_route.iblk2.fqos =
+					(!IS_PPPQ_MODE ||
+					 (IS_PPPQ_MODE &&
+					  IS_PPPQ_PATH(dev, skb)));
+#endif
+		} else {
+			entry.ipv6_5t_route.iblk2.fqos = 0;
+		}
+	}
+
+	/* The INFO2.port_mg and 2nd VLAN ID fields of PPE entry are redefined
+	 * by Wi-Fi whnat engine. These data and INFO2.dp will be updated and
+	 * the entry is set to BIND state in mtk_sw_nat_hook_tx().
+	 */
+	if (!whnat) {
+		entry.bfib1.ttl = 1;
+		entry.bfib1.state = BIND;
+	} else {
+		if (IS_IPV4_GRP(foe))
+			entry.ipv4_hnapt.act_dp |= UDF_HNAT_PRE_FILLED;
+		else
+			entry.ipv6_5t_route.act_dp |= UDF_HNAT_PRE_FILLED;
+	}
+
+	wmb();
+	memcpy(foe, &entry, sizeof(entry));
+	/*reset statistic for this entry*/
+	if (hnat_priv->data->per_flow_accounting &&
+	    skb_hnat_entry(skb) < hnat_priv->foe_etry_num &&
+	    skb_hnat_ppe(skb) < CFG_PPE_NUM)
+		memset(&hnat_priv->acct[skb_hnat_ppe(skb)][skb_hnat_entry(skb)],
+		       0, sizeof(struct mib_entry));
+
+	return 0;
+}
+
+int mtk_sw_nat_hook_tx(struct sk_buff *skb, int gmac_no)
+{
+	struct foe_entry *entry;
+	struct ethhdr *eth;
+	struct hnat_bind_info_blk bfib1_tx;
+
+	if (skb_hnat_alg(skb) ||
+	    !is_magic_tag_valid(skb) || !IS_SPACE_AVAILABLE_HEAD(skb))
+		return NF_ACCEPT;
+
+	trace_printk(
+		"[%s]entry=%x reason=%x gmac_no=%x wdmaid=%x rxid=%x wcid=%x bssid=%x\n",
+		__func__, skb_hnat_entry(skb), skb_hnat_reason(skb), gmac_no,
+		skb_hnat_wdma_id(skb), skb_hnat_bss_id(skb),
+		skb_hnat_wc_id(skb), skb_hnat_rx_id(skb));
+
+	if ((gmac_no != NR_WDMA0_PORT) && (gmac_no != NR_WDMA1_PORT) &&
+	    (gmac_no != NR_WDMA2_PORT) && (gmac_no != NR_WHNAT_WDMA_PORT))
+		return NF_ACCEPT;
+
+	if (unlikely(!skb_mac_header_was_set(skb)))
+		return NF_ACCEPT;
+
+	if (!skb_hnat_is_hashed(skb))
+		return NF_ACCEPT;
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return NF_ACCEPT;
+
+	entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+	if (entry_hnat_is_bound(entry))
+		return NF_ACCEPT;
+
+	if (skb_hnat_reason(skb) != HIT_UNBIND_RATE_REACH)
+		return NF_ACCEPT;
+
+	if (!is_hnat_pre_filled(entry))
+		return NF_ACCEPT;
+
+	eth = eth_hdr(skb);
+	memcpy(&bfib1_tx, &entry->bfib1, sizeof(entry->bfib1));
+
+	/*not bind multicast if PPE mcast not enable*/
+	if (!hnat_priv->data->mcast) {
+		if (is_multicast_ether_addr(eth->h_dest))
+			return NF_ACCEPT;
+
+		if (IS_IPV4_GRP(entry))
+			entry->ipv4_hnapt.iblk2.mcast = 0;
+		else
+			entry->ipv6_5t_route.iblk2.mcast = 0;
+	}
+
+	/* Some mt_wifi virtual interfaces, such as apcli,
+	 * will change the smac for specail purpose.
+	 */
+	switch ((int)bfib1_tx.pkt_type) {
+	case IPV4_HNAPT:
+	case IPV4_HNAT:
+		entry->ipv4_hnapt.smac_hi = swab32(*((u32 *)eth->h_source));
+		entry->ipv4_hnapt.smac_lo = swab16(*((u16 *)&eth->h_source[4]));
+		break;
+	case IPV4_DSLITE:
+	case IPV4_MAP_E:
+	case IPV6_6RD:
+	case IPV6_5T_ROUTE:
+	case IPV6_3T_ROUTE:
+	case IPV6_HNAPT:
+	case IPV6_HNAT:
+		entry->ipv6_5t_route.smac_hi = swab32(*((u32 *)eth->h_source));
+		entry->ipv6_5t_route.smac_lo = swab16(*((u16 *)&eth->h_source[4]));
+		break;
+	}
+
+	if (skb_vlan_tagged(skb)) {
+		bfib1_tx.vlan_layer = 1;
+		bfib1_tx.vpm = 1;
+		if (IS_IPV4_GRP(entry)) {
+			entry->ipv4_hnapt.etype = htons(ETH_P_8021Q);
+			entry->ipv4_hnapt.vlan1 = skb->vlan_tci;
+		} else if (IS_IPV6_GRP(entry)) {
+			entry->ipv6_5t_route.etype = htons(ETH_P_8021Q);
+			entry->ipv6_5t_route.vlan1 = skb->vlan_tci;
+		}
+	} else {
+		bfib1_tx.vpm = 0;
+		bfib1_tx.vlan_layer = 0;
+	}
+
+	/* MT7622 wifi hw_nat not support QoS */
+	if (IS_IPV4_GRP(entry)) {
+		entry->ipv4_hnapt.iblk2.fqos = 0;
+		if ((hnat_priv->data->version == MTK_HNAT_V1_2 &&
+		     gmac_no == NR_WHNAT_WDMA_PORT) ||
+		    ((hnat_priv->data->version == MTK_HNAT_V2 ||
+		      hnat_priv->data->version == MTK_HNAT_V3) &&
+		     (gmac_no == NR_WDMA0_PORT || gmac_no == NR_WDMA1_PORT ||
+		      gmac_no == NR_WDMA2_PORT))) {
+			entry->ipv4_hnapt.winfo.bssid = skb_hnat_bss_id(skb);
+			entry->ipv4_hnapt.winfo.wcid = skb_hnat_wc_id(skb);
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+			entry->ipv4_hnapt.tport_id = IS_HQOS_DL_MODE ? 1 : 0;
+			entry->ipv4_hnapt.iblk2.rxid = skb_hnat_rx_id(skb);
+			entry->ipv4_hnapt.iblk2.winfoi = 1;
+			entry->ipv4_hnapt.winfo_pao.usr_info =
+				skb_hnat_usr_info(skb);
+			entry->ipv4_hnapt.winfo_pao.tid = skb_hnat_tid(skb);
+			entry->ipv4_hnapt.winfo_pao.is_fixedrate =
+				skb_hnat_is_fixedrate(skb);
+			entry->ipv4_hnapt.winfo_pao.is_prior =
+				skb_hnat_is_prior(skb);
+			entry->ipv4_hnapt.winfo_pao.is_sp = skb_hnat_is_sp(skb);
+			entry->ipv4_hnapt.winfo_pao.hf = skb_hnat_hf(skb);
+			entry->ipv4_hnapt.winfo_pao.amsdu = skb_hnat_amsdu(skb);
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+			entry->ipv4_hnapt.iblk2.rxid = skb_hnat_rx_id(skb);
+			entry->ipv4_hnapt.iblk2.winfoi = 1;
+#else
+			entry->ipv4_hnapt.winfo.rxid = skb_hnat_rx_id(skb);
+			entry->ipv4_hnapt.iblk2w.winfoi = 1;
+			entry->ipv4_hnapt.iblk2w.wdmaid = skb_hnat_wdma_id(skb);
+#endif
+		} else {
+			if (IS_GMAC1_MODE && !hnat_dsa_is_enable(hnat_priv)) {
+				bfib1_tx.vpm = 1;
+				bfib1_tx.vlan_layer = 1;
+
+				if (FROM_GE_LAN_GRP(skb))
+					entry->ipv4_hnapt.vlan1 = 1;
+				else if (FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb))
+					entry->ipv4_hnapt.vlan1 = 2;
+			}
+
+			if (IS_HQOS_MODE &&
+			    (FROM_GE_LAN_GRP(skb) || FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb))) {
+				bfib1_tx.vpm = 0;
+				bfib1_tx.vlan_layer = 1;
+				entry->ipv4_hnapt.etype = htons(HQOS_MAGIC_TAG);
+				entry->ipv4_hnapt.vlan1 = skb_hnat_entry(skb);
+				entry->ipv4_hnapt.iblk2.fqos = 1;
+			}
+		}
+		entry->ipv4_hnapt.iblk2.dp = gmac_no;
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+	} else if (IS_IPV6_HNAPT(entry) || IS_IPV6_HNAT(entry)) {
+		entry->ipv6_hnapt.iblk2.dp = gmac_no;
+		entry->ipv6_hnapt.iblk2.rxid = skb_hnat_rx_id(skb);
+		entry->ipv6_hnapt.iblk2.winfoi = 1;
+
+		entry->ipv6_hnapt.winfo.bssid = skb_hnat_bss_id(skb);
+		entry->ipv6_hnapt.winfo.wcid = skb_hnat_wc_id(skb);
+		entry->ipv6_hnapt.winfo_pao.usr_info = skb_hnat_usr_info(skb);
+		entry->ipv6_hnapt.winfo_pao.tid = skb_hnat_tid(skb);
+		entry->ipv6_hnapt.winfo_pao.is_fixedrate =
+			skb_hnat_is_fixedrate(skb);
+		entry->ipv6_hnapt.winfo_pao.is_prior = skb_hnat_is_prior(skb);
+		entry->ipv6_hnapt.winfo_pao.is_sp = skb_hnat_is_sp(skb);
+		entry->ipv6_hnapt.winfo_pao.hf = skb_hnat_hf(skb);
+		entry->ipv6_hnapt.winfo_pao.amsdu = skb_hnat_amsdu(skb);
+		entry->ipv6_hnapt.tport_id = IS_HQOS_DL_MODE ? 1 : 0;
+#endif
+	} else {
+		entry->ipv6_5t_route.iblk2.fqos = 0;
+		if ((hnat_priv->data->version == MTK_HNAT_V1_2 &&
+		     gmac_no == NR_WHNAT_WDMA_PORT) ||
+		    ((hnat_priv->data->version == MTK_HNAT_V2 ||
+		      hnat_priv->data->version == MTK_HNAT_V3) &&
+		     (gmac_no == NR_WDMA0_PORT || gmac_no == NR_WDMA1_PORT ||
+		      gmac_no == NR_WDMA2_PORT))) {
+			entry->ipv6_5t_route.winfo.bssid = skb_hnat_bss_id(skb);
+			entry->ipv6_5t_route.winfo.wcid = skb_hnat_wc_id(skb);
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+			entry->ipv6_5t_route.tport_id = IS_HQOS_DL_MODE ? 1 : 0;
+			entry->ipv6_5t_route.iblk2.rxid = skb_hnat_rx_id(skb);
+			entry->ipv6_5t_route.iblk2.winfoi = 1;
+			entry->ipv6_5t_route.winfo_pao.usr_info =
+				skb_hnat_usr_info(skb);
+			entry->ipv6_5t_route.winfo_pao.tid =
+				skb_hnat_tid(skb);
+			entry->ipv6_5t_route.winfo_pao.is_fixedrate =
+				skb_hnat_is_fixedrate(skb);
+			entry->ipv6_5t_route.winfo_pao.is_prior =
+				skb_hnat_is_prior(skb);
+			entry->ipv6_5t_route.winfo_pao.is_sp =
+				skb_hnat_is_sp(skb);
+			entry->ipv6_5t_route.winfo_pao.hf =
+				skb_hnat_hf(skb);
+			entry->ipv6_5t_route.winfo_pao.amsdu =
+				skb_hnat_amsdu(skb);
+#elif defined(CONFIG_MEDIATEK_NETSYS_V2)
+			entry->ipv6_5t_route.iblk2.rxid = skb_hnat_rx_id(skb);
+			entry->ipv6_5t_route.iblk2.winfoi = 1;
+#else
+			entry->ipv6_5t_route.winfo.rxid = skb_hnat_rx_id(skb);
+			entry->ipv6_5t_route.iblk2w.winfoi = 1;
+			entry->ipv6_5t_route.iblk2w.wdmaid = skb_hnat_wdma_id(skb);
+#endif
+		} else {
+			if (IS_GMAC1_MODE && !hnat_dsa_is_enable(hnat_priv)) {
+				bfib1_tx.vpm = 1;
+				bfib1_tx.vlan_layer = 1;
+
+				if (FROM_GE_LAN_GRP(skb))
+					entry->ipv6_5t_route.vlan1 = 1;
+				else if (FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb))
+					entry->ipv6_5t_route.vlan1 = 2;
+			}
+
+			if (IS_HQOS_MODE &&
+			    (FROM_GE_LAN_GRP(skb) || FROM_GE_WAN(skb) || FROM_GE_VIRTUAL(skb))) {
+				bfib1_tx.vpm = 0;
+				bfib1_tx.vlan_layer = 1;
+				entry->ipv6_5t_route.etype = htons(HQOS_MAGIC_TAG);
+				entry->ipv6_5t_route.vlan1 = skb_hnat_entry(skb);
+				entry->ipv6_5t_route.iblk2.fqos = 1;
+			}
+		}
+		entry->ipv6_5t_route.iblk2.dp = gmac_no;
+	}
+
+	bfib1_tx.ttl = 1;
+	bfib1_tx.state = BIND;
+	wmb();
+	memcpy(&entry->bfib1, &bfib1_tx, sizeof(bfib1_tx));
+
+	if (IS_IPV4_GRP(entry))
+		entry->ipv4_hnapt.act_dp &= ~UDF_HNAT_PRE_FILLED;
+	else
+		entry->ipv6_5t_route.act_dp &= ~UDF_HNAT_PRE_FILLED;
+
+	return NF_ACCEPT;
+}
+
+int mtk_sw_nat_hook_rx(struct sk_buff *skb)
+{
+	if (!IS_SPACE_AVAILABLE_HEAD(skb) || !FROM_WED(skb)) {
+		skb_hnat_magic_tag(skb) = 0;
+		return NF_ACCEPT;
+	}
+
+	skb_hnat_alg(skb) = 0;
+	skb_hnat_magic_tag(skb) = HNAT_MAGIC_TAG;
+
+	if (skb_hnat_iface(skb) == FOE_MAGIC_WED0)
+		skb_hnat_sport(skb) = NR_WDMA0_PORT;
+	else if (skb_hnat_iface(skb) == FOE_MAGIC_WED1)
+		skb_hnat_sport(skb) = NR_WDMA1_PORT;
+	else if (skb_hnat_iface(skb) == FOE_MAGIC_WED2)
+		skb_hnat_sport(skb) = NR_WDMA2_PORT;
+
+	return NF_ACCEPT;
+}
+
+void mtk_ppe_dev_register_hook(struct net_device *dev)
+{
+	int i, number = 0;
+	struct extdev_entry *ext_entry;
+
+	for (i = 1; i < MAX_IF_NUM; i++) {
+		if (hnat_priv->wifi_hook_if[i] == dev) {
+			pr_info("%s : %s has been registered in wifi_hook_if table[%d]\n",
+				__func__, dev->name, i);
+			return;
+		}
+	}
+
+	for (i = 1; i < MAX_IF_NUM; i++) {
+		if (!hnat_priv->wifi_hook_if[i]) {
+			if (find_extif_from_devname(dev->name)) {
+				extif_set_dev(dev);
+				goto add_wifi_hook_if;
+			}
+
+			number = get_ext_device_number();
+			if (number >= MAX_EXT_DEVS) {
+				pr_info("%s : extdev array is full. %s is not registered\n",
+					__func__, dev->name);
+				return;
+			}
+
+			ext_entry = kzalloc(sizeof(*ext_entry), GFP_KERNEL);
+			if (!ext_entry)
+				return;
+
+			strncpy(ext_entry->name, dev->name, IFNAMSIZ - 1);
+			dev_hold(dev);
+			ext_entry->dev = dev;
+			ext_if_add(ext_entry);
+
+add_wifi_hook_if:
+			dev_hold(dev);
+			hnat_priv->wifi_hook_if[i] = dev;
+
+			break;
+		}
+	}
+	pr_info("%s : ineterface %s register (%d)\n", __func__, dev->name, i);
+}
+
+void mtk_ppe_dev_unregister_hook(struct net_device *dev)
+{
+	int i;
+
+	for (i = 1; i < MAX_IF_NUM; i++) {
+		if (hnat_priv->wifi_hook_if[i] == dev) {
+			hnat_priv->wifi_hook_if[i] = NULL;
+			dev_put(dev);
+
+			break;
+		}
+	}
+
+	extif_put_dev(dev);
+	pr_info("%s : ineterface %s set null (%d)\n", __func__, dev->name, i);
+}
+
+static unsigned int mtk_hnat_accel_type(struct sk_buff *skb)
+{
+	struct dst_entry *dst;
+	struct nf_conn *ct;
+	enum ip_conntrack_info ctinfo;
+	const struct nf_conn_help *help;
+
+	/* Do not accelerate 1st round of xfrm flow, and 2nd round of xfrm flow
+	 * is from local_out which is also filtered in sanity check.
+	 */
+	dst = skb_dst(skb);
+	if (dst && dst_xfrm(dst))
+		return 0;
+
+	ct = nf_ct_get(skb, &ctinfo);
+	if (!ct)
+		return 1;
+
+	/* rcu_read_lock()ed by nf_hook_slow */
+	help = nfct_help(ct);
+	if (help && rcu_dereference(help->helper))
+		return 0;
+
+	return 1;
+}
+
+static void mtk_hnat_dscp_update(struct sk_buff *skb, struct foe_entry *entry)
+{
+	struct iphdr *iph;
+	struct ethhdr *eth;
+	struct ipv6hdr *ip6h;
+	bool flag = false;
+
+	eth = eth_hdr(skb);
+	switch (ntohs(eth->h_proto)) {
+	case ETH_P_IP:
+		iph = ip_hdr(skb);
+		if (IS_IPV4_GRP(entry) && entry->ipv4_hnapt.iblk2.dscp != iph->tos)
+			flag = true;
+		break;
+	case ETH_P_IPV6:
+		ip6h = ipv6_hdr(skb);
+		if ((IS_IPV6_3T_ROUTE(entry) || IS_IPV6_5T_ROUTE(entry)) &&
+			(entry->ipv6_5t_route.iblk2.dscp !=
+			(ip6h->priority << 4 | (ip6h->flow_lbl[0] >> 4))))
+			flag = true;
+		break;
+	default:
+		return;
+	}
+
+	if (flag) {
+		if (debug_level >= 2)
+			pr_info("Delete entry idx=%d.\n", skb_hnat_entry(skb));
+		memset(entry, 0, sizeof(struct foe_entry));
+		hnat_cache_ebl(1);
+	}
+}
+
+static void mtk_hnat_nf_update(struct sk_buff *skb)
+{
+	struct nf_conn *ct;
+	struct nf_conn_acct *acct;
+	struct nf_conn_counter *counter;
+	enum ip_conntrack_info ctinfo;
+	struct hnat_accounting diff;
+
+	ct = nf_ct_get(skb, &ctinfo);
+	if (ct) {
+		if (!hnat_get_count(hnat_priv, skb_hnat_ppe(skb), skb_hnat_entry(skb), &diff))
+			return;
+
+		acct = nf_conn_acct_find(ct);
+		if (acct) {
+			counter = acct->counter;
+			atomic64_add(diff.packets, &counter[CTINFO2DIR(ctinfo)].packets);
+			atomic64_add(diff.bytes, &counter[CTINFO2DIR(ctinfo)].bytes);
+		}
+	}
+}
+
+int mtk_464xlat_fill_mac(struct foe_entry *entry, struct sk_buff *skb,
+			 const struct net_device *out, bool l2w)
+{
+	const struct in6_addr *ipv6_nexthop;
+	struct dst_entry *dst = skb_dst(skb);
+	struct neighbour *neigh = NULL;
+	struct rtable *rt = (struct rtable *)dst;
+	u32 nexthop;
+
+	rcu_read_lock_bh();
+	if (l2w) {
+		ipv6_nexthop = rt6_nexthop((struct rt6_info *)dst,
+					   &ipv6_hdr(skb)->daddr);
+		neigh = __ipv6_neigh_lookup_noref(dst->dev, ipv6_nexthop);
+		if (unlikely(!neigh)) {
+			dev_notice(hnat_priv->dev, "%s:No neigh (daddr=%pI6)\n",
+				   __func__, &ipv6_hdr(skb)->daddr);
+			rcu_read_unlock_bh();
+			return -1;
+		}
+	} else {
+		nexthop = (__force u32)rt_nexthop(rt, ip_hdr(skb)->daddr);
+		neigh = __ipv4_neigh_lookup_noref(dst->dev, nexthop);
+		if (unlikely(!neigh)) {
+			dev_notice(hnat_priv->dev, "%s:No neigh (daddr=%pI4)\n",
+				   __func__, &ip_hdr(skb)->daddr);
+			rcu_read_unlock_bh();
+			return -1;
+		}
+	}
+	rcu_read_unlock_bh();
+
+	entry->ipv4_dslite.dmac_hi = swab32(*((u32 *)neigh->ha));
+	entry->ipv4_dslite.dmac_lo = swab16(*((u16 *)&neigh->ha[4]));
+	entry->ipv4_dslite.smac_hi = swab32(*((u32 *)out->dev_addr));
+	entry->ipv4_dslite.smac_lo = swab16(*((u16 *)&out->dev_addr[4]));
+
+	return 0;
+}
+
+int mtk_464xlat_get_hash(struct sk_buff *skb, u32 *hash, bool l2w)
+{
+	struct in6_addr addr_v6, prefix;
+	struct ipv6hdr *ip6h;
+	struct iphdr *iph;
+	struct tcpudphdr *pptr, _ports;
+	struct foe_entry tmp;
+	u32 addr, protoff;
+
+	if (l2w) {
+		ip6h = ipv6_hdr(skb);
+		if (mtk_ppe_get_xlat_v4_by_v6(&ip6h->daddr, &addr))
+			return -1;
+		protoff = IPV6_HDR_LEN;
+
+		tmp.bfib1.pkt_type = IPV4_HNAPT;
+		tmp.ipv4_hnapt.sip = ntohl(ip6h->saddr.s6_addr32[3]);
+		tmp.ipv4_hnapt.dip = ntohl(addr);
+	} else {
+		iph = ip_hdr(skb);
+		if (mtk_ppe_get_xlat_v6_by_v4(&iph->saddr, &addr_v6, &prefix))
+			return -1;
+
+		protoff = iph->ihl * 4;
+
+		tmp.bfib1.pkt_type = IPV6_5T_ROUTE;
+		tmp.ipv6_5t_route.ipv6_sip0 = ntohl(addr_v6.s6_addr32[0]);
+		tmp.ipv6_5t_route.ipv6_sip1 = ntohl(addr_v6.s6_addr32[1]);
+		tmp.ipv6_5t_route.ipv6_sip2 = ntohl(addr_v6.s6_addr32[2]);
+		tmp.ipv6_5t_route.ipv6_sip3 = ntohl(addr_v6.s6_addr32[3]);
+		tmp.ipv6_5t_route.ipv6_dip0 = ntohl(prefix.s6_addr32[0]);
+		tmp.ipv6_5t_route.ipv6_dip1 = ntohl(prefix.s6_addr32[1]);
+		tmp.ipv6_5t_route.ipv6_dip2 = ntohl(prefix.s6_addr32[2]);
+		tmp.ipv6_5t_route.ipv6_dip3 = ntohl(iph->daddr);
+	}
+
+	pptr = skb_header_pointer(skb, protoff,
+				  sizeof(_ports), &_ports);
+	if (unlikely(!pptr))
+		return -1;
+
+	if (l2w) {
+		tmp.ipv4_hnapt.sport = ntohs(pptr->src);
+		tmp.ipv4_hnapt.dport = ntohs(pptr->dst);
+	} else {
+		tmp.ipv6_5t_route.sport = ntohs(pptr->src);
+		tmp.ipv6_5t_route.dport = ntohs(pptr->dst);
+	}
+
+	*hash = hnat_get_ppe_hash(&tmp);
+
+	return 0;
+}
+
+void mtk_464xlat_fill_info1(struct foe_entry *entry,
+			    struct sk_buff *skb, bool l2w)
+{
+	entry->bfib1.cah = 1;
+	entry->bfib1.ttl = 1;
+	entry->bfib1.state = BIND;
+	entry->bfib1.time_stamp = readl(hnat_priv->fe_base + 0x0010) & (0xFF);
+	if (l2w) {
+		entry->bfib1.pkt_type = IPV4_DSLITE;
+		entry->bfib1.udp = ipv6_hdr(skb)->nexthdr ==
+				   IPPROTO_UDP ? 1 : 0;
+	} else {
+		entry->bfib1.pkt_type = IPV6_6RD;
+		entry->bfib1.udp = ip_hdr(skb)->protocol ==
+				   IPPROTO_UDP ? 1 : 0;
+	}
+}
+
+void mtk_464xlat_fill_info2(struct foe_entry *entry, bool l2w)
+{
+	entry->ipv4_dslite.iblk2.mibf = 1;
+	entry->ipv4_dslite.iblk2.port_ag = 0xF;
+
+	if (l2w)
+		entry->ipv4_dslite.iblk2.dp = NR_GMAC2_PORT;
+	else
+		entry->ipv6_6rd.iblk2.dp = NR_GMAC1_PORT;
+}
+
+void mtk_464xlat_fill_ipv4(struct foe_entry *entry, struct sk_buff *skb,
+			   struct foe_entry *foe, bool l2w)
+{
+	struct iphdr *iph;
+
+	if (l2w) {
+		entry->ipv4_dslite.sip = foe->ipv4_dslite.sip;
+		entry->ipv4_dslite.dip = foe->ipv4_dslite.dip;
+		entry->ipv4_dslite.sport = foe->ipv4_dslite.sport;
+		entry->ipv4_dslite.dport = foe->ipv4_dslite.dport;
+	} else {
+		iph = ip_hdr(skb);
+		entry->ipv6_6rd.tunnel_sipv4 = ntohl(iph->saddr);
+		entry->ipv6_6rd.tunnel_dipv4 = ntohl(iph->daddr);
+		entry->ipv6_6rd.sport = foe->ipv6_6rd.sport;
+		entry->ipv6_6rd.dport = foe->ipv6_6rd.dport;
+		entry->ipv6_6rd.hdr_chksum = ppe_get_chkbase(iph);
+		entry->ipv6_6rd.ttl = iph->ttl;
+		entry->ipv6_6rd.dscp = iph->tos;
+		entry->ipv6_6rd.flag = (ntohs(iph->frag_off) >> 13);
+	}
+}
+
+int mtk_464xlat_fill_ipv6(struct foe_entry *entry, struct sk_buff *skb,
+			  struct foe_entry *foe, bool l2w)
+{
+	struct ipv6hdr *ip6h;
+	struct in6_addr addr_v6, prefix;
+	u32 addr;
+
+	if (l2w) {
+		ip6h = ipv6_hdr(skb);
+
+		if (mtk_ppe_get_xlat_v4_by_v6(&ip6h->daddr, &addr))
+			return -1;
+
+		if (mtk_ppe_get_xlat_v6_by_v4(&addr, &addr_v6, &prefix))
+			return -1;
+
+		entry->ipv4_dslite.tunnel_sipv6_0 =
+			ntohl(prefix.s6_addr32[0]);
+		entry->ipv4_dslite.tunnel_sipv6_1 =
+			ntohl(ip6h->saddr.s6_addr32[1]);
+		entry->ipv4_dslite.tunnel_sipv6_2 =
+			ntohl(ip6h->saddr.s6_addr32[2]);
+		entry->ipv4_dslite.tunnel_sipv6_3 =
+			ntohl(ip6h->saddr.s6_addr32[3]);
+		entry->ipv4_dslite.tunnel_dipv6_0 =
+			ntohl(ip6h->daddr.s6_addr32[0]);
+		entry->ipv4_dslite.tunnel_dipv6_1 =
+			ntohl(ip6h->daddr.s6_addr32[1]);
+		entry->ipv4_dslite.tunnel_dipv6_2 =
+			ntohl(ip6h->daddr.s6_addr32[2]);
+		entry->ipv4_dslite.tunnel_dipv6_3 =
+			ntohl(ip6h->daddr.s6_addr32[3]);
+
+		ppe_fill_flow_lbl(entry, ip6h);
+		entry->ipv4_dslite.priority = ip6h->priority;
+		entry->ipv4_dslite.hop_limit = ip6h->hop_limit;
+
+	} else {
+		entry->ipv6_6rd.ipv6_sip0 = foe->ipv6_6rd.ipv6_sip0;
+		entry->ipv6_6rd.ipv6_sip1 = foe->ipv6_6rd.ipv6_sip1;
+		entry->ipv6_6rd.ipv6_sip2 = foe->ipv6_6rd.ipv6_sip2;
+		entry->ipv6_6rd.ipv6_sip3 = foe->ipv6_6rd.ipv6_sip3;
+		entry->ipv6_6rd.ipv6_dip0 = foe->ipv6_6rd.ipv6_dip0;
+		entry->ipv6_6rd.ipv6_dip1 = foe->ipv6_6rd.ipv6_dip1;
+		entry->ipv6_6rd.ipv6_dip2 = foe->ipv6_6rd.ipv6_dip2;
+		entry->ipv6_6rd.ipv6_dip3 = foe->ipv6_6rd.ipv6_dip3;
+	}
+
+	return 0;
+}
+
+int mtk_464xlat_fill_l2(struct foe_entry *entry, struct sk_buff *skb,
+			const struct net_device *dev, bool l2w)
+{
+	const unsigned int *port_reg;
+	int port_index;
+	u16 sp_tag;
+
+	if (l2w)
+		entry->ipv4_dslite.etype = ETH_P_IP;
+	else {
+		if (IS_DSA_LAN(dev)) {
+			port_reg = of_get_property(dev->dev.of_node,
+						   "reg", NULL);
+			if (unlikely(!port_reg))
+				return -1;
+
+			port_index = be32_to_cpup(port_reg);
+			sp_tag = BIT(port_index);
+
+			entry->bfib1.vlan_layer = 1;
+			entry->bfib1.vpm = 0;
+			entry->ipv6_6rd.etype = sp_tag;
+		} else
+			entry->ipv6_6rd.etype = ETH_P_IPV6;
+	}
+
+	if (mtk_464xlat_fill_mac(entry, skb, dev, l2w))
+		return -1;
+
+	return 0;
+}
+
+
+int mtk_464xlat_fill_l3(struct foe_entry *entry, struct sk_buff *skb,
+			struct foe_entry *foe, bool l2w)
+{
+	mtk_464xlat_fill_ipv4(entry, skb, foe, l2w);
+
+	if (mtk_464xlat_fill_ipv6(entry, skb, foe, l2w))
+		return -1;
+
+	return 0;
+}
+
+int mtk_464xlat_post_process(struct sk_buff *skb, const struct net_device *out)
+{
+	struct foe_entry *foe, entry = {};
+	u32 hash;
+	bool l2w;
+
+	if (skb->protocol == htons(ETH_P_IPV6))
+		l2w = true;
+	else if (skb->protocol == htons(ETH_P_IP))
+		l2w = false;
+	else
+		return -1;
+
+	if (mtk_464xlat_get_hash(skb, &hash, l2w))
+		return -1;
+
+	if (hash >= hnat_priv->foe_etry_num)
+		return -1;
+
+	if (headroom[hash].crsn != HIT_UNBIND_RATE_REACH)
+		return -1;
+
+	foe = &hnat_priv->foe_table_cpu[headroom_ppe(headroom[hash])][hash];
+
+	mtk_464xlat_fill_info1(&entry, skb, l2w);
+
+	if (mtk_464xlat_fill_l3(&entry, skb, foe, l2w))
+		return -1;
+
+	mtk_464xlat_fill_info2(&entry, l2w);
+
+	if (mtk_464xlat_fill_l2(&entry, skb, out, l2w))
+		return -1;
+
+	/* We must ensure all info has been updated before set to hw */
+	wmb();
+	memcpy(foe, &entry, sizeof(struct foe_entry));
+
+	return 0;
+}
+
+static unsigned int mtk_hnat_nf_post_routing(
+	struct sk_buff *skb, const struct net_device *out,
+	unsigned int (*fn)(struct sk_buff *, const struct net_device *,
+			   struct flow_offload_hw_path *),
+	const char *func)
+{
+	struct foe_entry *entry;
+	struct flow_offload_hw_path hw_path = { .dev = (struct net_device*)out,
+						.virt_dev = (struct net_device*)out };
+	const struct net_device *arp_dev = out;
+
+	if (xlat_toggle && !mtk_464xlat_post_process(skb, out))
+		return 0;
+
+	if (skb_hnat_alg(skb) || unlikely(!is_magic_tag_valid(skb) ||
+					  !IS_SPACE_AVAILABLE_HEAD(skb)))
+		return 0;
+
+	if (unlikely(!skb_mac_header_was_set(skb)))
+		return 0;
+
+	if (unlikely(!skb_hnat_is_hashed(skb)))
+		return 0;
+
+	if (out->netdev_ops->ndo_flow_offload_check) {
+		out->netdev_ops->ndo_flow_offload_check(&hw_path);
+		out = (IS_GMAC1_MODE) ? hw_path.virt_dev : hw_path.dev;
+	}
+
+	if (!IS_LAN_GRP(out) && !IS_WAN(out) && !IS_EXT(out))
+		return 0;
+
+	trace_printk("[%s] case hit, %x-->%s, reason=%x\n", __func__,
+		     skb_hnat_iface(skb), out->name, skb_hnat_reason(skb));
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return -1;
+
+	entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+
+	switch (skb_hnat_reason(skb)) {
+	case HIT_UNBIND_RATE_REACH:
+		if (entry_hnat_is_bound(entry))
+			break;
+
+		if (fn && !mtk_hnat_accel_type(skb))
+			break;
+
+		if (fn && fn(skb, arp_dev, &hw_path))
+			break;
+
+		skb_to_hnat_info(skb, out, entry, &hw_path);
+		break;
+	case HIT_BIND_KEEPALIVE_DUP_OLD_HDR:
+		/* update hnat count to nf_conntrack by keepalive */
+		if (hnat_priv->data->per_flow_accounting && hnat_priv->nf_stat_en)
+			mtk_hnat_nf_update(skb);
+
+		if (fn && !mtk_hnat_accel_type(skb))
+			break;
+
+		/* update dscp for qos */
+		mtk_hnat_dscp_update(skb, entry);
+
+		/* update mcast timestamp*/
+		if (hnat_priv->data->version == MTK_HNAT_V1_3 &&
+		    hnat_priv->data->mcast && entry->bfib1.sta == 1)
+			entry->ipv4_hnapt.m_timestamp = foe_timestamp(hnat_priv);
+
+		if (entry_hnat_is_bound(entry)) {
+			memset(skb_hnat_info(skb), 0, sizeof(struct hnat_desc));
+
+			return -1;
+		}
+		break;
+	case HIT_BIND_MULTICAST_TO_CPU:
+	case HIT_BIND_MULTICAST_TO_GMAC_CPU:
+		/*do not forward to gdma again,if ppe already done it*/
+		if (IS_LAN_GRP(out) || IS_WAN(out))
+			return -1;
+		break;
+	}
+
+	return 0;
+}
+
+static unsigned int
+mtk_hnat_ipv6_nf_local_out(void *priv, struct sk_buff *skb,
+			   const struct nf_hook_state *state)
+{
+	struct foe_entry *entry;
+	struct ipv6hdr *ip6h;
+	struct iphdr _iphdr;
+	const struct iphdr *iph;
+	struct tcpudphdr _ports;
+	const struct tcpudphdr *pptr;
+	int udp = 0;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	if (unlikely(!skb_hnat_is_hashed(skb)))
+		return NF_ACCEPT;
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return NF_ACCEPT;
+
+	entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+	if (skb_hnat_reason(skb) == HIT_UNBIND_RATE_REACH) {
+		ip6h = ipv6_hdr(skb);
+		if (ip6h->nexthdr == NEXTHDR_IPIP) {
+			/* Map-E LAN->WAN: need to record orig info before fn. */
+			if (mape_toggle) {
+				iph = skb_header_pointer(skb, IPV6_HDR_LEN,
+							 sizeof(_iphdr), &_iphdr);
+				if (unlikely(!iph))
+					return NF_ACCEPT;
+
+				switch (iph->protocol) {
+				case IPPROTO_UDP:
+					udp = 1;
+				case IPPROTO_TCP:
+				break;
+
+				default:
+					return NF_ACCEPT;
+				}
+
+				pptr = skb_header_pointer(skb, IPV6_HDR_LEN + iph->ihl * 4,
+							  sizeof(_ports), &_ports);
+				if (unlikely(!pptr))
+                                        return NF_ACCEPT;
+				/* don't process inner fragment packets */
+				if (ip_is_fragment(iph))
+					return NF_ACCEPT;
+
+				entry->bfib1.udp = udp;
+
+				/* Map-E LAN->WAN record inner IPv4 header info. */
+#if defined(CONFIG_MEDIATEK_NETSYS_V2) || defined(CONFIG_MEDIATEK_NETSYS_V3)
+				entry->bfib1.pkt_type = IPV4_MAP_E;
+				entry->ipv4_dslite.iblk2.dscp = iph->tos;
+				entry->ipv4_mape.new_sip = ntohl(iph->saddr);
+				entry->ipv4_mape.new_dip = ntohl(iph->daddr);
+				entry->ipv4_mape.new_sport = ntohs(pptr->src);
+				entry->ipv4_mape.new_dport = ntohs(pptr->dst);
+#else
+				entry->ipv4_hnapt.iblk2.dscp = iph->tos;
+				entry->ipv4_hnapt.new_sip = ntohl(iph->saddr);
+				entry->ipv4_hnapt.new_dip = ntohl(iph->daddr);
+				entry->ipv4_hnapt.new_sport = ntohs(pptr->src);
+				entry->ipv4_hnapt.new_dport = ntohs(pptr->dst);
+#endif
+			} else {
+				entry->bfib1.pkt_type = IPV4_DSLITE;
+			}
+		}
+	}
+	return NF_ACCEPT;
+}
+
+static unsigned int
+mtk_hnat_ipv6_nf_post_routing(void *priv, struct sk_buff *skb,
+			      const struct nf_hook_state *state)
+{
+	if (!skb)
+		goto drop;
+
+	post_routing_print(skb, state->in, state->out, __func__);
+
+	if (!mtk_hnat_nf_post_routing(skb, state->out, hnat_ipv6_get_nexthop,
+				      __func__))
+		return NF_ACCEPT;
+
+drop:
+	if (skb)
+		trace_printk(
+			"%s:drop (iif=0x%x, out_dev=%s, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, skb_hnat_iface(skb), state->out->name,
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int
+mtk_hnat_ipv4_nf_post_routing(void *priv, struct sk_buff *skb,
+			      const struct nf_hook_state *state)
+{
+	if (!skb)
+		goto drop;
+
+	post_routing_print(skb, state->in, state->out, __func__);
+
+	if (!mtk_hnat_nf_post_routing(skb, state->out, hnat_ipv4_get_nexthop,
+				      __func__))
+		return NF_ACCEPT;
+
+drop:
+	if (skb)
+		trace_printk(
+			"%s:drop (iif=0x%x, out_dev=%s, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, skb_hnat_iface(skb), state->out->name,
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int
+mtk_pong_hqos_handler(void *priv, struct sk_buff *skb,
+		      const struct nf_hook_state *state)
+{
+	struct vlan_ethhdr *veth;
+
+	if (!skb)
+		goto drop;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	veth = (struct vlan_ethhdr *)skb_mac_header(skb);
+
+	if (IS_HQOS_MODE && eth_hdr(skb)->h_proto == HQOS_MAGIC_TAG) {
+		skb_hnat_entry(skb) = ntohs(veth->h_vlan_TCI) & 0x3fff;
+		skb_hnat_reason(skb) = HIT_BIND_FORCE_TO_CPU;
+	}
+
+	if (skb_hnat_iface(skb) == FOE_MAGIC_EXT)
+		clr_from_extge(skb);
+
+	/* packets from external devices -> xxx ,step 2, learning stage */
+	if (do_ext2ge_fast_learn(state->in, skb) && (!qos_toggle ||
+	    (qos_toggle && eth_hdr(skb)->h_proto != HQOS_MAGIC_TAG))) {
+		if (!do_hnat_ext_to_ge2(skb, __func__))
+			return NF_STOLEN;
+		goto drop;
+	}
+
+	/* packets form ge -> external device */
+	if (do_ge2ext_fast(state->in, skb)) {
+		if (!do_hnat_ge_to_ext(skb, __func__))
+			return NF_STOLEN;
+		goto drop;
+	}
+
+	return NF_ACCEPT;
+
+drop:
+	if (skb)
+		printk_ratelimited(KERN_WARNING
+			"%s:drop (in_dev=%s, iif=0x%x, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, state->in->name, skb_hnat_iface(skb),
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int
+mtk_hnat_br_nf_local_out(void *priv, struct sk_buff *skb,
+			 const struct nf_hook_state *state)
+{
+	if (!skb)
+		goto drop;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	post_routing_print(skb, state->in, state->out, __func__);
+
+	if (!mtk_hnat_nf_post_routing(skb, state->out, 0, __func__))
+		return NF_ACCEPT;
+
+drop:
+	if (skb)
+		trace_printk(
+			"%s:drop (iif=0x%x, out_dev=%s, CB2=0x%x, ppe_hash=0x%x,\n"
+			"sport=0x%x, reason=0x%x, alg=0x%x)\n",
+			__func__, skb_hnat_iface(skb), state->out->name,
+			HNAT_SKB_CB2(skb)->magic, skb_hnat_entry(skb),
+			skb_hnat_sport(skb), skb_hnat_reason(skb),
+			skb_hnat_alg(skb));
+
+	return NF_DROP;
+}
+
+static unsigned int
+mtk_hnat_ipv4_nf_local_out(void *priv, struct sk_buff *skb,
+			   const struct nf_hook_state *state)
+{
+	struct foe_entry *entry;
+	struct iphdr *iph;
+
+	if (!is_magic_tag_valid(skb))
+		return NF_ACCEPT;
+
+	if (unlikely(skb_headroom(skb) < FOE_INFO_LEN))
+		return NF_ACCEPT;
+
+	if (!skb_hnat_is_hashed(skb))
+		return NF_ACCEPT;
+
+	if (skb_hnat_entry(skb) >= hnat_priv->foe_etry_num ||
+	    skb_hnat_ppe(skb) >= CFG_PPE_NUM)
+		return NF_ACCEPT;
+
+	entry = &hnat_priv->foe_table_cpu[skb_hnat_ppe(skb)][skb_hnat_entry(skb)];
+
+	/* Make the flow from local not be bound. */
+	iph = ip_hdr(skb);
+	if (iph->protocol == IPPROTO_IPV6) {
+		entry->udib1.pkt_type = IPV6_6RD;
+		hnat_set_head_frags(state, skb, 0, hnat_set_alg);
+	} else {
+		hnat_set_head_frags(state, skb, 1, hnat_set_alg);
+	}
+
+	return NF_ACCEPT;
+}
+
+static unsigned int mtk_hnat_br_nf_forward(void *priv,
+					   struct sk_buff *skb,
+					   const struct nf_hook_state *state)
+{
+	if ((hnat_priv->data->version == MTK_HNAT_V1_2) &&
+	    unlikely(IS_EXT(state->in) && IS_EXT(state->out)))
+		hnat_set_head_frags(state, skb, 1, hnat_set_alg);
+
+	return NF_ACCEPT;
+}
+
+static struct nf_hook_ops mtk_hnat_nf_ops[] __read_mostly = {
+	{
+		.hook = mtk_hnat_ipv4_nf_pre_routing,
+		.pf = NFPROTO_IPV4,
+		.hooknum = NF_INET_PRE_ROUTING,
+		.priority = NF_IP_PRI_FIRST + 1,
+	},
+	{
+		.hook = mtk_hnat_ipv6_nf_pre_routing,
+		.pf = NFPROTO_IPV6,
+		.hooknum = NF_INET_PRE_ROUTING,
+		.priority = NF_IP_PRI_FIRST + 1,
+	},
+	{
+		.hook = mtk_hnat_ipv6_nf_post_routing,
+		.pf = NFPROTO_IPV6,
+		.hooknum = NF_INET_POST_ROUTING,
+		.priority = NF_IP_PRI_LAST,
+	},
+	{
+		.hook = mtk_hnat_ipv6_nf_local_out,
+		.pf = NFPROTO_IPV6,
+		.hooknum = NF_INET_LOCAL_OUT,
+		.priority = NF_IP_PRI_LAST,
+	},
+	{
+		.hook = mtk_hnat_ipv4_nf_post_routing,
+		.pf = NFPROTO_IPV4,
+		.hooknum = NF_INET_POST_ROUTING,
+		.priority = NF_IP_PRI_LAST,
+	},
+	{
+		.hook = mtk_hnat_ipv4_nf_local_out,
+		.pf = NFPROTO_IPV4,
+		.hooknum = NF_INET_LOCAL_OUT,
+		.priority = NF_IP_PRI_LAST,
+	},
+	{
+		.hook = mtk_hnat_br_nf_local_in,
+		.pf = NFPROTO_BRIDGE,
+		.hooknum = NF_BR_LOCAL_IN,
+		.priority = NF_BR_PRI_FIRST,
+	},
+	{
+		.hook = mtk_hnat_br_nf_local_out,
+		.pf = NFPROTO_BRIDGE,
+		.hooknum = NF_BR_LOCAL_OUT,
+		.priority = NF_BR_PRI_LAST - 1,
+	},
+	{
+		.hook = mtk_pong_hqos_handler,
+		.pf = NFPROTO_BRIDGE,
+		.hooknum = NF_BR_PRE_ROUTING,
+		.priority = NF_BR_PRI_FIRST + 1,
+	},
+};
+
+int hnat_register_nf_hooks(void)
+{
+	return nf_register_net_hooks(&init_net, mtk_hnat_nf_ops, ARRAY_SIZE(mtk_hnat_nf_ops));
+}
+
+void hnat_unregister_nf_hooks(void)
+{
+	nf_unregister_net_hooks(&init_net, mtk_hnat_nf_ops, ARRAY_SIZE(mtk_hnat_nf_ops));
+}
+
+int whnat_adjust_nf_hooks(void)
+{
+	struct nf_hook_ops *hook = mtk_hnat_nf_ops;
+	unsigned int n = ARRAY_SIZE(mtk_hnat_nf_ops);
+
+	while (n-- > 0) {
+		if (hook[n].hook == mtk_hnat_br_nf_local_in) {
+			hook[n].hooknum = NF_BR_PRE_ROUTING;
+			hook[n].priority = NF_BR_PRI_FIRST + 1;
+		} else if (hook[n].hook == mtk_hnat_br_nf_local_out) {
+			hook[n].hooknum = NF_BR_POST_ROUTING;
+		} else if (hook[n].hook == mtk_pong_hqos_handler) {
+			hook[n].hook = mtk_hnat_br_nf_forward;
+			hook[n].hooknum = NF_BR_FORWARD;
+			hook[n].priority = NF_BR_PRI_LAST - 1;
+		}
+	}
+
+	return 0;
+}
+
+int mtk_hqos_ptype_cb(struct sk_buff *skb, struct net_device *dev,
+		      struct packet_type *pt, struct net_device *unused)
+{
+	struct vlan_ethhdr *veth = (struct vlan_ethhdr *)skb_mac_header(skb);
+
+	skb_hnat_entry(skb) = ntohs(veth->h_vlan_TCI) & 0x3fff;
+	skb_hnat_reason(skb) = HIT_BIND_FORCE_TO_CPU;
+
+	if (do_hnat_ge_to_ext(skb, __func__) == -1)
+		return 1;
+
+	return 0;
+}
+
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_stag.c b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_stag.c
new file mode 100644
index 00000000..75c3a75c
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/hnat_stag.c
@@ -0,0 +1,63 @@
+/* SPDX-License-Identifier: GPL-2.0
+ *
+ * Copyright (c) 2020 MediaTek Inc.
+ * Author: Landen Chao <landen.chao@mediatek.com>
+ */
+
+#include <linux/of_device.h>
+#include <net/netfilter/nf_flow_table.h>
+#include "hnat.h"
+
+u32 hnat_dsa_fill_stag(const struct net_device *netdev,
+		       struct foe_entry *entry,
+		       struct flow_offload_hw_path *hw_path,
+		       u16 eth_proto,
+		       int mape)
+{
+	const struct net_device *ndev;
+	const unsigned int *port_reg;
+	int port_index;
+	u16 sp_tag;
+
+	if (hw_path->flags & FLOW_OFFLOAD_PATH_VLAN)
+		ndev = hw_path->dev;
+	else
+		ndev = netdev;
+
+	port_reg = of_get_property(ndev->dev.of_node, "reg", NULL);
+	if (unlikely(!port_reg))
+		return -EINVAL;
+
+	port_index = be32_to_cpup(port_reg);
+	sp_tag = BIT(port_index);
+
+	if (!entry->bfib1.vlan_layer)
+		entry->bfib1.vlan_layer = 1;
+	else
+		/* VLAN existence indicator */
+		sp_tag |= BIT(8);
+	entry->bfib1.vpm = 0;
+
+	switch (eth_proto) {
+	case ETH_P_IP:
+		if (entry->ipv4_hnapt.bfib1.pkt_type == IPV4_DSLITE
+			|| (entry->ipv4_hnapt.bfib1.pkt_type == IPV4_MAP_E))
+			entry->ipv4_dslite.etype = sp_tag;
+		else
+			entry->ipv4_hnapt.etype = sp_tag;
+		break;
+	case ETH_P_IPV6:
+		/* In the case MAPE LAN --> WAN, binding entry is to CPU.
+		 * Do not add special tag.
+		 */
+		if (!mape)
+			/* etype offset of ipv6 entries are the same. */
+			entry->ipv6_5t_route.etype = sp_tag;
+
+		break;
+	default:
+		pr_info("DSA + HNAT unsupport protocol\n");
+	}
+
+	return port_index;
+}
diff --git a/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/nf_hnat_mtk.h b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/nf_hnat_mtk.h
new file mode 100644
index 00000000..800c8213
--- /dev/null
+++ b/trunk/linux-4.4.x/drivers/net/ethernet/raeth/mtk_hnat/nf_hnat_mtk.h
@@ -0,0 +1,179 @@
+/*   This program is free software; you can redistribute it and/or modify
+ *   it under the terms of the GNU General Public License as published by
+ *   the Free Software Foundation; version 2 of the License
+ *
+ *   This program is distributed in the hope that it will be useful,
+ *   but WITHOUT ANY WARRANTY; without even the implied warranty of
+ *   MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
+ *   GNU General Public License for more details.
+ *
+ *   Copyright (C) 2014-2016 Sean Wang <sean.wang@mediatek.com>
+ *   Copyright (C) 2016-2017 John Crispin <blogic@openwrt.org>
+ */
+
+#ifndef NF_HNAT_MTK_H
+#define NF_HNAT_MTK_H
+
+#include <linux/dma-mapping.h>
+#include <linux/netdevice.h>
+#include "../mtk_eth_soc.h"
+
+#define HNAT_SKB_CB2(__skb) ((struct hnat_skb_cb2 *)&((__skb)->cb[44]))
+struct hnat_skb_cb2 {
+	__u32 magic;
+};
+
+#if defined(CONFIG_MEDIATEK_NETSYS_V3)
+struct hnat_desc {
+	u32 entry : 15;
+	u32 filled : 3;
+	u32 crsn : 5;
+	u32 resv1 : 3;
+	u32 sport : 4;
+	u32 resv2 : 1;
+	u32 alg : 1;
+	u32 iface : 8;
+	u32 wdmaid : 2;
+	u32 rxid : 2;
+	u32 wcid : 16;
+	u32 bssid : 8;
+	u32 usr_info : 16;
+	u32 tid : 4;
+	u32 is_fixedrate : 1;
+	u32 is_prior : 1;
+	u32 is_sp : 1;
+	u32 hf : 1;
+	u32 amsdu : 1;
+	u32 resv3 : 19;
+	u32 magic_tag_protect : 16;
+} __packed;
+#elif defined(CONFIG_MEDIATEK_NETSYS_RX_V2)
+struct hnat_desc {
+	u32 entry : 15;
+	u32 filled : 3;
+	u32 crsn : 5;
+	u32 resv1 : 3;
+	u32 sport : 4;
+	u32 resv2 : 1;
+	u32 alg : 1;
+	u32 iface : 8;
+	u32 wdmaid : 2;
+	u32 rxid : 2;
+	u32 wcid : 10;
+	u32 bssid : 6;
+	u32 resv5 : 20;
+	u32 magic_tag_protect : 16;
+} __packed;
+#else
+struct hnat_desc {
+	u32 entry : 14;
+	u32 crsn : 5;
+	u32 sport : 4;
+	u32 alg : 1;
+	u32 iface : 8;
+	u32 filled : 3;
+	u32 resv : 1;
+	u32 magic_tag_protect : 16;
+	u32 wdmaid : 8;
+	u32 rxid : 2;
+	u32 wcid : 10;
+	u32 bssid : 6;
+} __packed;
+#endif
+
+#define HQOS_MAGIC_TAG 0x5678
+#define HAS_HQOS_MAGIC_TAG(skb) (qos_toggle && skb->protocol == HQOS_MAGIC_TAG)
+
+#define HNAT_MAGIC_TAG 0x6789
+#define WIFI_INFO_LEN 6
+#define FOE_INFO_LEN (10 + WIFI_INFO_LEN)
+#define IS_SPACE_AVAILABLE_HEAD(skb)                                           \
+	((((skb_headroom(skb) >= FOE_INFO_LEN) ? 1 : 0)))
+
+#define skb_hnat_info(skb) ((struct hnat_desc *)(skb->head))
+#define skb_hnat_magic(skb) (((struct hnat_desc *)(skb->head))->magic)
+#define skb_hnat_reason(skb) (((struct hnat_desc *)(skb->head))->crsn)
+#define skb_hnat_entry(skb) (((struct hnat_desc *)(skb->head))->entry)
+#define skb_hnat_sport(skb) (((struct hnat_desc *)(skb->head))->sport)
+#define skb_hnat_alg(skb) (((struct hnat_desc *)(skb->head))->alg)
+#define skb_hnat_iface(skb) (((struct hnat_desc *)(skb->head))->iface)
+#define skb_hnat_filled(skb) (((struct hnat_desc *)(skb->head))->filled)
+#define skb_hnat_magic_tag(skb) (((struct hnat_desc *)((skb)->head))->magic_tag_protect)
+#define skb_hnat_wdma_id(skb) (((struct hnat_desc *)((skb)->head))->wdmaid)
+#define skb_hnat_rx_id(skb) (((struct hnat_desc *)((skb)->head))->rxid)
+#define skb_hnat_wc_id(skb) (((struct hnat_desc *)((skb)->head))->wcid)
+#define skb_hnat_bss_id(skb) (((struct hnat_desc *)((skb)->head))->bssid)
+#define skb_hnat_usr_info(skb) (((struct hnat_desc *)((skb)->head))->usr_info)
+#define skb_hnat_tid(skb) (((struct hnat_desc *)((skb)->head))->tid)
+#define skb_hnat_is_fixedrate(skb)				\
+	(((struct hnat_desc *)((skb)->head))->is_fixedrate)
+#define skb_hnat_is_prior(skb) (((struct hnat_desc *)((skb)->head))->is_prior)
+#define skb_hnat_is_sp(skb) (((struct hnat_desc *)((skb)->head))->is_sp)
+#define skb_hnat_hf(skb) (((struct hnat_desc *)((skb)->head))->hf)
+#define skb_hnat_amsdu(skb) (((struct hnat_desc *)((skb)->head))->amsdu)
+#define skb_hnat_ppe2(skb)						\
+	((skb_hnat_iface(skb) == FOE_MAGIC_GE_LAN2 ||			\
+	 skb_hnat_iface(skb) == FOE_MAGIC_WED2) && CFG_PPE_NUM == 3)
+#define skb_hnat_ppe1(skb)						\
+	((skb_hnat_iface(skb) == FOE_MAGIC_GE_WAN && CFG_PPE_NUM == 3) ||	\
+	 (skb_hnat_iface(skb) == FOE_MAGIC_WED1 && CFG_PPE_NUM > 1))
+#define skb_hnat_ppe(skb)						\
+	(skb_hnat_ppe2(skb) ? 2 : (skb_hnat_ppe1(skb) ? 1 : 0))
+#define headroom_iface(h) (h.iface)
+#define headroom_ppe1(h)						\
+		((headroom_iface(h) == FOE_MAGIC_GE_LAN2 ||		\
+		 headroom_iface(h) == FOE_MAGIC_WED2) && CFG_PPE_NUM == 3)
+#define headroom_ppe2(h)						\
+	((headroom_iface(h) == FOE_MAGIC_GE_LAN2 ||			\
+	 headroom_iface(h) == FOE_MAGIC_WED2) && CFG_PPE_NUM == 3)
+#define headroom_ppe(h) \
+	(headroom_ppe2(h) ? 2 : (headroom_ppe1(h) ? 1 : 0))
+
+#define do_ext2ge_fast_try(dev, skb)						\
+	((skb_hnat_iface(skb) == FOE_MAGIC_EXT) && !is_from_extge(skb))
+#define set_from_extge(skb) (HNAT_SKB_CB2(skb)->magic = 0x78786688)
+#define clr_from_extge(skb) (HNAT_SKB_CB2(skb)->magic = 0x0)
+#define set_to_ppe(skb) (HNAT_SKB_CB2(skb)->magic = 0x78681415)
+#define is_from_extge(skb) (HNAT_SKB_CB2(skb)->magic == 0x78786688)
+#define is_magic_tag_valid(skb) (skb_hnat_magic_tag(skb) == HNAT_MAGIC_TAG)
+#define set_from_mape(skb) (HNAT_SKB_CB2(skb)->magic = 0x78787788)
+#define is_from_mape(skb) (HNAT_SKB_CB2(skb)->magic == 0x78787788)
+#define is_unreserved_port(hdr)						       \
+	((ntohs(hdr->source) > 1023) && (ntohs(hdr->dest) > 1023))
+
+#define TTL_0 0x02
+#define HAS_OPTION_HEADER 0x03
+#define NO_FLOW_IS_ASSIGNED 0x07
+#define IPV4_WITH_FRAGMENT 0x08
+#define IPV4_HNAPT_DSLITE_WITH_FRAGMENT 0x09
+#define IPV4_HNAPT_DSLITE_WITHOUT_TCP_UDP 0x0A
+#define IPV6_5T_6RD_WITHOUT_TCP_UDP 0x0B
+#define TCP_FIN_SYN_RST                                                        \
+	0x0C /* Ingress packet is TCP fin/syn/rst (for IPv4 NAPT/DS-Lite or IPv6 5T-route/6RD) */
+#define UN_HIT 0x0D /* FOE Un-hit */
+#define HIT_UNBIND 0x0E /* FOE Hit unbind */
+#define HIT_UNBIND_RATE_REACH 0x0F
+#define HIT_BIND_TCP_FIN 0x10
+#define HIT_BIND_TTL_1 0x11
+#define HIT_BIND_WITH_VLAN_VIOLATION 0x12
+#define HIT_BIND_KEEPALIVE_UC_OLD_HDR 0x13
+#define HIT_BIND_KEEPALIVE_MC_NEW_HDR 0x14
+#define HIT_BIND_KEEPALIVE_DUP_OLD_HDR 0x15
+#define HIT_BIND_FORCE_TO_CPU 0x16
+#define HIT_BIND_WITH_OPTION_HEADER 0x17
+#define HIT_BIND_MULTICAST_TO_CPU 0x18
+#define HIT_BIND_MULTICAST_TO_GMAC_CPU 0x19
+#define HIT_PRE_BIND 0x1A
+#define HIT_BIND_PACKET_SAMPLING 0x1B
+#define HIT_BIND_EXCEED_MTU 0x1C
+
+#define TPORT_ID(x) ((x) & GENMASK(3, 0))
+#define TOPS_ENTRY(x) ((x) & GENMASK(5, 0))
+#define CDRT_ID(x) ((x) & GENMASK(7, 0))
+
+u32 hnat_tx(struct sk_buff *skb);
+u32 hnat_set_skb_info(struct sk_buff *skb, u32 *rxd);
+u32 hnat_reg(struct net_device *, void __iomem *);
+u32 hnat_unreg(void);
+
+#endif
-- 
2.32.0.windows.2

